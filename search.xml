<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>多模态工业异常检测（Multimodal Industrial Anomaly Detection）</title>
      <link href="/2025/01/09/038-mmad/"/>
      <url>/2025/01/09/038-mmad/</url>
      
        <content type="html"><![CDATA[<h1 id="多模态异常（缺陷）检测"><a href="#多模态异常（缺陷）检测" class="headerlink" title="多模态异常（缺陷）检测"></a>多模态异常（缺陷）检测</h1><h2 id="数据模态"><a href="#数据模态" class="headerlink" title="数据模态"></a>数据模态</h2><p>目前多模态检测算法涉及的数据模态主要（或者说仅仅）有RGB图像、单视角点云（深度图）、文本</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>常见的数据集包括MVTec 3D-AD、EyeCandies</p><table><thead><tr><th>数据集名称</th><th>类别数</th></tr></thead><tbody><tr><td>MVTec 3D-AD</td><td>10</td></tr><tr><td>EyeCandies</td><td>10</td></tr><tr><td>Real3D-AD</td><td>12</td></tr></tbody></table><h2 id="检测算法（基于-RGB-Point-Cloud）"><a href="#检测算法（基于-RGB-Point-Cloud）" class="headerlink" title="检测算法（基于 RGB + Point Cloud）"></a>检测算法（基于 RGB + Point Cloud）</h2><h3 id="（写在前面）PatchCore"><a href="#（写在前面）PatchCore" class="headerlink" title="（写在前面）PatchCore"></a>（写在前面）PatchCore</h3><p>论文：Towards Total Recall in Industrial Anomaly Detection （CVPR 2022）[注：后面大量的工作都是基于PatchCore的模式]</p><p>关键思想：<code>Maximally Representative Memory Bank of Nominal Patch-features.</code></p><p><strong>Memory Bank 机制</strong></p><p>MemoryBank 建立在一个具有内存检索和更新机制的<a href="https://so.csdn.net/so/search?q=%E5%86%85%E5%AD%98%E5%AD%98%E5%82%A8%E5%99%A8&spm=1001.2101.3001.7020">内存存储器</a>上，能够总结过去的事件。通过不断的记忆更新不断进化，通过合成以前的信息，随着时间的推移理解，根据经过的时间和记忆的相对重要性来忘记和强化记忆。每次出现查询请求时，都会遍历一遍历史对话记录，然后当前查询的内容遗忘保留率 <code>s+1</code></p><p>参考链接：</p><p><a href="https://blog.csdn.net/lllllli_/article/details/138252931">MemoryBank：Enhancing Large Language Models with Long-Term Memory_memory bank-CSDN博客</a></p><p><a href="https://www.zhihu.com/question/364132423">(艾宾浩斯记忆曲线有无数学模型？ - 知乎 (zhihu.com)</a></p><p><img src="C:\Users\SvyJ\AppData\Roaming\Typora\typora-user-images\image-20241106092732766.png" alt="image-20241106092732766"></p><h3 id="Back-to-Features-BTF-aka-3D-ADS"><a href="#Back-to-Features-BTF-aka-3D-ADS" class="headerlink" title="Back to Features (BTF aka. 3D-ADS)"></a>Back to Features (BTF aka. 3D-ADS)</h3><p>论文：Back to the feature: classical 3d features are (almost) all you need for 3d anomaly detection (CVPRW 2023)</p><p>核心思想：CNN （RGB图像特征提取）+ FPFH （点云深度特征提取）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Model Fitting.</span><br><span class="hljs-comment">## 1. Extracting Train Features and Fusion.</span><br><span class="hljs-keyword">for</span> (rgb, pc), _ <span class="hljs-keyword">in</span> tqdm(train_loader):<br>    <span class="hljs-comment">### (1) Extracting RGB Patch Features.</span><br>    rgb_patch = self.deep_feature_extractor(rgb) <span class="hljs-comment"># e.g., wide_resnet50_2</span><br>    <br>    <span class="hljs-comment">### (2) Extracting FPFH (Fast Point Feature Histogram) Patch Features.</span><br>    unorganized_pc = organized_pc_to_unorganized_pc(pc)<br>    unorganized_pc_no_zeros = unorganized_pc[np.nonzero(unorganized_pc), :]<br>    o3d_pc = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(unorganized_pc_no_zeros))<br>    <br>    <span class="hljs-comment">### (3) Normal Estimation (法线估计)</span><br>    radius_normal = voxel_size * <span class="hljs-number">2</span><br>    o3d_pc.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=<span class="hljs-number">30</span>))<br>    <br>    <span class="hljs-comment">### (4) Geometric Feature Representation (几何特征描述)</span><br>    radius_feature = voxel_size * <span class="hljs-number">5</span><br>    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature( \<br>       o3d_pc, o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=<span class="hljs-number">100</span>)<br>)<br>    fpfh = pcd_fpfh.data.T<br>    fpfh_patch = np.zeros((unorganized_pc.shape[<span class="hljs-number">0</span>], fpfh.shape[<span class="hljs-number">1</span>]), dtype=fpfh.dtype)<br>    fpfh_patch[nonzero_indices, :] = fpfh<br>    <br>    <span class="hljs-comment">### (5) Add Sample to Memory Bank.</span><br>    self.patch_lib.append(torch.cat([rgb_patch, fpfh_patch], dim=<span class="hljs-number">1</span>))<br><br><span class="hljs-comment">## 2. Get Coreset for Each Feature Extractor.</span><br><span class="hljs-keyword">for</span> method_name, method <span class="hljs-keyword">in</span> self.methods.items():<br>    <span class="hljs-keyword">if</span> self.f_coreset &lt; <span class="hljs-number">1</span>:<br>        <span class="hljs-comment">### Get n coreset idx for given patch_lib</span><br>        transformer = random_projection.SparseRandomProjection(eps=eps)<br>        patch_lib = torch.tensor(transformer.fit_transform(z_lib))<br>        select_idx = <span class="hljs-number">0</span><br>        last_item = z_lib[select_idx:select_idx + <span class="hljs-number">1</span>]<br>        coreset_idx = [torch.tensor(select_idx)]<br>        min_distances = torch.linalg.norm(z_lib - last_item, dim=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>        <br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(n - <span class="hljs-number">1</span>)):<br>            distances = torch.linalg.norm(z_lib - last_item, dim=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># broadcasting step</span><br>            min_distances = torch.minimum(distances, min_distances)  <span class="hljs-comment"># iterative step</span><br>            select_idx = torch.argmax(min_distances)  <span class="hljs-comment"># selection step</span><br>            coreset_idx.append(select_idx)<br>        coreset_idx = torch.stack(coreset_idx)<br>        <br>    self.patch_lib = self.patch_lib[self.coreset_idx] <br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Model Inference.</span><br><span class="hljs-keyword">for</span> (rgb, pc), mask, label <span class="hljs-keyword">in</span> tqdm(test_loader):<br>    <span class="hljs-comment">## 1. Extracting Features.</span><br>    rgb_patch = self.deep_feature_extractor(rgb)<br>    depth_patch = get_fpfh_features(pc)<br>    patch = torch.cat([rgb_patch, depth_patch], dim=<span class="hljs-number">1</span>)<br>    feature_map_dims = torch.cat([rgb_patch[<span class="hljs-number">0</span>], depth_patch], dim=<span class="hljs-number">1</span>).shape[-<span class="hljs-number">2</span>:]<br>    <br>    <span class="hljs-comment">## 2. Compute Aomaly Score and Segmentation Map.</span><br>    dist = torch.cdist(patch, self.patch_lib)<br>    min_val, min_idx = torch.<span class="hljs-built_in">min</span>(dist, dim=<span class="hljs-number">1</span>)<br>    s_idx, s_star = torch.argmax(min_val), torch.<span class="hljs-built_in">max</span>(min_val)<br>    <span class="hljs-comment">### (1) Re-weighting</span><br>    m_test = patch[s_idx].unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># anomalous patch</span><br>    m_star = self.patch_lib[min_idx[s_idx]].unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># closest neighbour</span><br>    w_dist = torch.cdist(m_star, self.patch_lib)  <span class="hljs-comment"># find knn to m_star pt.1</span><br>    _, nn_idx = torch.topk(w_dist, k=self.n_reweight, largest=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># pt.2</span><br>    m_star_knn = torch.linalg.norm(m_test - self.patch_lib[nn_idx[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>:]], dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># Eq. 7 in paper</span><br>    <span class="hljs-comment">### (2) Softmax normalization trick as in transformers.</span><br>    <span class="hljs-comment">### (3) As the patch vectors grow larger, their norm might differ a lot. exp(norm) can give infinities.</span><br>    D = torch.sqrt(torch.tensor(patch.shape[<span class="hljs-number">1</span>]))<br>    w = <span class="hljs-number">1</span> - (torch.exp(s_star / D) / (torch.<span class="hljs-built_in">sum</span>(torch.exp(m_star_knn / D))))<br>    s = w * s_star<br>    <span class="hljs-comment">### (4) Calculate Segmentation Map</span><br>    s_map = min_val.view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, *feature_map_dims)<br>    s_map = torch.nn.functional.interpolate(s_map, size=(self.image_size, self.image_size), mode=<span class="hljs-string">&#x27;bilinear&#x27;</span>)<br>    s_map = self.blur(s_map)<br></code></pre></td></tr></table></figure><h3 id="Shape-Guided-Dual-Memory-Learning"><a href="#Shape-Guided-Dual-Memory-Learning" class="headerlink" title="Shape-Guided Dual-Memory Learning"></a>Shape-Guided Dual-Memory Learning</h3><p>论文：Shape-Guided Dual-Memory Learning for 3D Anomaly Detection (ICML 2023)</p><p><img src="C:\Users\SvyJ\AppData\Roaming\Typora\typora-user-images\image-20240930112928606.png" alt="image-20240930112928606"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Model Fitting.</span><br><span class="hljs-comment">## 1. Model Instantiation.</span><br>self.methods = RGBSDFFeatures(conf, pro_limit, output_dir)<br>self.rgb_feature_extractor = RGB_Model()<br>self.sdf = SDFFeature() <span class="hljs-comment"># include self.encoder = encoder_BN(), self.NIF = local_NIF()</span><br><br><span class="hljs-comment">## 2. Extract RGB and SDF Features.</span><br><span class="hljs-keyword">for</span> train_data_id, (img, pc, _) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(data_loader):<br>    rgb_feature_maps = self.rgb_feature_extractor(img) <span class="hljs-comment"># Extract RGB features.</span><br>    sdf_feature, rgb_feature_indices_patch = self.sdf.get_feature(pc, train_data_id) <span class="hljs-comment"># Extract SDF features.</span><br>    self.sdf_patch_lib.append(sdf_feature)<br>    self.rgb_patch_lib.append(rgb_patch_size28)<br>    self.rgb_f_idx_patch_lib.extend(rgb_feature_indices_patch)<br><br><span class="hljs-comment">## 3. Foreground Subsampling.</span><br>self.sdf_patch_lib = torch.cat(self.sdf_patch_lib, <span class="hljs-number">0</span>)<br>self.rgb_patch_lib = torch.cat(self.rgb_patch_lib, <span class="hljs-number">0</span>)<br>use_f_idices = torch.unique(torch.cat(self.rgb_f_idx_patch_lib, dim=<span class="hljs-number">0</span>))<br>self.rgb_patch_lib = self.rgb_patch_lib[use_f_idices] <span class="hljs-comment"># Remove unused RGB features</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Model Inference.</span><br><span class="hljs-comment">## 1. Generate Predictions for alignment.</span><br><span class="hljs-keyword">for</span> align_data_id, (img, pc, _) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br><span class="hljs-keyword">if</span> align_data_id &lt; <span class="hljs-number">25</span>:<br>        <span class="hljs-string">&#x27;&#x27;&#x27;RGB patch&#x27;&#x27;&#x27;</span><br>        rgb_feature_maps = self.rgb_feature_extractor(img)<br>        rgb_map, rgb_s = self.Dict_compute_rgb_map(rgb_feature_maps, rgb_features_indices, lib_idices, mode=<span class="hljs-string">&#x27;alignment&#x27;</span>)<br>        <span class="hljs-string">&#x27;&#x27;&#x27;SDF patch&#x27;&#x27;&#x27;</span><br>        feature, rgb_features_indices = self.sdf.get_feature(pc, align_data_id, <span class="hljs-string">&#x27;test&#x27;</span>) <br>        NN_feature, Dict_features, lib_idices, sdf_s = self.Find_KNN_feature(feature, mode=<span class="hljs-string">&#x27;alignment&#x27;</span>)<br>        sdf_map = self.sdf.get_score_map(Dict_features, sample[<span class="hljs-number">1</span>], sample[<span class="hljs-number">2</span>])<br>        <span class="hljs-string">&#x27;&#x27;&#x27;Image_level predictions&#x27;&#x27;&#x27;</span><br>        self.sdf_image_preds.append(sdf_s)<br>        self.rgb_image_preds.append(rgb_s)<br>        <span class="hljs-string">&#x27;&#x27;&#x27;Pixel_level predictions&#x27;&#x27;&#x27;</span><br>        self.rgb_pixel_preds.extend(rgb_map.flatten())<br>        self.sdf_pixel_preds.extend(sdf_map.flatten())<br><br><span class="hljs-comment">## 2. Computing weight and bias for alignment with SDF and RGB distribution.</span><br><span class="hljs-comment">### (1) Compute SDF distribution</span><br>non_zero_sdf_map = sdf_map[np.nonzero(sdf_map)]<br>sdf_lower = np.mean(non_zero_sdf_map) - <span class="hljs-number">3</span> * np.std(non_zero_sdf_map)<br>sdf_upper = np.mean(non_zero_sdf_map) + <span class="hljs-number">3</span> * np.std(non_zero_sdf_map)<br><span class="hljs-comment">### (2) Compute RGB distribution</span><br>non_zero_rgb_map = rgb_map[np.nonzero(rgb_map)]<br>rgb_lower = np.mean(non_zero_rgb_map) - <span class="hljs-number">3</span> * np.std(non_zero_rgb_map)<br>rgb_upper = np.mean(non_zero_rgb_map) + <span class="hljs-number">3</span> * np.std(non_zero_rgb_map)<br><span class="hljs-comment">### (3) Compute weight and bias for alignment</span><br>self.weight = (sdf_upper - sdf_lower) / (rgb_upper - rgb_lower)<br>self.bias = sdf_lower - self.weight * rgb_lower<br><br><span class="hljs-comment">## 3. Extract testing features and predict.</span><br><span class="hljs-keyword">for</span> test_data_id, (sample, mask, label) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(test_loader):<br><span class="hljs-comment">### (1) RGB patch</span><br>    rgb_feature_maps = self.rgb_feature_extractor(img)<br>    rgb_map, rgb_s = self.Dict_compute_rgb_map(rgb_feature_maps, rgb_features_indices, lib_idices, mode=<span class="hljs-string">&#x27;alignment&#x27;</span>)<br>    <span class="hljs-comment">### (2) SDF patch</span><br>    feature, rgb_features_indices = self.sdf.get_feature(pc, align_data_id, <span class="hljs-string">&#x27;test&#x27;</span>) <br>    NN_feature, Dict_features, lib_idices, sdf_s = self.Find_KNN_feature(feature, mode=<span class="hljs-string">&#x27;alignment&#x27;</span>)<br>    sdf_map = self.sdf.get_score_map(Dict_features, sample[<span class="hljs-number">1</span>], sample[<span class="hljs-number">2</span>])<br>    <span class="hljs-comment">### (3) Predictions                                         </span><br>image_score = sdf_s * rgb_s <span class="hljs-comment"># Image-level prediction</span><br>new_rgb_map = rgb_map * self.weight + self.bias<br>pixel_map = torch.maximum(new_rgb_map, sdf_map) <span class="hljs-comment"># Pixel-level prediction</span><br></code></pre></td></tr></table></figure><h3 id="Multi-3D-Memory-M3DM"><a href="#Multi-3D-Memory-M3DM" class="headerlink" title="Multi-3D-Memory (M3DM)"></a>Multi-3D-Memory (M3DM)</h3><p>论文：Multimodal Industrial Anomaly Detection via Hybrid Fusion (CVPR 2023)</p><p>复现：<a href="https://svyj.github.io/2023/05/03/038-M3DM/">M3DM复现记录 | “干杯( ﾟ-ﾟ)っロ” (svyj.github.io)</a></p><p><img src="C:\Users\SvyJ\AppData\Roaming\Typora\typora-user-images\image-20240930112450543.png" alt="image-20240930112450543"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Train UFF.</span><br><span class="hljs-keyword">for</span> data_iter_step, (samples, _) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(metric_logger.log_every(data_loader, print_freq, header)):<br>    xyz_samples = samples[:,:,:<span class="hljs-number">1152</span>].to(device, non_blocking=<span class="hljs-literal">True</span>)<br>    rgb_samples = samples[:,:,<span class="hljs-number">1152</span>:].to(device, non_blocking=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment">## 1. Normalize</span><br>    q = nn.functional.normalize(q, dim=<span class="hljs-number">1</span>)<br>    k = nn.functional.normalize(k, dim=<span class="hljs-number">1</span>)<br>    <span class="hljs-comment">## 2. Gather All Targets, Einstein Sum is More Intuitive</span><br>    logits = torch.einsum(<span class="hljs-string">&#x27;nc,mc-&gt;nm&#x27;</span>, [q, k]) / self.T<br>    N = logits.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># batch size per GPU</span><br>    labels = (torch.arange(N, dtype=torch.long) + N * torch.distributed.get_rank()).cuda()<br>    loss = nn.CrossEntropyLoss()(logits, labels) * (<span class="hljs-number">2</span> * self.T) <span class="hljs-comment"># Contrastive Loss</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Model Fitting.</span><br><span class="hljs-comment">## 1. Extracting Train Features and Fusion.</span><br><span class="hljs-keyword">for</span> sample, _ <span class="hljs-keyword">in</span> tqdm(train_loader):<br>    <span class="hljs-comment">### (1) Extracting Train Features.</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    RGB backbone - vit_base_patch8_224_dino</span><br><span class="hljs-string">    XYZ backbone - Point_MAE</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    rgb_feature_maps, xyz_feature_maps, _, _, _, interpolated_pc = \<br>    self.deep_feature_extractor(rgb, xyz)<br>    <br>    <span class="hljs-comment">### (2) Feature Fusion.</span><br>    xyz_feature  = self.xyz_mlp(self.xyz_norm(xyz_feature))<br>    rgb_feature  = self.rgb_mlp(self.rgb_norm(rgb_feature))<br>    patch = torch.cat([xyz_feature, rgb_feature], dim=<span class="hljs-number">2</span>)<br>    <br>    <span class="hljs-comment">### (3) Add Sample to Memory Bank.</span><br>    self.patch_lib.append(rgb_patch)<br>    <br><span class="hljs-comment">## 2. Get Coreset from Memory Bank.</span><br><span class="hljs-keyword">for</span> method_name, method <span class="hljs-keyword">in</span> self.methods.items():<br>    <span class="hljs-keyword">if</span> self.f_coreset &lt; <span class="hljs-number">1</span>:<br>        self.coreset_idx = \<br>            self.get_coreset_idx_randomp(self.patch_lib, n=self.f_coreset*self.patch_lib.shape[<span class="hljs-number">0</span>]))<br>        self.patch_lib = self.patch_lib[self.coreset_idx]    <br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Model Inference.</span><br><span class="hljs-keyword">for</span> sample, mask, label, rgb_path <span class="hljs-keyword">in</span> tqdm(test_loader):<br>    <span class="hljs-comment">## 1. Extracting Features.</span><br>    rgb_feature_maps, xyz_feature_maps, center, neighbor_idx, center_idx, interpolated_pc = \<br>    self.deep_feature_extractor(rgb, xyz)<br>    <br>    <span class="hljs-comment">## 2. Feature Fusion.</span><br>    xyz_feature  = self.xyz_mlp(self.xyz_norm(xyz_feature))<br>    rgb_feature  = self.rgb_mlp(self.rgb_norm(rgb_feature))<br>    patch = torch.cat([xyz_feature, rgb_feature], dim=<span class="hljs-number">2</span>)<br>    <br>    <span class="hljs-comment">## 3. Compute Aomaly Score and Segmentation Map.</span><br>    dist = torch.cdist(patch, self.patch_lib) <span class="hljs-comment"># Calculate the distance between two vectors.</span><br>    min_val, min_idx = torch.<span class="hljs-built_in">min</span>(dist, dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># Anomaly</span><br>    s_idx, s_star = torch.argmax(min_val), torch.<span class="hljs-built_in">max</span>(min_val)<br>    <br>    m_test = patch[s_idx].unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># anomalous patch</span><br>    m_star = self.patch_lib[min_idx[s_idx]].unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># closest neighbour</span><br>    w_dist = torch.cdist(m_star, self.patch_lib)  <span class="hljs-comment"># find knn to m_star pt.1</span><br>    _, nn_idx = torch.topk(w_dist, k=self.n_reweight, largest=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># pt.2</span><br>    <br>    m_star_knn = torch.linalg.norm(m_test - self.patch_lib[nn_idx[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>:]], dim=<span class="hljs-number">1</span>)<br>    D = torch.sqrt(torch.tensor(patch.shape[<span class="hljs-number">1</span>]))<br>    w = <span class="hljs-number">1</span> - (torch.exp(s_star / D) / (torch.<span class="hljs-built_in">sum</span>(torch.exp(m_star_knn / D))))<br>    s = w * s_star<br>    <br>    s_map = min_val.view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, *feature_map_dims)<br>    s_map = torch.nn.functional.interpolate(s_map, size=(self.image_size, self.image_size), mode=<span class="hljs-string">&#x27;bilinear&#x27;</span>)<br>    s_map = self.blur(s_map) <span class="hljs-comment"># segmentation map</span><br></code></pre></td></tr></table></figure><h3 id="Complementary-Pseudo-Multimodal-Feature-CPMF"><a href="#Complementary-Pseudo-Multimodal-Feature-CPMF" class="headerlink" title="Complementary Pseudo Multimodal Feature (CPMF)"></a>Complementary Pseudo Multimodal Feature (CPMF)</h3><p>论文：Complementary pseudo multimodal feature for point cloud anomaly detection (PR 2024)</p><p><img src="C:\Users\SvyJ\AppData\Roaming\Typora\typora-user-images\image-20240930113515145.png" alt="image-20240930113515145"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Model Fitting (Multi-view PatchCore, same as M3DM except feature extraction).</span><br><span class="hljs-comment">## 1. Extracting Train Features and Fusion.</span><br><span class="hljs-keyword">for</span> (img, resized_organized_pc, features, view_images, view_positions), _, _ <span class="hljs-keyword">in</span> tqdm(train_loader):<br>    <span class="hljs-comment">### (1) Extracting RGB Patch Features.</span><br>    <span class="hljs-keyword">if</span> self.n_views &gt; <span class="hljs-number">0</span>:<br>        view_invariant_features = self.calculate_view_invariance_feature(sample)  <span class="hljs-comment"># 视图不变性特征</span><br>        view_invariant_features = F.normalize(view_invariant_features, dim=<span class="hljs-number">1</span>, p=<span class="hljs-number">2</span>)<br>        <br>    <span class="hljs-comment">### (2) Extracting FPFH Patch Features.</span><br>    fpfh_feature_maps = sample[<span class="hljs-number">2</span>][<span class="hljs-number">0</span>]<br>    fpfh_feature_maps = F.normalize(fpfh_feature_maps, dim=<span class="hljs-number">1</span>, p=<span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment">### (3) Feature Fusion.</span><br><span class="hljs-keyword">if</span> self.n_views &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> self.no_fpfh:<br>        concat_patch = torch.cat([view_invariant_features], dim=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">elif</span> self.n_views &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.no_fpfh:<br>        concat_patch = torch.cat([view_invariant_features, fpfh_feature_maps], dim=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">else</span>:<br>        concat_patch = fpfh_feature_maps<br>        <br><span class="hljs-comment">### (4) Add Sample to Memory Bank.</span><br>self.patch_lib.append(concat_patch)<br><br><span class="hljs-comment">## 2. Get Coreset from Memory Bank.</span><br>self.patch_lib = torch.cat(self.patch_lib, <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">if</span> self.f_coreset &lt; <span class="hljs-number">1</span>:<br>        self.coreset_idx = self.get_coreset_idx_randomp(self.patch_lib, <br>                                                        n=<span class="hljs-built_in">int</span>(self.f_coreset * self.patch_lib.shape[<span class="hljs-number">0</span>]), <br>                                                        eps=self.coreset_eps, )<br>self.patch_lib = self.patch_lib[self.coreset_idx] <span class="hljs-comment"># patch 本身的维度并没有变化，而是选择了稀疏的若干特征组成了新的特征库</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Model Evaluation (Same as M3DM except feature extraction).</span><br><span class="hljs-comment">## 1. Extracting Train Features and Fusion.</span><br><span class="hljs-keyword">for</span> sample, mask, label <span class="hljs-keyword">in</span> tqdm(test_loader):<br>    <span class="hljs-comment">### (1) Extracting RGB Patch Features.</span><br>    <span class="hljs-keyword">if</span> self.n_views &gt; <span class="hljs-number">0</span>:<br>        view_invariant_features = self.calculate_view_invariance_feature(sample)<br>        view_invariant_features = F.normalize(view_invariant_features, dim=<span class="hljs-number">1</span>, p=<span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment">### (2) Extracting FPFH Patch Features.</span><br>    fpfh_feature_maps = sample[<span class="hljs-number">2</span>][<span class="hljs-number">0</span>]<br>    fpfh_feature_maps = F.normalize(fpfh_feature_maps, dim=<span class="hljs-number">1</span>, p=<span class="hljs-number">2</span>)<br><br><span class="hljs-comment">### (3) Feature Fusion.</span><br><span class="hljs-keyword">if</span> self.n_views &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> self.no_fpfh:<br>        concat_patch = torch.cat([view_invariant_features], dim=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">elif</span> self.n_views &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.no_fpfh:<br>        concat_patch = torch.cat([view_invariant_features, fpfh_feature_maps], dim=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">else</span>:<br>        concat_patch = fpfh_feature_maps<br><br><span class="hljs-comment">### (4) Compute Aomaly Score and Segmentation Map. (Same as M3DM)</span><br>dist = torch.cdist(patch, self.patch_lib)<br>    min_val, min_idx = torch.<span class="hljs-built_in">min</span>(dist, dim=<span class="hljs-number">1</span>)<br>    s_idx, s_star = torch.argmax(min_val), torch.<span class="hljs-built_in">max</span>(min_val)<br>    <br>    m_test = patch[s_idx].unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># anomalous patch</span><br>    m_star = self.patch_lib[min_idx[s_idx]].unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># closest neighbour</span><br>    w_dist = torch.cdist(m_star, self.patch_lib)  <span class="hljs-comment"># find knn to m_star pt.1</span><br>    _, nn_idx = torch.topk(w_dist, k=self.n_reweight, largest=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># pt.2</span><br>    <br>    m_star_knn = torch.linalg.norm(m_test - self.patch_lib[nn_idx[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>:]], dim=<span class="hljs-number">1</span>)<br>    D = torch.sqrt(torch.tensor(patch.shape[<span class="hljs-number">1</span>]))<br>    w = <span class="hljs-number">1</span> - (torch.exp(s_star / D) / (torch.<span class="hljs-built_in">sum</span>(torch.exp(m_star_knn / D))))<br>    s = w * s_star<br>    <br>    s_map = min_val.view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, *feature_map_dims)<br>    s_map = torch.nn.functional.interpolate(s_map, size=(self.image_size, self.image_size), mode=<span class="hljs-string">&#x27;bilinear&#x27;</span>)<br>    s_map = self.blur(s_map) <span class="hljs-comment"># segmentation map</span><br></code></pre></td></tr></table></figure><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="Multi-modality-Reconstruction-Network-EasyNet"><a href="#Multi-modality-Reconstruction-Network-EasyNet" class="headerlink" title="Multi-modality Reconstruction Network (EasyNet)"></a>Multi-modality Reconstruction Network (EasyNet)</h3><p>论文：EasyNet: An Easy Network for 3D Industrial Anomaly Detection (ACMMM 2023)</p><p><img src="C:\Users\SvyJ\AppData\Roaming\Typora\typora-user-images\image-20240930112745044.png" alt="image-20240930112745044"></p><h3 id="Crossmodal-Feature-Mapping-CFM"><a href="#Crossmodal-Feature-Mapping-CFM" class="headerlink" title="Crossmodal Feature Mapping (CFM)"></a>Crossmodal Feature Mapping (CFM)</h3><p>论文：Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping (CVPR 2024)</p><p><img src="C:\Users\SvyJ\AppData\Roaming\Typora\typora-user-images\image-20240930113052528.png" alt="image-20240930113052528"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Training.</span><br><span class="hljs-comment">## 1. Model Instantiation.</span><br>Model instantiation.<br>CFM_2Dto3D = FeatureProjectionMLP(in_features = <span class="hljs-number">768</span>, out_features = <span class="hljs-number">1152</span>)<br>CFM_3Dto2D = FeatureProjectionMLP(in_features = <span class="hljs-number">1152</span>, out_features = <span class="hljs-number">768</span>)<br><br><span class="hljs-comment">## 2. Iteration.</span><br><span class="hljs-keyword">for</span> (rgb, pc, _), _ <span class="hljs-keyword">in</span> tqdm(train_loader):<br>rgb_patch, xyz_patch = feature_extractor.get_features_maps(rgb, pc)<br>    rgb_feat_pred = CFM_3Dto2D(xyz_patch)<br>    xyz_feat_pred = CFM_2Dto3D(rgb_patch)<br>    <br><span class="hljs-comment">## 3. Losses.</span><br>loss_3Dto2D = <span class="hljs-number">1</span> - metric(xyz_feat_pred[~xyz_mask], xyz_patch[~xyz_mask]).mean()<br>loss_2Dto3D = <span class="hljs-number">1</span> - metric(rgb_feat_pred[~xyz_mask], rgb_patch[~xyz_mask]).mean()<br>cos_sim_3Dto2D, cos_sim_2Dto3D = <span class="hljs-number">1</span> - loss_3Dto2D.cpu(), <span class="hljs-number">1</span> - loss_2Dto3D.cpu()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Inference.</span><br><span class="hljs-keyword">for</span> (rgb, pc, depth), gt, label, rgb_path <span class="hljs-keyword">in</span> tqdm(test_loader):<br>    rgb_patch, xyz_patch = feature_extractor.get_features_maps(rgb, pc)<br>    rgb_feat_pred = CFM_3Dto2D(xyz_patch)<br>    xyz_feat_pred = CFM_2Dto3D(rgb_patch)<br>    xyz_mask = (xyz_patch.<span class="hljs-built_in">sum</span>(axis = -<span class="hljs-number">1</span>) == <span class="hljs-number">0</span>) <span class="hljs-comment"># Mask only the feature vectors that are 0 everywhere.</span><br>    <br>    <span class="hljs-comment">## 1. 3D Cosine Similarity.</span><br>cos_3d = (xyz_feat_pred - xyz_patch).<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>).sqrt()        <br>    cos_3d[xyz_mask] = <span class="hljs-number">0.</span><br>    cos_3d = cos_3d.reshape(<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)<br><br>    <span class="hljs-comment">## 2. 2D Cosine Similarity.</span><br>    cos_2d = (rgb_feat_pred - rgb_patch).<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>).sqrt()        <br>    cos_2d[xyz_mask] = <span class="hljs-number">0.</span><br>    cos_2d = cos_2d.reshape(<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)<br><br>    <span class="hljs-comment">## 3. Combined Cosine Similarity.</span><br>    cos_comb = (cos_2d * cos_3d) <br>    cos_comb.reshape(-<span class="hljs-number">1</span>)[xyz_mask] = <span class="hljs-number">0.</span><br>    cos_comb = cos_comb.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>) <span class="hljs-comment"># Repeated box filters to approximate a Gaussian blur.</span><br></code></pre></td></tr></table></figure><h3 id="3D-Dual-Subspace-Reprojection-3DSR"><a href="#3D-Dual-Subspace-Reprojection-3DSR" class="headerlink" title="3D Dual Subspace Reprojection (3DSR)"></a>3D Dual Subspace Reprojection (3DSR)</h3><p>论文：Cheating Depth: Enhancing 3D Surface Anomaly Detection via Depth Simulation (WACV 2024)</p><p><img src="C:\Users\SvyJ\AppData\Roaming\Typora\typora-user-images\image-20240930113848502.png" alt="image-20240930113848502"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Depth-Aware Discrete Autoencoder.</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DiscreteLatentModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, ...</span>):<br>...<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># Encoder Hi</span><br>        enc_b = self._encoder_b(x) <span class="hljs-comment"># 3 × nn.Conv2d + ResidualStack</span><br><br>        <span class="hljs-comment"># Encoder Lo -- F_Lo</span><br>        enc_t = self._encoder_t(enc_b) <span class="hljs-comment"># 2 × nn.Conv2d + ResidualStack</span><br>        zt = self._pre_vq_conv_top(enc_t) <span class="hljs-comment"># nn.Conv2d</span><br><br>        <span class="hljs-comment"># Quantize F_Lo with K_Lo</span><br>        loss_t, quantized_t, perplexity_t, encodings_t = self._vq_vae_top(zt) <span class="hljs-comment"># nn.Embedding</span><br>        <span class="hljs-comment"># Upsample Q_Lo</span><br>        up_quantized_t = self.upsample_t(quantized_t)<br><br>        <span class="hljs-comment"># Concatenate and transform the output of Encoder_Hi and upsampled Q_lo -- F_Hi</span><br>        feat = torch.cat((enc_b, up_quantized_t), dim=<span class="hljs-number">1</span>)<br>        zb = self._pre_vq_conv_bot(feat) <span class="hljs-comment"># nn.Conv2d</span><br><br>        <span class="hljs-comment"># Quantize F_Hi with K_Hi</span><br>        loss_b, quantized_b, perplexity_b, encodings_b = self._vq_vae_bot(zb) <span class="hljs-comment"># nn.Embedding</span><br><br>        <span class="hljs-comment"># Concatenate Q_Hi and Q_Lo and input it into the General appearance decoder</span><br>        quant_join = torch.cat((up_quantized_t, quantized_b), dim=<span class="hljs-number">1</span>)<br>        recon_fin = self._decoder_b(quant_join) <span class="hljs-comment"># nn.Conv2d + ResidualStack + 2 × nn.ConvTranspose2d</span><br><br>        <span class="hljs-comment"># return loss_b, loss_t, recon_fin, encodings_t, encodings_b, quantized_t, quantized_b</span><br>        <span class="hljs-keyword">return</span> loss_b, loss_t, recon_fin, quantized_t, quantized_b<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Depth-Aware Discrete Autoencoder (DADA) Training.</span><br>model = DiscreteLatentModel()<br><span class="hljs-keyword">for</span> sample_batched <span class="hljs-keyword">in</span> dataloader:<br>    depth_image = sample_batched[<span class="hljs-string">&quot;image&quot;</span>].cuda()<br>    rgb_image = sample_batched[<span class="hljs-string">&quot;rgb_image&quot;</span>].cuda()<br>    model_in = torch.cat((depth_image, rgb_image), dim=<span class="hljs-number">1</span>).<span class="hljs-built_in">float</span>()<br><br>    loss_b, loss_t, recon_out, _, _ = model(model_in)<br>    loss_vq = loss_b + loss_t<br>    <br><span class="hljs-comment"># L2 loss when input (depth) image only.</span><br>    recon_loss = torch.mean((model_in - recon_out)**<span class="hljs-number">2</span>)<br>    <span class="hljs-comment"># L1 loss when input (rgb + depth) images only(may work better and lead to improved reconstructions).</span><br>    recon_loss = torch.mean(torch.<span class="hljs-built_in">abs</span>(model_in - recon_out))<br>    <br>    recon_loss = recon_loss + loss_vq<br>    loss = recon_loss<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Dual Subspace Reprojection (DSR) Training.</span><br><span class="hljs-comment">## 1. Model Instantiation.</span><br>model = DiscreteLatentModel()<br><br><span class="hljs-comment">## 2. Modules using the codebooks K_hi and K_lo for feature quantization.</span><br>embedder_hi = model._vq_vae_bot<br>embedder_lo = model._vq_vae_top<br><br><span class="hljs-comment">## 3. Define the subspace restriction modules - Encoder decoder networks.</span><br>sub_res_model_lo = SubspaceRestrictionModule(embedding_size=embedding_dim)<br>sub_res_model_hi = SubspaceRestrictionModule(embedding_size=embedding_dim)<br><br><span class="hljs-comment"># 4. Define the anomaly detection module - UNet-based network.</span><br>decoder_seg = AnomalyDetectionModule(in_channels=<span class="hljs-number">2</span>, base_width=<span class="hljs-number">32</span>) <span class="hljs-comment"># U-Net</span><br><br><span class="hljs-comment">## 5. Image reconstruction network reconstructs the image from discrete features.</span><br><span class="hljs-comment">### It is trained for a specific object.</span><br>model_decode = ImageReconstructionNetwork()<br><br><span class="hljs-comment">## 6. Training.</span><br><span class="hljs-keyword">for</span> i_batch, (depth_image, rgb_image, anomaly_mask) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataloader):<br>    in_image = torch.cat((depth_image, rgb_image),dim=<span class="hljs-number">1</span>)<br><br>    anomaly_strength_lo = (torch.rand(in_image.shape[<span class="hljs-number">0</span>]) * <span class="hljs-number">0.90</span> + <span class="hljs-number">0.10</span>).cuda()<br>    anomaly_strength_hi = (torch.rand(in_image.shape[<span class="hljs-number">0</span>]) * <span class="hljs-number">0.90</span> + <span class="hljs-number">0.10</span>).cuda()<br>    <br>    <span class="hljs-comment">### (1) Extract features from the discrete model.</span><br>    enc_b, enc_t = model._encoder_b(in_image), model._encoder_t(enc_b)<br>    zt = model._pre_vq_conv_top(enc_t)<br><br>    <span class="hljs-comment">### (2) Quantize the extracted features.</span><br>    _, quantized_t, _, _ = embedder_lo(zt)<br><br>    <span class="hljs-comment">### (3) Quantize the features augmented with anomalies.</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Generate feature-based anomalies on low-level feature.&#x27;&#x27;&#x27;</span><br>    anomaly_embedding_lo = generate_fake_anomalies_joined(zt, quantized_t, embedder_lo, anomaly_strength_lo)<br>    <span class="hljs-string">&#x27;&#x27;&#x27;Upsample the quantized features augmented with anomalies&#x27;&#x27;&#x27;</span><br>    zb = model._pre_vq_conv_bot(torch.cat((enc_b, model.upsample_t(anomaly_embedding_lo)), dim=<span class="hljs-number">1</span>))<br>    <span class="hljs-string">&#x27;&#x27;&#x27;Upsample the extracted quantized features&#x27;&#x27;&#x27;</span><br>    zb_real = model._pre_vq_conv_bot(torch.cat((enc_b, model.upsample_t(quantized_t)), dim=<span class="hljs-number">1</span>))<br>    <span class="hljs-string">&#x27;&#x27;&#x27;Quantize the upsampled features - F_hi&#x27;&#x27;&#x27;</span><br>    _, quantized_b, _, _ = embedder_hi(zb)<br>    _, quantized_b_real, _, _ = embedder_hi(zb_real)<br><br>    <span class="hljs-comment">### (4) Generate feature-based anomalies on F_hi.</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Generate feature-based anomalies on low-level feature augmented with anomalies.&#x27;&#x27;&#x27;</span><br>    anomaly_embedding_hi = generate_fake_anomalies_joined(zb, quantized_b, embedder_hi, anomaly_strength_hi)<br><span class="hljs-string">&#x27;&#x27;&#x27;Generate feature-based anomalies on low-level feature.&#x27;&#x27;&#x27;</span><br>    anomaly_embedding_hi_usebot = generate_fake_anomalies_joined(zb_real, quantized_b_real, embedder_hi, anomaly_strength_hi)<br>    <br>    use_both = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>,(in_image.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br>    use_lo = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>,(in_image.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br>    use_hi = (<span class="hljs-number">1</span> - use_lo)<br>   <br>    anomaly_embedding_lo_usebot = quantized_t<br>    anomaly_embedding_hi_usetop = quantized_b_real<br>    anomaly_embedding_lo_usetop = anomaly_embedding_lo<br>    anomaly_embedding_hi_not_both =  (<span class="hljs-number">1</span> - use_lo) * anomaly_embedding_hi_usebot + use_lo * anomaly_embedding_hi_usetop<br>    anomaly_embedding_lo_not_both =  (<span class="hljs-number">1</span> - use_lo) * anomaly_embedding_lo_usebot + use_lo * anomaly_embedding_lo_usetop<br>    anomaly_embedding_hi = (anomaly_embedding_hi * use_both + anomaly_embedding_hi_not_both * (<span class="hljs-number">1.0</span> - use_both))<br>    anomaly_embedding_lo = (anomaly_embedding_lo * use_both + anomaly_embedding_lo_not_both * (<span class="hljs-number">1.0</span> - use_both))<br><br>    <span class="hljs-comment">### (5) Restore the features to normality with the Subspace restriction modules.</span><br>    recon_feat_hi, recon_embeddings_hi, _ = sub_res_model_hi(anomaly_embedding_hi, embedder_hi)<br>    recon_feat_lo, recon_embeddings_lo, _ = sub_res_model_lo(anomaly_embedding_lo, embedder_lo)<br><br>    <span class="hljs-comment">### (6) Reconstruct the image from the anomalous features with the general appearance decoder.</span><br>    up_quantized_anomaly_t = model.upsample_t(anomaly_embedding_lo)<br>    quant_join_anomaly = torch.cat((up_quantized_anomaly_t, anomaly_embedding_hi), dim=<span class="hljs-number">1</span>)<br>    recon_image_general = model._decoder_b(quant_join_anomaly)<br><br>    <span class="hljs-comment">### (7) Reconstruct the image with the object-specific image reconstruction module.</span><br>    up_quantized_recon_t = model.upsample_t(recon_embeddings_lo)<br>    quant_join = torch.cat((up_quantized_recon_t, recon_embeddings_hi), dim=<span class="hljs-number">1</span>)<br>    recon_image_recon = model_decode(quant_join)<br><br>    out_mask = decoder_seg(recon_image_recon,recon_image_general)<br>    out_mask_sm = torch.softmax(out_mask, dim=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment">### (8) Calculate losses</span><br>    loss_feat_hi = torch.nn.functional.mse_loss(recon_feat_hi, quantized_b_real.detach())<br>    loss_feat_lo = torch.nn.functional.mse_loss(recon_feat_lo, quantized_t.detach())<br>    loss_l2_recon_img = torch.nn.functional.mse_loss(in_image, recon_image_recon)<br>    total_recon_loss = loss_feat_lo + loss_feat_hi + loss_l2_recon_img*<span class="hljs-number">10</span><br><span class="hljs-string">&#x27;&#x27;&#x27;Resize the ground truth anomaly map to closely match the augmented features&#x27;&#x27;&#x27;</span><br>    down_ratio_x_hi = <span class="hljs-built_in">int</span>(anomaly_mask.shape[<span class="hljs-number">3</span>] / quantized_b.shape[<span class="hljs-number">3</span>])<br>    anomaly_mask_hi = max_pool2d(anomaly_mask, (down_ratio_x_hi, down_ratio_x_hi))<br>    down_ratio_x_lo = <span class="hljs-built_in">int</span>(anomaly_mask.shape[<span class="hljs-number">3</span>] / quantized_t.shape[<span class="hljs-number">3</span>])<br>    anomaly_mask_lo = max_pool2d(anomaly_mask, (down_ratio_x_lo, down_ratio_x_lo))<br>    anomaly_mask = anomaly_mask_lo * use_both + (anomaly_mask_lo * use_lo + anomaly_mask_hi * use_hi) * (<span class="hljs-number">1.0</span> - use_both)<br>    <span class="hljs-string">&#x27;&#x27;&#x27;Calculate the segmentation loss (# L1 may improve results in some cases)&#x27;&#x27;&#x27;</span><br>    segment_loss = loss_focal(out_mask_sm, anomaly_mask)<br>    l1_mask_loss = torch.mean(torch.<span class="hljs-built_in">abs</span>(out_mask_sm - torch.cat((<span class="hljs-number">1.0</span> - anomaly_mask, anomaly_mask), dim=<span class="hljs-number">1</span>)))<br>    segment_loss = segment_loss + l1_mask_loss<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Dual Subspace Reprojection (DSR) Inference.</span><br><span class="hljs-comment">## 1. Model Instantiation.</span><br><span class="hljs-string">&#x27;&#x27;&#x27;Import subspace restriction modules - Encoder decoder networks.&#x27;&#x27;&#x27;</span><br>sub_res_model_lo = SubspaceRestrictionModule() <span class="hljs-comment"># Load checkpoints then.</span><br>sub_res_model_hi = SubspaceRestrictionModule() <span class="hljs-comment"># Load checkpoints then.</span><br><span class="hljs-string">&#x27;&#x27;&#x27;Import anomaly detection module - UNet-based network&#x27;&#x27;&#x27;</span><br>decoder_seg = AnomalyDetectionModule() <span class="hljs-comment"># Load checkpoints then.</span><br><span class="hljs-string">&#x27;&#x27;&#x27;Import image reconstruction network&#x27;&#x27;&#x27;</span><br>model_decode = ImageReconstructionNetwork() <span class="hljs-comment"># Load checkpoints then.</span><br><br><span class="hljs-comment">## 2. Inference.</span><br><span class="hljs-keyword">for</span> i_batch, (depth_image, rgb_image) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataloader):<br>    in_image = torch.cat((depth_image, rgb_image), dim=<span class="hljs-number">1</span>)<br>    <br>    <span class="hljs-comment">### (1) Extract features from the discrete model.</span><br>    _, _, recon_out, embeddings_lo, embeddings_hi = model(in_image)<br>    recon_image_general = recon_out<br>    _, recon_embeddings_hi, _ = sub_res_model_hi(embeddings_hi, embedder_hi)<br>    _, recon_embeddings_lo, _ = sub_res_model_lo(embeddings_lo, embedder_lo)<br>    <br>    <span class="hljs-comment">### (2) Reconstruct the image with the object-specific image reconstruction module</span><br>    up_quantized_recon_t = model.upsample_t(recon_embeddings_lo)<br>    quant_join = torch.cat((up_quantized_recon_t, recon_embeddings_hi), dim=<span class="hljs-number">1</span>)<br>    recon_image_recon = model_decode(quant_join)<br>    <br>    <span class="hljs-comment">### (3) Generate the anomaly segmentation map</span><br>    out_mask = decoder_seg(recon_image_recon, recon_image_general)<br>    out_mask_sm = softmax(out_mask, dim=<span class="hljs-number">1</span>)<br>    out_mask_averaged = avg_pool2d(out_mask_sm[:, <span class="hljs-number">1</span>:, :, :], <span class="hljs-number">11</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">11</span> // <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h3 id="Multi-Modal-Reverse-Distillation-MMRD"><a href="#Multi-Modal-Reverse-Distillation-MMRD" class="headerlink" title="Multi-Modal Reverse Distillation (MMRD)"></a>Multi-Modal Reverse Distillation (MMRD)</h3><p>论文：Rethinking Reverse Distillation for Multi-Modal Anomaly Detection (AAAI 2024)</p><p><img src="C:\Users\SvyJ\AppData\Roaming\Typora\typora-user-images\image-20240930113716721.png" alt="image-20240930113716721"></p><h3 id="Dual-modality-Anomaly-Synthesis-DAS3D"><a href="#Dual-modality-Anomaly-Synthesis-DAS3D" class="headerlink" title="Dual-modality Anomaly Synthesis (DAS3D)"></a>Dual-modality Anomaly Synthesis (DAS3D)</h3><p>论文：DAS3D: Dual-modality Anomaly Synthesis for 3D Anomaly Detection (arXiv 2024)</p><p><img src="C:\Users\SvyJ\AppData\Roaming\Typora\typora-user-images\image-20241106163312867.png" alt="image-20241106163312867"></p><h2 id="检测算法（基于-Text-RGB-Point-Cloud）"><a href="#检测算法（基于-Text-RGB-Point-Cloud）" class="headerlink" title="检测算法（基于 Text + RGB + Point Cloud）"></a>检测算法（基于 Text + RGB + Point Cloud）</h2><h3 id="Noisy-Resistant-Multi-3D-Memory-M3DM-NR"><a href="#Noisy-Resistant-Multi-3D-Memory-M3DM-NR" class="headerlink" title="Noisy-Resistant Multi-3D-Memory (M3DM-NR)"></a>Noisy-Resistant Multi-3D-Memory (M3DM-NR)</h3><p>论文：M3DM-NR: RGB-3D Noisy-Resistant Industrial Anomaly Detection via Multimodal Denoising (arXiv 2024) </p><p><img src="C:\Users\SvyJ\AppData\Roaming\Typora\typora-user-images\image-20241106162144247.png" alt="image-20241106162144247"></p><h2 id="结果现状"><a href="#结果现状" class="headerlink" title="结果现状"></a>结果现状</h2><h3 id="Image-AUROC"><a href="#Image-AUROC" class="headerlink" title="Image-AUROC"></a>Image-AUROC</h3><table><thead><tr><th>Method</th><th>Pubication</th><th>Bagel</th><th>Cable Gland</th><th>Carrot</th><th>Cookie</th><th>Dowel</th><th>Foam</th><th>Peach</th><th>Potato</th><th>Rope</th><th>Tire</th><th>Mean</th></tr></thead><tbody><tr><td>DepthGAN [1]</td><td>VISIGRAPP’22</td><td>0.538</td><td>0.372</td><td>0.580</td><td>0.603</td><td>0.430</td><td>0.534</td><td>0.642</td><td>0.601</td><td>0.443</td><td>0.577</td><td>0.532</td></tr><tr><td>DepthAE [1]</td><td>VISIGRAPP’22</td><td>0.648</td><td>0.502</td><td>0.650</td><td>0.488</td><td>0.805</td><td>0.522</td><td>0.712</td><td>0.529</td><td>0.540</td><td>0.552</td><td>0.595</td></tr><tr><td>DepthVM [1]</td><td>VISIGRAPP’22</td><td>0.513</td><td>0.551</td><td>0.477</td><td>0.581</td><td>0.617</td><td>0.716</td><td>0.450</td><td>0.421</td><td>0.598</td><td>0.623</td><td>0.555</td></tr><tr><td>VoxelGAN [1]</td><td>VISIGRAPP’22</td><td>0.680</td><td>0.324</td><td>0.565</td><td>0.399</td><td>0.497</td><td>0.482</td><td>0.566</td><td>0.579</td><td>0.601</td><td>0.482</td><td>0.518</td></tr><tr><td>VoxelAE [1]</td><td>VISIGRAPP’22</td><td>0.510</td><td>0.540</td><td>0.384</td><td>0.693</td><td>0.446</td><td>0.632</td><td>0.550</td><td>0.494</td><td>0.721</td><td>0.413</td><td>0.538</td></tr><tr><td>VoxelVM [1]</td><td>VISIGRAPP’22</td><td>0.553</td><td>0.772</td><td>0.484</td><td>0.701</td><td>0.751</td><td>0.578</td><td>0.480</td><td>0.466</td><td>0.689</td><td>0.611</td><td>0.609</td></tr><tr><td>3D-ST [2]</td><td>WACV’23</td><td>0.950</td><td>0.483</td><td>0.986</td><td>0.921</td><td>0.905</td><td>0.632</td><td>0.945</td><td>0.988</td><td>0.976</td><td>0.542</td><td>0.833</td></tr><tr><td>BTF [3]</td><td>CVPR’23</td><td>0.918</td><td>0.748</td><td>0.967</td><td>0.883</td><td>0.932</td><td>0.582</td><td>0.896</td><td>0.912</td><td>0.921</td><td>0.886</td><td>0.865</td></tr><tr><td>EasyNet [4]</td><td>MM’23</td><td>0.991</td><td><strong>0.998</strong></td><td>0.918</td><td>0.968</td><td>0.945</td><td>0.945</td><td>0.905</td><td>0.807</td><td>0.994</td><td>0.793</td><td>0.926</td></tr><tr><td>AST [5]</td><td>WACV’23</td><td>0.983</td><td>0.873</td><td>0.976</td><td>0.971</td><td>0.932</td><td>0.885</td><td>0.974</td><td>0.981</td><td>1.000</td><td>0.797</td><td>0.937</td></tr><tr><td>CMDIAD [6]</td><td>arXiv’24</td><td>0.992</td><td>0.893</td><td>0.977</td><td>0.960</td><td>0.953</td><td>0.883</td><td>0.950</td><td>0.937</td><td>0.943</td><td>0.893</td><td>0.938</td></tr><tr><td>M3DM [7]</td><td>CVPR’23</td><td>0.994</td><td>0.909</td><td>0.972</td><td>0.976</td><td>0.960</td><td>0.942</td><td>0.973</td><td>0.899</td><td>0.972</td><td>0.850</td><td>0.945</td></tr><tr><td>M3DM-NR [8]</td><td>arXiv’24</td><td>0.993</td><td>0.911</td><td>0.977</td><td>0.976</td><td>0.960</td><td>0.922</td><td>0.973</td><td>0.899</td><td>0.955</td><td>0.882</td><td>0.945</td></tr><tr><td>Shape-Guided [9]</td><td>ICML’23</td><td>0.986</td><td>0.894</td><td>0.983</td><td>0.991</td><td>0.976</td><td>0.857</td><td><strong>0.990</strong></td><td>0.965</td><td>0.960</td><td>0.869</td><td>0.947</td></tr><tr><td>MMRD [10]</td><td>AAAI’24</td><td><strong>0.999</strong></td><td>0.943</td><td>0.964</td><td>0.943</td><td>0.992</td><td>0.912</td><td>0.949</td><td>0.901</td><td>0.994</td><td>0.901</td><td>0.950</td></tr><tr><td>CPMF [11]</td><td>PR’24</td><td>0.983</td><td>0.889</td><td>0.989</td><td>0.991</td><td>0.958</td><td>0.802</td><td>0.988</td><td>0.959</td><td>0.979</td><td>0.969</td><td>0.951</td></tr><tr><td>CFM [12]</td><td>CVPR’24</td><td>0.994</td><td>0.888</td><td>0.984</td><td><strong>0.993</strong></td><td>0.980</td><td>0.888</td><td>0.941</td><td>0.943</td><td>0.980</td><td>0.953</td><td>0.954</td></tr><tr><td>LSFA [13]</td><td>arXiv’24</td><td>1.000</td><td>0.939</td><td>0.982</td><td>0.989</td><td>0.961</td><td>0.951</td><td>0.983</td><td>0.962</td><td>0.989</td><td>0.951</td><td>0.971</td></tr><tr><td>3DSR [14]</td><td>WACV’24</td><td>0.981</td><td>0.867</td><td>0.996</td><td>0.981</td><td><strong>1.000</strong></td><td>0.994</td><td>0.986</td><td><strong>0.978</strong></td><td><strong>1.000</strong></td><td><strong>0.995</strong></td><td>0.978</td></tr><tr><td>DAS3D [15]</td><td>arXiv’24</td><td>0.997</td><td>0.973</td><td><strong>0.999</strong></td><td>0.992</td><td>0.970</td><td><strong>0.995</strong></td><td>0.962</td><td>0.954</td><td>0.998</td><td>0.977</td><td><strong>0.982</strong></td></tr></tbody></table><h3 id="Pixel-AUROC"><a href="#Pixel-AUROC" class="headerlink" title="Pixel-AUROC"></a>Pixel-AUROC</h3><table><thead><tr><th>Method</th><th>Pubication</th><th>Bagel</th><th>Cable Gland</th><th>Carrot</th><th>Cookie</th><th>Dowel</th><th>Foam</th><th>Peach</th><th>Potato</th><th>Rope</th><th>Tire</th><th>Mean</th></tr></thead><tbody><tr><td>AST [5]</td><td>WACV’23</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.976</td></tr><tr><td>CMDIAD [6]</td><td>arXiv’24</td><td>0.995</td><td>0.993</td><td>0.996</td><td>0.976</td><td>0.984</td><td>0.988</td><td>0.996</td><td>0.995</td><td>0.997</td><td>0.996</td><td>0.992</td></tr><tr><td>BTF [3]</td><td>CVPR’23</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.992</td></tr><tr><td>M3DM [7]</td><td>CVPR’23</td><td>0.995</td><td>0.993</td><td>0.997</td><td>0.979</td><td>0.985</td><td>0.989</td><td>0.996</td><td>0.994</td><td>0.997</td><td>0.996</td><td>0.992</td></tr><tr><td>M3DM-NR [8]</td><td>arXiv’24</td><td>0.996</td><td>0.993</td><td>0.997</td><td>0.979</td><td>0.985</td><td>0.989</td><td>0.996</td><td>0.995</td><td>0.997</td><td>0.996</td><td>0.992</td></tr><tr><td>CFM [12]</td><td>CVPR’24</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.993</td></tr><tr><td>DAS3D [15]</td><td>arXiv’24</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.993</td></tr><tr><td>3DSR [14]</td><td>WACV’24</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td><strong>0.995</strong></td></tr></tbody></table><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Bergmann P, Jin X, Sattlegger D, et al. The mvtec 3d-ad dataset for unsupervised 3d anomaly detection and localization[J]. <em>arXiv preprint arXiv:2112.09045</em>, 2021.</p><p>[2] Bergmann P, Sattlegger D. Anomaly detection in 3d point clouds using deep geometric descriptors[C]&#x2F;&#x2F;<em>Proceedings of the IEEE&#x2F;CVF Winter Conference on Applications of Computer Vision</em>. 2023: 2613-2623.</p><p>[3] Horwitz E, Hoshen Y. Back to the feature: classical 3d features are (almost) all you need for 3d anomaly detection[C]&#x2F;&#x2F;<em>Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition Workshops</em>. 2023: 2968-2977.</p><p>[4] Chen R, Xie G, Liu J, et al. Easynet: An easy network for 3d industrial anomaly detection[C]&#x2F;&#x2F;<em>Proceedings of the 31st ACM International Conference on Multimedia</em>. 2023: 7038-7046.</p><p>[5] Rudolph M, Wehrbein T, Rosenhahn B, et al. Asymmetric student-teacher networks for industrial anomaly detection[C]&#x2F;&#x2F;<em>Proceedings of the IEEE&#x2F;CVF winter conference on applications of computer vision</em>. 2023: 2592-2602.</p><p>[6] Sui W, Lichau D, Lefèvre J, et al. Incomplete Multimodal Industrial Anomaly Detection via Cross-Modal Distillation[J]. <em>arXiv preprint arXiv:2405.13571</em>, 2024.</p><p>[7] Wang Y, Peng J, Zhang J, et al. Multimodal industrial anomaly detection via hybrid fusion[C]&#x2F;&#x2F;<em>Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition</em>. 2023: 8032-8041.</p><p>[8] Wang C, Zhu H, Peng J, et al. M3DM-NR: RGB-3D Noisy-Resistant Industrial Anomaly Detection via Multimodal Denoising[J]. <em>arXiv preprint arXiv:2406.02263</em>, 2024.</p><p>[9] Chu Y M, Liu C, Hsieh T I, et al. Shape-Guided Dual-Memory Learning for 3D Anomaly Detection[C]&#x2F;&#x2F;<em>International Conference on Machine Learning</em>. PMLR, 2023: 6185-6194.</p><p>[10] Gu Z, Zhang J, Liu L, et al. Rethinking Reverse Distillation for Multi-Modal Anomaly Detection[C]&#x2F;&#x2F;Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(8): 8445-8453.</p><p>[11] Cao Y, Xu X, Shen W. Complementary pseudo multimodal feature for point cloud anomaly detection[J]. <em>Pattern Recognition</em>, 2024, 156: 110761.</p><p>[12] Costanzino A, Ramirez P Z, Lisanti G, et al. Multimodal industrial anomaly detection by crossmodal feature mapping[C]&#x2F;&#x2F;<em>Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition</em>. 2024: 17234-17243.</p><p>[13] Tu Y, Zhang B, Liu L, et al. Self-supervised Feature Adaptation for 3D Industrial Anomaly Detection[J]. <em>arXiv preprint arXiv:2401.03145</em>, 2024.</p><p>[14] Zavrtanik V, Kristan M, Skočaj D. Cheating depth: Enhancing 3d surface anomaly detection via depth simulation[C]&#x2F;&#x2F;<em>Proceedings of the IEEE&#x2F;CVF Winter Conference on Applications of Computer Vision</em>. 2024: 2164-2172.</p><p>[15] Li K, Dai B, Fu J, et al. DAS3D: Dual-modality Anomaly Synthesis for 3D Anomaly Detection[J]. <em>arXiv preprint arXiv:2410.09821</em>, 2024.</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>少样本学习（Few Shot Learning）</title>
      <link href="/2023/05/03/037-FewShotLearning/"/>
      <url>/2023/05/03/037-FewShotLearning/</url>
      
        <content type="html"><![CDATA[<p>挖坑待填～</p><p>开始之前，我们先了解一下什么是元学习（Meta Lerrning）？</p><h1 id="Meta-Learning"><a href="#Meta-Learning" class="headerlink" title="Meta Learning"></a>Meta Learning</h1>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>M3DM复现记录</title>
      <link href="/2023/05/03/038-M3DM/"/>
      <url>/2023/05/03/038-M3DM/</url>
      
        <content type="html"><![CDATA[<h2 id="1-创建-python3-9-环境"><a href="#1-创建-python3-9-环境" class="headerlink" title="1 创建 python3.9 环境"></a>1 创建 <code>python3.9</code> 环境</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda create -n pointnet2_ops python=3.9<br><span class="hljs-built_in">source</span> activate pointnet2_ops<br></code></pre></td></tr></table></figure><h2 id="2-更改-nvcc-版本"><a href="#2-更改-nvcc-版本" class="headerlink" title="2 更改 nvcc 版本"></a>2 更改 <code>nvcc</code> 版本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /M3DM<br>conda search cuda-nvcc -c nvidia<br>conda install cuda-nvcc=11.3.58 -c nvidia<br>nvcc -V<br></code></pre></td></tr></table></figure><h2 id="3-安装-pytorch"><a href="#3-安装-pytorch" class="headerlink" title="3 安装 pytorch"></a>3 安装 <code>pytorch</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html<br></code></pre></td></tr></table></figure><p>或 （为使用 <code>torch.frombuffer</code>，推荐兼容性更高的新版本）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113<br></code></pre></td></tr></table></figure><h2 id="4-安装通用工具包"><a href="#4-安装通用工具包" class="headerlink" title="4 安装通用工具包"></a>4 安装通用工具包</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install -r requirements.txt<br>pip install numpy==1.26 (要求python≥3.9)<br>pip install ninja<br>pip install open3d (要求python≤3.11)<br></code></pre></td></tr></table></figure><h2 id="5-安装老版本-GCC-≤8"><a href="#5-安装老版本-GCC-≤8" class="headerlink" title="5 安装老版本 GCC (≤8)"></a>5 安装老版本 <code>GCC (≤8)</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt-get install g++-8<br>sudo apt-get install gcc-8<br>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 20<br>sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-8 20<br></code></pre></td></tr></table></figure><h2 id="6-安装-pointnet2-ops-lib"><a href="#6-安装-pointnet2-ops-lib" class="headerlink" title="6 安装 pointnet2_ops_lib"></a>6 安装 <code>pointnet2_ops_lib</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install --upgrade pip<br>pip install --upgrade setuptools<br>pip install Pointnet2_PyTorch/pointnet2_ops_lib/.<br>Successfully built pointnet2-ops<br>Installing collected packages: pointnet2-ops<br>Successfully installed pointnet2-ops-3.0.0<br></code></pre></td></tr></table></figure><h2 id="7-安装-KNN-CUDA"><a href="#7-安装-KNN-CUDA" class="headerlink" title="7 安装 KNN-CUDA"></a>7 安装 <code>KNN-CUDA</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> packages<br>wget https://github.com/unlimblue/KNN_CUDA/releases/download/0.2/KNN_CUDA-0.2-py3-none-any.whl<br>pip install pakages/KNN_CUDA-0.2-py3-none-any.whl<br></code></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Wang Y, Peng J, Zhang J, et al. Multimodal industrial anomaly detection via hybrid fusion[C]&#x2F;&#x2F;Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition. 2023: 8032-8041.<br>[2] Costanzino A, Ramirez P Z, Lisanti G, et al. Multimodal industrial anomaly detection by crossmodal feature mapping[C]&#x2F;&#x2F;Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition. 2024: 17234-17243.<br>[3] Chu Y M, Liu C, Hsieh T I, et al. Shape-Guided Dual-Memory Learning for 3D Anomaly Detection[C]&#x2F;&#x2F;International Conference on Machine Learning. PMLR, 2023: 6185-6194.</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023博士申请记录</title>
      <link href="/2023/04/03/036-DoctorApplication/"/>
      <url>/2023/04/03/036-DoctorApplication/</url>
      
        <content type="html"><![CDATA[<h1 id="个人背景"><a href="#个人背景" class="headerlink" title="个人背景"></a>个人背景</h1><p>本硕学校：湘潭大学，<br>研究方向：计算机视觉方向<br>论文情况：1篇中科院二区TOP、1篇CCF A中文（均为导师一作，本人二作）<br>硕士绩点：3.42&#x2F;4，排名第一<br>获奖情况：国家奖学金、校长奖学金、特等学业奖学金（两次）<br>外语水平：英语六级474</p><h1 id="选择学校"><a href="#选择学校" class="headerlink" title="选择学校"></a>选择学校</h1><p>中山大学<br>中南大学<br>湖南大学<br>华南理工大学<br>武汉大学<br>华中科技大学<br>重庆大学</p><h1 id="联系导师"><a href="#联系导师" class="headerlink" title="联系导师"></a>联系导师</h1><p><strong>套磁邮件</strong></p><blockquote><p>邮件内容：<br>尊敬的X教授，<br>    您好！我叫蒋帅，来自湘潭大学智能计算与信息处理教育部重实验室，目前是计算机技术专业（专硕）的研三学生，硕士期间的研究方向是深度学习和医学图像分析，我想知道您目前是否还有博士招生的名额，希望有机会能够成为您的博士研究生。<br>    以下是本人攻读硕士学位期间的学习经历和科研成果简述：<br>        1）学习方面，本人连续三年在学院的综合测评中排名专业第一，并获国家奖学金、湘潭大学特等奖学金（连续两年）和“湘潭大学三好研究生”称号。<br>        2）科研方面，本人作为骨干成员参与国家自然科学基金青年项目2项、湖南省教育厅优秀青年基金2项。在国内外期刊上发表学术论文2篇，其中中科院二区论文1篇，CCF A类中文期刊论文1篇（均为导师一作，本人二作）。<br>    感谢您在百忙之中抽出时间阅信，非常期待您的回信。个人简历添加至邮件附件，烦请您查阅。<br>    祝老师工作顺利，心想事成！预祝新春快乐！<br>蒋帅</p></blockquote><h1 id="材料准备"><a href="#材料准备" class="headerlink" title="材料准备"></a>材料准备</h1><h1 id="复试准备"><a href="#复试准备" class="headerlink" title="复试准备"></a>复试准备</h1><h2 id="笔试（机试）"><a href="#笔试（机试）" class="headerlink" title="笔试（机试）"></a>笔试（机试）</h2><p>湖南大学：CCF CSP认证考试系统 <a href="http://118.190.20.162/home.page">http://118.190.20.162/home.page</a> </p><h2 id="英语"><a href="#英语" class="headerlink" title="英语"></a>英语</h2><p><strong>自我介绍</strong></p><blockquote><p>Good morning or afternoon, professors! As a Frequent Visitor of Hunan University, it is my great honor and pleasure to come here Again, but this time in the Most Important Identity. Now let me Introduce Myself Briefly.<br><strong><em>Personal Information</em></strong><br>My name is Jiang Shuai, a third-year graduate student majoring in Computer Technology at Xiangtan University. I have been studying and researching at the School of Computer Science of Xiangtan University for Seven Years to pursued my bachelor’s and master’s degrees. Within seven years, my degree course grades and rankings have been among the top, with my major ranking First during my graduate studies. Furthermore, I have served as the Chairman of the Computer Association, accumulating a wealth of professional knowledge during my tenure([ˈtenjər]). Over the past seven years, I have received multiple commendations, including outstanding student association workers , special scholarship and National Scholarships.<br><strong><em>Research Experience</em></strong><br>Actually, I have been engaged in research on Computer Vision since the fourth year of my undergraduate studies, and my undergraduate projects and master’s thesis are both in the field of computer vision. My current research interests focus on computer vision based on Deep Learning and its applications in Medical Image Analysis., particularly in Ophthalmology([ˌɑːfθəˈmɑːlədʒi]), including OCT and OCTA. The research content includes designing more efficient and accurate biomarker segmentation methods from 2-D and 3-D perspectives, aiming at providing references for Clinical Diagnosis([ˌdaɪəɡˈnoʊsɪs]) and Treatment. After long-term research, I have Published Two Papers in the field of Medical Imaging, including IEEE Transactions on Instrumentation and Measurement. My four years of research in computer vision have given me a comprehensive understanding of the field, so I hope to have Further Development and Research in this direction, this is one of the most important reasons why I chose to apply for a Doctoral Entrance Opportunity.<br><strong><em>Research Plans</em></strong><br>I’m really looking forward to being admitted and carrying out my doctoral study under Professor Liu’s Supervision. If accepted as a Ph.D. Candidate, I would like to Continue my research work in the field of Computer Vision, with a particular focus on Defect Detection in Industrial Vision. Considering that in practical industrial scenarios([səˈnærioʊ]), defect samples are often difficult to collect and have a very high cost of annotation, the design of defect detection methods would be based on Few-Shot Learning.<br>That’s all of my self-introduction. Thank you for your listening.</p></blockquote><h2 id="综合面试"><a href="#综合面试" class="headerlink" title="综合面试"></a>综合面试</h2>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>2022 秋招</title>
      <link href="/2022/09/28/035-Autumn_Recruiment_Summary/"/>
      <url>/2022/09/28/035-Autumn_Recruiment_Summary/</url>
      
        <content type="html"><![CDATA[<p>以本文记录本人在互联网寒冬中的第一次秋招经历。</p><p>写在前面：<br>Offer 机会：宣讲会 &gt; 官网投递 &gt; 第三方招聘平台<br>面试机会：宣讲会 &gt; 第三方招聘平台 &gt; 官网投递</p><hr><h1 id="什么时候开始？"><a href="#什么时候开始？" class="headerlink" title="什么时候开始？"></a>什么时候开始？</h1><p><strong><code>8</code> 月底实验室集体放了个假，想着回学校还能再观望观望，当时可能还没意识到 <code>2022</code> 年互联网行业招聘的寒气，觉得好像金 <code>9</code> 银 <code>10</code> 的传统还会保持下去。其中极大一部分原因还是因为身边的同学和朋友都没怎么开始找工作，只有儒哥在卷提前批而且还拿到了蔚来的 <code>offer</code>。</strong></p><p><strong>直到同班同学在提前批也拿了 <code>offer</code>，加上这个过程中在脉脉、牛客这些论坛上看到双 <code>985</code> 都找不到工作（制造焦虑），才觉得自己应该也要开始找工作了。</strong></p><h1 id="简历投递"><a href="#简历投递" class="headerlink" title="简历投递"></a>简历投递</h1><h2 id="试探"><a href="#试探" class="headerlink" title="试探"></a>试探</h2><p><strong><code>9</code> 月的第一周，抱着给同门试水的心态投了若干家北京、上海和杭州等地的公司。</strong></p><p><strong>第一家投的网易云音乐，很快就发来了笔试邀请。在 <code>A</code> 了 <code>3/4</code> 的情况下挂了笔试。</strong></p><p><img src="https://s2.loli.net/2022/09/25/P4G2M5ycgXWVv3a.png"></p><p><strong>之后又陆续投了 OPPO、VIVO、京东、小红书、字节等等一系列大厂，不出意外地过不了初筛。</strong></p><h2 id="海投"><a href="#海投" class="headerlink" title="海投"></a>海投</h2><p><strong>认清现实，开始海投~</strong></p><h1 id="笔试"><a href="#笔试" class="headerlink" title="笔试"></a>笔试</h1><h1 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h1><p><strong>大多数面试的公司问的问题都挺水的，放一些些微有技术含量的。<br>（以下仅为技术面能够回忆起来的部分（差不多就是能答上来的），毕竟只要技术面过了，HR 面大概率就是走流程了）</strong></p><h2 id="三诺生物"><a href="#三诺生物" class="headerlink" title="三诺生物"></a>三诺生物</h2><h2 id="万兴科技（40min）"><a href="#万兴科技（40min）" class="headerlink" title="万兴科技（40min）"></a>万兴科技（40min）</h2><details><summary>python 相关（点击展开）</summary><pre><code>Python 深拷贝和浅拷贝区别Python 多线程介绍一下类的继承手撕代码：无序列表找出前 K 个最大元素，要求 O(1) 空间复杂度，提示双指针</code></pre></details><details><summary>项目相关（点击展开）</summary><pre><code>介绍论文，手写细节编码器的选择？OCTA 图像的特点和原理？论文创新点和出发点？编解码架构的 backbone 选择？编码器在有插入中间层的情况下怎么加载预训练模型？介绍感受野有效感受野和理论感受野，以及二者的关系？损失函数的设计细节和优化方法，手写公式表示Adam 优化器的参数更新，手动推导Adam 后续如何解决梯度消失问题的？</code></pre></details><details><summary>反问环节（点击展开）</summary><pre><code>图像算法岗位的工作内容，偏图像还是视频？抠图软件的技术实现方向，图像分割？图像分割组人员规模？有没有专门做科研发论文的部门？</code></pre></details><h2 id="长沙北斗研究院（20min）"><a href="#长沙北斗研究院（20min）" class="headerlink" title="长沙北斗研究院（20min）"></a>长沙北斗研究院（20min）</h2><details><summary>问答环节（点击展开）</summary><pre><code>Python GIL 了解过吗？对 Python 的了解程度，举例说明一下Python 的自建库用过哪些？介绍一下项目对 Pytorch 的熟练程度，用过 Pytorch 中哪些库用过哪些图形计算库</code></pre></details><h2 id="兴盛优选"><a href="#兴盛优选" class="headerlink" title="兴盛优选"></a>兴盛优选</h2><h3 id="一面（36min）"><a href="#一面（36min）" class="headerlink" title="一面（36min）"></a>一面（36min）</h3><details><summary>做题环节（点击展开）</summary><pre><code>30 的阶乘后面有多少个 0，讲一下思路从初始字符串编辑（删除、插入或替换操作）至目标字符串需要的最小编辑距离（Leetcode No.72）</code></pre></details><details><summary>问答环节（点击展开）</summary><pre><code>讲一下选择排序，插入排序，归并排序和希尔排序的思路分析一下序列的有序程度对排序算法的复杂度影响了解过哪些推荐算法有了解 Boosting 或者 GBDT 吗讲一下 SVM 的思路讲一下 SVM 求解过程讲一下逻辑回归逻辑回归模型能否处理连续型变量Logistic 如何体现的用过哪些模型优化方法讲一下 SGD 和 Adam，以及它们的区别树模型的优化方法有哪些怎么解决过拟合问题机器学习中用过哪些评价指标介绍一下 AUCROC 曲线的绘制方式讲一下 TransformerTransformer 中有哪些重要组成部分，以及他们的作用是什么讲一下多头自注意力过程经过多头自注意力的序列信息发生了哪些变化，序列维度是否改变GRU 如何应用到图像数据的讲一下RNN，LSTM，GRU，以及它们的区别和改进的地方</code></pre></details><details><summary>反问环节</summary></details><h3 id="二面（48min）"><a href="#二面（48min）" class="headerlink" title="二面（48min）"></a>二面（48min）</h3><details><summary>问答环节（点击展开）</summary><pre><code>30 的阶乘后面有多少个 0编辑距离的细节解释，dp 数组对角线和非对角线元素的意义（哪个表示插入，删除和替换操作）防止过拟合的方法偏差和方差的区别ROC 曲线和 P-R 曲线的绘制，以及二者的区别如何判断用户搜索关键词的意图（推荐算法）SEO 优化和 MD5 函数如何实现图片搜索图片的过程介绍一下两个工作中的创新点设计的出发点以及灵感来源介绍学校社团工作经历有没有给自己制定目标，以及如何实现的，具体一点</code></pre></details><details><summary>反问环节（点击展开）</summary><pre><code>后续还有几轮面试推荐算法工作涉及到大量的用户数据，工作是否有大数据处理相关的内容胜任推荐算法的工作，需要首先了解哪方面的知识</code></pre></details><h2 id="长沙智能驾驶研究院（CIDI）（62min）"><a href="#长沙智能驾驶研究院（CIDI）（62min）" class="headerlink" title="长沙智能驾驶研究院（CIDI）（62min）"></a>长沙智能驾驶研究院（CIDI）（62min）</h2><details><summary>问答环节（点击展开）</summary><pre><code>介绍一下第一个项目什么是联合分割介绍一下可变形卷积可变形卷积中为什么要用双线性插值介绍一下空洞卷积和转置卷积，及二者的区别什么是感受野为什么两个 3×3 卷积可以等价于一个 5×5 卷积图像分辨率大小对分割精度的影响介绍一下第二个项目OCT 和 OCTA 的区别什么是图像中的断层现象什么是图像块，如何采集的GRU 如何作用到图像的GRU 有几个门，跟 LSTM 等的区别介绍一下第三个项目介绍 W-GAN，生成对抗如何用于分割任务的如何解决数据集少时，GAN 用于生成数据出现的过拟合的问题深度学习中的正则化方法图像任务中特有的正则化方法（提示：卷积层中的，提示 DropBlock）介绍一下 TransformerQKV 机制和自注意力的计算介绍一下多头自注意力多头自注意力的结果如何合并的介绍一下 SVM如何找到 SVM 的超平面有哪些核函数介绍一下 K-means 的过程了解过哪些目标检测方法，介绍一下 YOLOV1什么是正样本和负样本YOLO 的损失函数组成介绍一下做的车牌检测任务了解哪些优化方法讲一下批量梯度下降，与随机梯度下降的区别Python 深拷贝和浅拷贝，深拷贝如何实现介绍一下 pytorch 的两种训练方式：数据并行和模型并行</code></pre></details>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>For Offersssss! （数据结构与算法篇）</title>
      <link href="/2022/09/25/030-InterviewQs_Leetcode/"/>
      <url>/2022/09/25/030-InterviewQs_Leetcode/</url>
      
        <content type="html"><![CDATA[<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><h2 id="顺序表"><a href="#顺序表" class="headerlink" title="顺序表"></a>顺序表</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs CPP"><span class="hljs-comment">// 动态数组</span><br>vector&lt;<span class="hljs-type">int</span>&gt; a;<br>a.<span class="hljs-built_in">push_back</span>(x);                      <span class="hljs-comment">// 添加元素 </span><br><span class="hljs-comment">// 以下方法均不包含右端点</span><br><span class="hljs-built_in">sort</span>(a.<span class="hljs-built_in">begin</span>(),a.<span class="hljs-built_in">end</span>());             <span class="hljs-comment">//从a.begin()到a.end()从小到大排列</span><br><span class="hljs-built_in">reverse</span>(a.<span class="hljs-built_in">begin</span>(),a.<span class="hljs-built_in">end</span>());          <span class="hljs-comment">//从a.begin()到a.end()的元素倒置，但不排列</span><br><span class="hljs-built_in">copy</span>(a.<span class="hljs-built_in">begin</span>(),a.<span class="hljs-built_in">end</span>(),b.<span class="hljs-built_in">begin</span>()+<span class="hljs-number">1</span>); <span class="hljs-comment">//从a.begin()到a.end()的元素复制到b中，从b.begin()+1的位置（包括它）开始复制，覆盖掉原有元素</span><br><span class="hljs-built_in">find</span>(a.<span class="hljs-built_in">begin</span>(),a.<span class="hljs-built_in">end</span>(),<span class="hljs-number">10</span>);          <span class="hljs-comment">//从a.begin()到a.end()的元素中查找10，存在则返回其位置</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># Python列表的定义方式</span><br><span class="hljs-comment">#（1）一维数组：</span><br><span class="hljs-built_in">list</span> = [<span class="hljs-number">0</span>]*n<br><span class="hljs-comment">#（2）二维数组：</span><br><span class="hljs-built_in">list</span> = [[<span class="hljs-number">0</span>]*m <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/pascals-triangle/">118. 杨辉三角</a><br><img src="https://pic.leetcode-cn.com/1626927345-DZmfxB-PascalTriangleAnimated2.gif"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">self, numRows</span>):<br>    ret = [[<span class="hljs-number">1</span>]*i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, numRows+<span class="hljs-number">1</span>)]<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, numRows):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, i):<br>            ret[i][j] = ret[i-<span class="hljs-number">1</span>][j-<span class="hljs-number">1</span>] + ret[i-<span class="hljs-number">1</span>][j]<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><blockquote><p><strong>总之，！！！善用哈希表！！！</strong></p></blockquote><hr><h2 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h2><p>注：以下图源<a href="#cite_link_list">参考链接 1</a></p><h3 id="单链表"><a href="#单链表" class="headerlink" title="单链表"></a>单链表</h3><p>双指针、快慢指针等 （<a href="#double_pointer">双指针中等难度例题点这里</a>）<br>（1）链表节点的定义<br><img src="https://s2.loli.net/2022/07/04/5QbtNGTw9AXVgqs.png" alt="单链表"><br><img src="https://s2.loli.net/2022/07/04/DqyQ1phbnXTJgPI.png" alt="带头指针的链表"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ListNode</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, val=<span class="hljs-number">0</span>, <span class="hljs-built_in">next</span>=<span class="hljs-literal">None</span></span>):<br>        self.val = val<br>        self.<span class="hljs-built_in">next</span> = <span class="hljs-built_in">next</span><br></code></pre></td></tr></table></figure><p>（2）链表节点的删除<br><img src="https://s2.loli.net/2022/07/04/zfeg7vJdsZSpyIT.jpg" alt="链表删除节点的过程图示"><br><a href="https://leetcode.cn/problems/delete-middle-node-lcci/">面试题 02.03. 删除中间节点</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">deleteNode</span>(<span class="hljs-params">self, node</span>):<br>    node.val = node.<span class="hljs-built_in">next</span>.val<br>    node.<span class="hljs-built_in">next</span> = node.<span class="hljs-built_in">next</span>.<span class="hljs-built_in">next</span><br></code></pre></td></tr></table></figure><blockquote><p>删除 <code>中间节点</code> 好写，一旦涉及 <code>头尾元素</code> 就有点麻烦了……比如下面这个题 👇👇👇</p></blockquote><p><a href="https://leetcode.cn/problems/remove-linked-list-elements/">203. 移除链表元素</a></p><blockquote><p>输出 <code>head.val</code> 发现<strong>头节点指向的是第一个元素</strong>，因此需要添加一个虚拟头结点 <code>dummy</code> 来指向头结点</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">removeElements</span>(<span class="hljs-params">self, head: ListNode, val: <span class="hljs-built_in">int</span></span>) -&gt; ListNode:<br>    dummy = ListNode(<span class="hljs-number">0</span>)<br>    dummy.<span class="hljs-built_in">next</span> = head<br>    prev = dummy<br>    node = head<br>    <span class="hljs-keyword">while</span>(node):<br>        <span class="hljs-keyword">if</span> node.val == val:<br>            prev.<span class="hljs-built_in">next</span> = node.<span class="hljs-built_in">next</span><br>        <span class="hljs-keyword">else</span>:<br>            prev = node<br>        node = node.<span class="hljs-built_in">next</span><br>    <span class="hljs-keyword">return</span> dummy.<span class="hljs-built_in">next</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/merge-nodes-in-between-zeros/">2181. 合并零之间的节点</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mergeNodes</span>(<span class="hljs-params">self, head</span>):<br>    end, last, cur = <span class="hljs-literal">None</span>, head, head.<span class="hljs-built_in">next</span><br>    <span class="hljs-keyword">while</span> cur:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> cur.val:<br>            end,last = last,cur<br>        <span class="hljs-keyword">else</span>:<br>            last.val += cur.val<br>            last.<span class="hljs-built_in">next</span> = cur.<span class="hljs-built_in">next</span><br>        cur = cur.<span class="hljs-built_in">next</span><br>    end.<span class="hljs-built_in">next</span> = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">return</span> head<br></code></pre></td></tr></table></figure><p>（3）链表节点的插入<br><img src="https://s2.loli.net/2022/07/04/DbL5vNrMATxOy2V.png" alt="链表插入节点的过程图示"><br>（4）链表的合并<br><a href="https://leetcode.cn/problems/merge-two-sorted-lists/">21. 合并两个有序链表</a></p><blockquote><p>我的代码—<code>丑陋至极</code> 👇👇👇</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mergeTwoLists</span>(<span class="hljs-params">self, list1, list2</span>):<br>    <span class="hljs-keyword">if</span> list1 == <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> list2 == <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> list2 == <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> list1:<br>        <span class="hljs-keyword">return</span> list1<br>    <span class="hljs-keyword">elif</span> list2 <span class="hljs-keyword">and</span> list1 == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> list2<br>    head = ListNode(<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">if</span> list1.val &lt;= list2.val:<br>        head.<span class="hljs-built_in">next</span> = list1<br>        node1 = list1.<span class="hljs-built_in">next</span><br>        node2 = list2<br>    <span class="hljs-keyword">else</span>:<br>        head.<span class="hljs-built_in">next</span> = list2<br>        node1 = list1<br>        node2 = list2.<span class="hljs-built_in">next</span><br>    node = head.<span class="hljs-built_in">next</span><br>    <span class="hljs-keyword">while</span>(node1 <span class="hljs-keyword">and</span> node2):<br>        <span class="hljs-keyword">if</span> node1.val &gt; node2.val:<br>            node.<span class="hljs-built_in">next</span> = node2<br>            node2 = node2.<span class="hljs-built_in">next</span><br>        <span class="hljs-keyword">else</span>:<br>            node.<span class="hljs-built_in">next</span> = node1<br>            node1 = node1.<span class="hljs-built_in">next</span><br>        node = node.<span class="hljs-built_in">next</span><br>    <span class="hljs-keyword">if</span> node1 == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">while</span>(node2):<br>            node.<span class="hljs-built_in">next</span> = node2<br>            node2 = node2.<span class="hljs-built_in">next</span><br>            node = node.<span class="hljs-built_in">next</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">while</span>(node1):<br>            node.<span class="hljs-built_in">next</span> = node1<br>            node1 = node1.<span class="hljs-built_in">next</span><br>            node = node.<span class="hljs-built_in">next</span><br>    <span class="hljs-keyword">return</span> head.<span class="hljs-built_in">next</span><br></code></pre></td></tr></table></figure><blockquote><p>别人的代码—<code>简洁明了</code></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mergeTwoLists</span>(<span class="hljs-params">self, list1, list2</span>):<br>    head = ListNode(<span class="hljs-literal">None</span>)<br>    node = head<br>    <span class="hljs-keyword">while</span> list1 <span class="hljs-keyword">and</span> list2:<br>        <span class="hljs-keyword">if</span> list1.val &lt; list2.val:<br>            node.<span class="hljs-built_in">next</span>, list1 = list1, list1.<span class="hljs-built_in">next</span><br>        <span class="hljs-keyword">else</span>:<br>            node.<span class="hljs-built_in">next</span>, list2 = list2, list2.<span class="hljs-built_in">next</span><br>        node = node.<span class="hljs-built_in">next</span><br>    <span class="hljs-keyword">if</span> list1:<br>        node.<span class="hljs-built_in">next</span> = list1<br>    <span class="hljs-keyword">else</span>:<br>        node.<span class="hljs-built_in">next</span> = list2<br>    <span class="hljs-keyword">return</span> head.<span class="hljs-built_in">next</span>  <br></code></pre></td></tr></table></figure><p>但是我的效率反而更高？？？<br><img src="https://s2.loli.net/2022/07/07/S4bCXfjp2V6FvJq.png"></p><h3 id="双链表"><a href="#双链表" class="headerlink" title="双链表"></a>双链表</h3><h3 id="循环链表"><a href="#循环链表" class="headerlink" title="循环链表"></a>循环链表</h3><p><a href="https://leetcode.cn/problems/4ueAj6/">剑指 Offer II 029. 排序的循环链表</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">insert</span>(<span class="hljs-params">self, head, insertVal</span>):<br>    insertNode = Node(insertVal)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> head:<br>        insertNode.<span class="hljs-built_in">next</span> = insertNode<br>        <span class="hljs-keyword">return</span> insertNode<br>        <br>    pre = cur = head<br>    <span class="hljs-keyword">while</span> pre.<span class="hljs-built_in">next</span> != head:<br>        pre = cur<br>        cur = cur.<span class="hljs-built_in">next</span><br>        <span class="hljs-keyword">if</span> pre.val &lt;= insertVal <span class="hljs-keyword">and</span> cur.val &gt;= insertVal:<br>            <span class="hljs-keyword">break</span><br>        <span class="hljs-keyword">if</span> pre.val &gt; cur.val <span class="hljs-keyword">and</span> (insertVal &gt;= pre.val <span class="hljs-keyword">or</span> insertVal &lt;= cur.val):              <br>            <span class="hljs-keyword">break</span>        <br>    pre.<span class="hljs-built_in">next</span> = insertNode<br>    insertNode.<span class="hljs-built_in">next</span> = cur<br>    <span class="hljs-keyword">return</span> head<br></code></pre></td></tr></table></figure><hr><h2 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h2><p><code>stack</code>：先进后出</p><h3 id="栈的实现"><a href="#栈的实现" class="headerlink" title="栈的实现"></a>栈的实现</h3><hr><h2 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h2><p><code>queue</code>：先进先出</p><h3 id="队列的实现"><a href="#队列的实现" class="headerlink" title="队列的实现"></a>队列的实现</h3><p><a href="https://leetcode.cn/problems/implement-queue-using-stacks/">232. 用栈实现队列</a></p><blockquote><p>双栈实现队列：<code>stack1</code> 为输入栈，<code>stack2</code> 为输出栈。当 <code>stack2</code> 为空时，将 <code>stack1</code> 中的元素挨个弹出压入至 <code>stack2</code> 中，从 <code>stack2</code> 中进行 <code>pop</code> 和 <code>peek</code> 操作。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyQueue</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.stack1 = []<br>        self.stack2 = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">push</span>(<span class="hljs-params">self, x</span>):<br>        self.stack1.append(x)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">pop</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.stack2:<br>            <span class="hljs-keyword">while</span> self.stack1:<br>                self.stack2.append(self.stack1.pop())<br>        <span class="hljs-keyword">return</span> self.stack2.pop()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">peek</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.stack2:<br>            <span class="hljs-keyword">while</span> self.stack1:<br>                self.stack2.append(self.stack1.pop())<br>        <span class="hljs-keyword">return</span> self.stack2[-<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">empty</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.stack1 == [] <span class="hljs-keyword">and</span> self.stack2 == []<br></code></pre></td></tr></table></figure><h3 id="双端队列"><a href="#双端队列" class="headerlink" title="双端队列"></a>双端队列</h3><p><code>deque</code>相比于list实现的队列，<code>deque</code> 实现拥有更低的时间和空间复杂度。 <code>list</code> 实现在出队 <code>（pop）</code> 和插入 <code>（insert）</code> 时的空间复杂度大约为 <code>O(n)</code>，<code>deque</code> 在出队 <code>（pop）</code> 和入队 <code>（append）</code> 时的时间复杂度是 <code>O(1)</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> collections<br>deque = collections.deque()<br><span class="hljs-comment"># 入列</span><br>deque.append(value)     <span class="hljs-comment"># 默认右侧入列</span><br>deque.appendleft(value) <span class="hljs-comment"># 左侧入列</span><br><span class="hljs-comment"># 出列</span><br>deque.popleft()          <span class="hljs-comment"># 左侧出列</span><br>deque.popright()         <span class="hljs-comment"># 右侧出列</span><br></code></pre></td></tr></table></figure><h3 id="优先队列"><a href="#优先队列" class="headerlink" title="优先队列"></a>优先队列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> queue<br>priority_queue = queue.PriorityQueue()<br></code></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs CPP"><span class="hljs-comment">// 一般类型：</span><br>priority_queue&lt;<span class="hljs-type">int</span>&gt;  q;<br><span class="hljs-comment">// 指定类型：</span><br>priority_queue&lt;<span class="hljs-type">int</span>, vector&lt;<span class="hljs-type">int</span>&gt;, greater&lt;<span class="hljs-type">int</span>&gt; &gt; q;   <span class="hljs-comment">// 小顶堆</span><br>priority_queue&lt;<span class="hljs-type">int</span>, vector&lt;<span class="hljs-type">int</span>&gt;, less&lt;<span class="hljs-type">int</span>&gt; &gt; q;      <span class="hljs-comment">// 大顶堆</span><br><span class="hljs-comment">// 自定义优先队列运算符</span><br>priority_queue&lt;pair&lt;<span class="hljs-type">int</span>, <span class="hljs-type">int</span>&gt; &gt; q;<br>q.<span class="hljs-built_in">push</span>(<span class="hljs-built_in">make_pair</span>(exist[i], i));                      <span class="hljs-comment">// 压入</span><br>pair&lt;<span class="hljs-type">int</span>, <span class="hljs-type">int</span>&gt; temp = q.<span class="hljs-built_in">top</span>();                       <span class="hljs-comment">// 弹出</span><br>a = temp.first;  <br>b = temp.second;<br></code></pre></td></tr></table></figure><h3 id="循环队列"><a href="#循环队列" class="headerlink" title="循环队列"></a>循环队列</h3><p>用列表实现循环队列，关键有两点：<br>（1）队空和队满的判断（考研知识点的温习）</p><blockquote><p>队空：<code>self.front == self.rear</code><br>队满：若列表填满的情况下，队满时同样有 <code>self.front == self.rear</code>，因此需要区分。可以使用额外的一个空间，约定 <strong>“队头指针在队尾指针的下一位置即为队满的标志”</strong>。</p></blockquote><p><img src="https://s2.loli.net/2022/08/02/jRCDS2QBLPMnTaX.jpg" alt="图源《2020王道数据结构》"></p><p>（2）首尾指针的移动</p><blockquote><p>入队：队尾指针加 1，<code>self.rear = (self.rear+1) % self.MAX_SIZE</code><br>出队：队首指针加 1，<code>self.front = (self.front+1) % self.MAX_SIZE</code></p></blockquote><p><a href="https://leetcode.cn/problems/design-circular-queue/">622. 设计循环队列</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyCircularQueue</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, k: <span class="hljs-built_in">int</span></span>):<br>        self.MAX_SIZE = k+<span class="hljs-number">1</span><br>        self.front, self.rear = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>        self.queue = [<span class="hljs-number">0</span>]*(k+<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">enQueue</span>(<span class="hljs-params">self, value: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-keyword">if</span> self.isFull():<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        self.queue[self.rear] = value<br>        self.rear = (self.rear+<span class="hljs-number">1</span>) % self.MAX_SIZE<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">deQueue</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-keyword">if</span> self.isEmpty():<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        self.front = (self.front+<span class="hljs-number">1</span>) % self.MAX_SIZE<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">Front</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">if</span> self.isEmpty():<br>            <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> self.queue[self.front]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">Rear</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">if</span> self.isEmpty():<br>            <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> self.queue[self.rear-<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">isEmpty</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-keyword">return</span> self.front == self.rear<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">isFull</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-keyword">return</span> (self.rear+<span class="hljs-number">1</span>) % self.MAX_SIZE == self.front<br><br><br><span class="hljs-comment"># Your MyCircularQueue object will be instantiated and called as such:</span><br><span class="hljs-comment"># obj = MyCircularQueue(k)</span><br><span class="hljs-comment"># param_1 = obj.enQueue(value)</span><br><span class="hljs-comment"># param_2 = obj.deQueue()</span><br><span class="hljs-comment"># param_3 = obj.Front()</span><br><span class="hljs-comment"># param_4 = obj.Rear()</span><br><span class="hljs-comment"># param_5 = obj.isEmpty()</span><br><span class="hljs-comment"># param_6 = obj.isFull()</span><br></code></pre></td></tr></table></figure><hr><h2 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h2><blockquote><p>写在最前：有关于二叉树，不超时的情况下尽量用递归做吧，迭代太费脑子了</p></blockquote><h3 id="不同类型的二叉树"><a href="#不同类型的二叉树" class="headerlink" title="不同类型的二叉树"></a>不同类型的二叉树</h3><p><img src="https://s2.loli.net/2022/07/13/1KeO42izhZHw9RN.png" alt="从左至右依次为：完满二叉树，完全二叉树，满二叉树"><br>（1）<strong>满二叉树</strong>（完美二叉树）</p><blockquote><p><strong>满二叉树</strong>：每一层的结点数目都达到最大值，即第 <code>i</code> 层有 $2^{i-1}$ 个结点。</p></blockquote><p><a href="https://leetcode.cn/problems/populating-next-right-pointers-in-each-node/">116. 填充每个节点的下一个右侧节点指针</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">connect</span>(<span class="hljs-params">self, root: <span class="hljs-string">&#x27;Optional[Node]&#x27;</span></span>) -&gt; <span class="hljs-string">&#x27;Optional[Node]&#x27;</span>:<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<br>        <span class="hljs-keyword">return</span> root<br>    queue = [root]<br>    <span class="hljs-keyword">while</span> queue:<br>        node = queue.pop(<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">if</span> node.left:<br>            node.left.<span class="hljs-built_in">next</span> = node.right<br>            <span class="hljs-keyword">if</span> node.<span class="hljs-built_in">next</span>:<br>                node.right.<span class="hljs-built_in">next</span> = node.<span class="hljs-built_in">next</span>.left<br><br>            queue.append(node.left)<br>            queue.append(node.right)<br>    <span class="hljs-keyword">return</span> root<br></code></pre></td></tr></table></figure><p>（2）<strong>完全二叉树</strong></p><blockquote><p><strong>完全二叉树</strong>：由满二叉树而引出来的。对于深度为 <code>K</code>的，有 <code>n</code> 个结点的二叉树，当且仅当其每一个结点都与深度为 <code>K</code> 的满二叉树中编号从 <code>1</code> 至 <code>n</code> 的结点<strong>一一对应</strong>。<strong>满二叉树是一种特殊的完全二叉树</strong>。</p></blockquote><p>（3）<strong>二叉搜索树</strong>（Binary Searc Tree, BST，也称<strong>二叉排序树</strong>）<br><img src="https://s2.loli.net/2022/07/13/WkBs65hxZe1aipb.png"></p><blockquote><p>性质：任意结点都保证 <code>左儿子</code> &lt; <code>根节点</code> &lt; <code>右儿子</code></p></blockquote><ul><li>若它的左子树不为空，则左子树上所有节点的值都小于根节点的值</li><li>若它的右子树不为空，则右子树上所有节点的值都大于根节点的值</li><li>它的左右子树也分别为二叉搜索树</li></ul><p><a href="https://leetcode.cn/problems/insert-into-a-binary-search-tree/">701. 二叉搜索树中的插入操作</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">insertIntoBST</span>(<span class="hljs-params">self, root: TreeNode, val: <span class="hljs-built_in">int</span></span>) -&gt; TreeNode:<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<br>        <span class="hljs-keyword">return</span> TreeNode(val)<br>    <span class="hljs-keyword">if</span> root.val &lt; val:<br>        <span class="hljs-keyword">if</span> root.right == <span class="hljs-literal">None</span>:<br>            root.right = TreeNode(val)<br>        <span class="hljs-keyword">else</span>:<br>            self.insertIntoBST(root.right, val)<br>    <span class="hljs-keyword">elif</span> root.val &gt; val:<br>        <span class="hljs-keyword">if</span> root.left == <span class="hljs-literal">None</span>:<br>            root.left = TreeNode(val)<br>        <span class="hljs-keyword">else</span>:<br>            self.insertIntoBST(root.left, val)<br>    <span class="hljs-keyword">return</span> root<br></code></pre></td></tr></table></figure><p>（4）平衡二叉树（Balance Tree, BT，也称 AVL 树）</p><blockquote><p>平衡二叉树：任意结点的左右两个<strong>子树的高度差的绝对值不超过 1</strong>。<br>平衡因子：指的是该结点的左子树和右子树的高度差，即用左子树的高度减去右子树的高度。</p></blockquote><p>在对平衡二叉树进行插入操作时，有可能造成平衡二叉树失衡，则需要通过 <code>左旋</code> &#x2F; <code>右旋</code> 对其进行修正，保证<strong>任意结点的平衡因子的绝对值不超过 1</strong>，修正方法包括 <code>LL</code>、<code>RR</code>、<code>LR</code> 和 <code>RL</code> 平衡旋转。<br>（以下图源：<a href="https://blog.csdn.net/jarvan5/article/details/112428036%EF%BC%89">https://blog.csdn.net/jarvan5/article/details/112428036）</a><br><img src="https://s2.loli.net/2022/07/13/ODNnFKLXT4kuW6C.png"></p><p>（5）线索二叉树</p><h3 id="二叉树的遍历"><a href="#二叉树的遍历" class="headerlink" title="二叉树的遍历"></a>二叉树的遍历</h3><p>（1）前序遍历</p><blockquote><p>访问顺序：<code>根节点</code> -&gt; <code>左子树</code>  -&gt; <code>右子树</code></p></blockquote><p><a href="https://leetcode.cn/problems/binary-tree-preorder-traversal/">144. 二叉树的前序遍历</a></p><blockquote><p>递归算法</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">preorderTraversal</span>(<span class="hljs-params">self, root</span>):<br>    ret = []<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> ret<br>    ret += [root.val]<br>    ret += self.preorderTraversal(root.left)<br>    ret += self.preorderTraversal(root.right)<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><blockquote><p>迭代算法（用栈实现，左、右链入栈）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">preorderTraversal</span>(<span class="hljs-params">self, root</span>):<br>    ret = []<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> []<br><br>    stack = [root]<br>    <span class="hljs-keyword">while</span>(stack):<br>        node = stack.pop()<br>        ret.append(node.val)            <br>        <span class="hljs-keyword">if</span> node.right:                 <span class="hljs-comment"># 先压入右子树使其后出栈</span><br>            stack.append(node.right)<br>        <span class="hljs-keyword">if</span> node.left:<br>            stack.append(node.left)<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><p>（2）中序遍历</p><blockquote><p>访问顺序：<code>左子树</code> -&gt; <code>根节点</code> -&gt; <code>右子树</code></p></blockquote><p><a href="https://leetcode.cn/problems/binary-tree-inorder-traversal/">94. 二叉树的中序遍历</a></p><blockquote><p>递归算法</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">inorderTraversal</span>(<span class="hljs-params">self, root</span>):<br>    ret = []<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> ret<br>    ret += self.inorderTraversal(root.left)<br>    ret += [root.val]<br>    ret += self.inorderTraversal(root.right)<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><blockquote><p>迭代算法（用栈实现，左链入栈）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">inorderTraversal</span>(<span class="hljs-params">self, root</span>):<br>    ret = []<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> ret<br><br>    stack = []<br>    node = root<br>    <span class="hljs-keyword">while</span>(stack <span class="hljs-keyword">or</span> node):<br>        <span class="hljs-keyword">if</span> node:<br>            stack.append(node)    <span class="hljs-comment"># 左子树不为空则始终压入左儿子</span><br>            node = node.left<br>        <span class="hljs-keyword">else</span>:<br>            node = stack.pop()    <span class="hljs-comment"># 否则弹出当前节点</span><br>            ret.append(node.val)  <span class="hljs-comment"># 再遍历根节点的值</span><br>            node = node.right     <span class="hljs-comment"># 访问右子树</span><br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><p>（3）后序遍历</p><blockquote><p>访问顺序：<code>左子树</code> -&gt; <code>右子树</code> -&gt; <code>根节点</code></p></blockquote><p><a href="https://leetcode.cn/problems/binary-tree-postorder-traversal/">145. 二叉树的后序遍历</a></p><blockquote><p>递归算法</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">postorderTraversal</span>(<span class="hljs-params">self, root</span>):<br>    ret = []<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> ret<br>    ret += self.postorderTraversal(root.left)<br>    ret += self.postorderTraversal(root.right)<br>    ret += [root.val]<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><blockquote><p>迭代算法（用栈实现，有亿点点难度）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">postorderTraversal</span>(<span class="hljs-params">self, root</span>):<br>ret = []<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> ret<br>    <br>    stack = []<br>    prev = <span class="hljs-literal">None</span>                                   <span class="hljs-comment"># 记录上一次访问的位置，若为右子树则弹出当前子树的根节点</span><br><br>    <span class="hljs-keyword">while</span> root <span class="hljs-keyword">or</span> stack:<br>        <span class="hljs-keyword">while</span> root:                               <span class="hljs-comment"># 当前节点不为空时压入，并一直移动直至左子树为空</span><br>            stack.append(root)<br>            root = root.left<br>        root = stack.pop()                        <span class="hljs-comment"># 当前节点为空则弹出，转入其右子树</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root.right <span class="hljs-keyword">or</span> root.right == prev:  <span class="hljs-comment"># 若右子树为空或当前从右子树访问后返回</span><br>            ret.append(root.val)<br>            prev = root<br>            root = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">elif</span> root.right != prev:                  <span class="hljs-comment"># 若当前从左子树访问后返回，则访问右子树，并将prev设定为右子树</span><br>            stack.append(root)<br>            root = root.right<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><blockquote><p>迭代算法（评论区看到的题解，用栈实现，<strong>按中右左前序遍历，反转遍历结果</strong>即可，妙啊！！！！！！）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">postorderTraversal</span>(<span class="hljs-params">self, root</span>):<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> []<br><br>    ret = []<br>    stack = [root]<br>    <span class="hljs-keyword">while</span>(stack):<br>        node = stack.pop()<br>        ret.append(node.val)            <br>        <span class="hljs-keyword">if</span> node.left:<br>            stack.append(node.left)<br>        <span class="hljs-keyword">if</span> node.right:<br>            stack.append(node.right)<br>    <span class="hljs-keyword">return</span> ret[::-<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><p>（4）层次遍历<br><a href="https://leetcode.cn/problems/binary-tree-level-order-traversal/">102. 二叉树的层序遍历</a></p><blockquote><p>迭代算法（用队列实现，分层输出正确解）<br>自己写的跟官方题解不一样，区别在于用变量存储了层级，然后遍历的时候直接加入到对应的层级列表中！</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">levelOrder</span>(<span class="hljs-params">self, root</span>):<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> []<br><br>    ret = [[]]<br>    queue = [[root, <span class="hljs-number">1</span>]]<br>    <span class="hljs-keyword">while</span>(queue):<br>        node, depth = queue.pop(<span class="hljs-number">0</span>)<br>        ret[depth-<span class="hljs-number">1</span>].append(node.val)<br>        <span class="hljs-keyword">if</span> node.left:<br>            queue.append([node.left, depth+<span class="hljs-number">1</span>])<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(ret) &lt; depth+<span class="hljs-number">1</span>:<br>                ret += [[]]<br>        <span class="hljs-keyword">if</span> node.right:<br>            queue.append([node.right, depth+<span class="hljs-number">1</span>])<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(ret) &lt; depth+<span class="hljs-number">1</span>:<br>                ret += [[]]<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><blockquote><p>迭代算法（<code>BFS</code> 模板的意思）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">levelOrder</span>(<span class="hljs-params">self, root</span>):<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> []<br><br>    ret = []<br>    queue = [root]<br>    <span class="hljs-keyword">while</span>(queue):<br>        temp = []<br>        n = <span class="hljs-built_in">len</span>(queue)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            node = queue.pop(<span class="hljs-number">0</span>)<br>            temp.append(node.val)<br>            <span class="hljs-keyword">if</span> node.left:<br>                queue.append(node.left)<br>            <span class="hljs-keyword">if</span> node.right:<br>                queue.append(node.right)<br>        ret.append(temp)<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><blockquote><p>迭代算法（用队列实现，不分层输出，不适用于解本题，仅供参考）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">levelOrder</span>(<span class="hljs-params">self, root</span>):<br>    ret = []<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> ret<br>    <br>    queue = [root]<br>    <span class="hljs-keyword">while</span>(queue):<br>        node = queue.pop(<span class="hljs-number">0</span>)<br>        ret.append(node.val)<br>        <span class="hljs-keyword">if</span> node.left:<br>            queue.append(node.left)<br>        <span class="hljs-keyword">if</span> node.right:<br>            queue.append(node.right)<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><h3 id="哈夫曼树"><a href="#哈夫曼树" class="headerlink" title="哈夫曼树"></a>哈夫曼树</h3><h3 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h3><hr><h2 id="图"><a href="#图" class="headerlink" title="图"></a>图</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>（1）邻接矩阵<br>（2）邻接表<br>（3）十字链表<br>（4）邻接多重表</p><h3 id="广度优先遍历"><a href="#广度优先遍历" class="headerlink" title="广度优先遍历"></a>广度优先遍历</h3><h3 id="深度优先遍历"><a href="#深度优先遍历" class="headerlink" title="深度优先遍历"></a>深度优先遍历</h3><h3 id="最小生成树"><a href="#最小生成树" class="headerlink" title="最小生成树"></a>最小生成树</h3><h3 id="最短路径"><a href="#最短路径" class="headerlink" title="最短路径"></a>最短路径</h3><h3 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h3><blockquote><p><strong>拓扑排序的实现</strong>：对于有向图 <code>G</code>，每次访问 <code>G</code> 中入度为 <code>0</code> 的节点，访问得到的序列即为拓扑排序的结果。<br>（<strong>注意</strong>：每一轮中入度为 <code>0</code> 的节点不唯一，因此拓扑排序的结果也不唯一。）</p></blockquote><p><a href="https://leetcode.cn/problems/ur2n8P/">剑指 Offer II 115. 重建序列</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sequenceReconstruction</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], sequences: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        inDegrees = [<span class="hljs-number">0</span>]*(n+<span class="hljs-number">1</span>)<br>        dst = defaultdict(<span class="hljs-built_in">list</span>)<br><br>        <span class="hljs-keyword">for</span> sequence <span class="hljs-keyword">in</span> sequences:<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sequence)-<span class="hljs-number">1</span>):<br>                dst[sequence[i]].append(sequence[i+<span class="hljs-number">1</span>])<br>                inDegrees[sequence[i+<span class="hljs-number">1</span>]] += <span class="hljs-number">1</span><br><br>        queue = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n+<span class="hljs-number">1</span>) <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> inDegrees[i]]<br>        <span class="hljs-keyword">while</span> queue:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(queue) &gt;= <span class="hljs-number">2</span>:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>            u = queue.pop(<span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> dst[u]:<br>                inDegrees[v] -= <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> inDegrees[v]:<br>                    queue.append(v)<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><blockquote><p><strong>拓扑排序的应用</strong>：若访问结束后若还<strong>存在入度不为 <code>0</code> 的节点</strong>，则 <strong><code>G</code> 中必有环</strong>。</p></blockquote><p><a href="https://leetcode.cn/problems/longest-cycle-in-a-graph/">6135. 图中的最长环</a></p><blockquote><p>第 304 场周赛的压轴题，题解：<a href="https://svyj.github.io/2022/07/10/032-Leetcode/">https://svyj.github.io/2022/07/10/032-Leetcode/</a></p></blockquote><h3 id="关键路径"><a href="#关键路径" class="headerlink" title="关键路径"></a>关键路径</h3><hr><h2 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h2><h3 id="顺序查找"><a href="#顺序查找" class="headerlink" title="顺序查找"></a>顺序查找</h3><h3 id="折半查找（二分查找）"><a href="#折半查找（二分查找）" class="headerlink" title="折半查找（二分查找）"></a>折半查找（二分查找）</h3><p>（<a href="#binary_search">二分查找例题点这里</a>）</p><h3 id="分块查找"><a href="#分块查找" class="headerlink" title="分块查找"></a>分块查找</h3><hr><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="Python内置排序方法"><a href="#Python内置排序方法" class="headerlink" title="Python内置排序方法"></a>Python内置排序方法</h3><p>一维列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-built_in">list</span>.sort(cmp=<span class="hljs-literal">None</span>, key=<span class="hljs-literal">None</span>, reverse=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 或</span><br><span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">list</span>, key=<span class="hljs-literal">None</span>, reverse=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>二维列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-built_in">list</span> = [[ <span class="hljs-string">&#x27;A&#x27;</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">99</span>], [ <span class="hljs-string">&#x27;J&#x27;</span>, <span class="hljs-number">0</span>, <span class="hljs-number">86</span>], [ <span class="hljs-string">&#x27;T&#x27;</span>, <span class="hljs-number">1</span>, <span class="hljs-number">65</span>], [ <span class="hljs-string">&#x27;S&#x27;</span>, -<span class="hljs-number">2</span>, <span class="hljs-number">100</span>], [<span class="hljs-string">&#x27;B&#x27;</span>, -<span class="hljs-number">4</span>, <span class="hljs-number">77</span>], [<span class="hljs-string">&#x27;L&#x27;</span>, <span class="hljs-number">2</span>, <span class="hljs-number">59</span>]]<br><br><span class="hljs-comment"># 按第一个元素排序</span><br><span class="hljs-built_in">list</span>.sort(key=(<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">0</span>]), reverse=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># 按第二个元素的绝对值排序</span><br><span class="hljs-built_in">list</span>.sort(key=<span class="hljs-keyword">lambda</span> x:(<span class="hljs-built_in">abs</span>(x[<span class="hljs-number">1</span>]),x[<span class="hljs-number">1</span>]), reverse=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 先按绝对值（第一关键词），再按本身大小（第二关键词）  如果绝对值相同，则正数在前面</span><br></code></pre></td></tr></table></figure><h3 id="内部排序和外部排序"><a href="#内部排序和外部排序" class="headerlink" title="内部排序和外部排序"></a>内部排序和外部排序</h3><blockquote><p><strong>排序算法中各种与初始序列无关</strong><br>（1）元素的移动次数与关键字的初始排列次序无关的是：基数排序<br>（2）元素的比较次数与初始序列无关是：选择排序、折半插入排序<br>（3）算法的时间复杂度与初始序列无关的是：选择排序、堆排序、归并排序、基数排序<br>（4）算法的排序趟数与初始序列无关的是：插入排序、选择排序、基数排序</p></blockquote><h3 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h3><p>（1）直接插入<br>（2）折半插入<br>（3）希尔排序</p><h3 id="交换排序"><a href="#交换排序" class="headerlink" title="交换排序"></a>交换排序</h3><p>（1）冒泡排序<br>（2）快速排序</p><h3 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h3><p>（1）简单选择排序<br>（2）堆排序</p><h3 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h3><h3 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h3><hr><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><h2 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h2><p>（<a href="https://blog.csdn.net/qq_45978890/article/details/116094046">详细图解</a>） <a id="binary_search"></a></p><blockquote><p><strong>必要条件</strong>：<br>（1）查找的内容逻辑有序；<br>（2）查找元素数量为 <code>1</code>。</p></blockquote><p><a href="https://leetcode.cn/problems/binary-search/">704. 二分查找</a></p><blockquote><p>左闭右闭区间查找</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">search</span>(<span class="hljs-params">self, nums, target, n</span>):<br>    left, right = <span class="hljs-number">0</span>, n-<span class="hljs-number">1</span>                <span class="hljs-comment"># right初始值为 n-1</span><br>    <span class="hljs-keyword">while</span>(left &lt;= right):               <span class="hljs-comment"># 取等号, 当 left==right 时, 区间[left, right]有意义</span><br>        mid = (left + right) // <span class="hljs-number">2</span><br>        <span class="hljs-keyword">if</span> nums[mid] &gt; target:<br>            right = mid - <span class="hljs-number">1</span>             <span class="hljs-comment"># target 在左区间, 右端点减 1 以排除</span><br>        <span class="hljs-keyword">elif</span> nums[mid] &lt; target:<br>            left = mid + <span class="hljs-number">1</span>              <span class="hljs-comment"># target 在右区间, 左端点加 1 以排除</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> mid<br>    <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>                           <span class="hljs-comment"># 查找失败</span><br></code></pre></td></tr></table></figure><blockquote><p>左闭右开区间查找</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">search</span>(<span class="hljs-params">self, nums, target, n</span>):<br>    left, right = <span class="hljs-number">0</span>, n                  <span class="hljs-comment"># right初始值为 n</span><br>    <span class="hljs-keyword">while</span>(left &lt; right):                <span class="hljs-comment"># 不取等号, 当 left==right 时, 区间[left, right)无意义</span><br>        mid = (left + right) // <span class="hljs-number">2</span>     <br>        <span class="hljs-keyword">if</span> nums[mid] &gt; target:        <br>            right = mid                 <span class="hljs-comment"># target 在左区间, 右端点自动排除, 无需减 1 </span><br>        <span class="hljs-keyword">elif</span> nums[mid] &lt; target:<br>            left = mid + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> mid<br>    <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>                           <span class="hljs-comment"># 查找失败</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/first-bad-version/">278. 第一个错误的版本</a></p><hr><h2 id="双指针"><a href="#双指针" class="headerlink" title="双指针 "></a>双指针 <a id="double_pointer"></a></h2><h3 id="简单双指针"><a href="#简单双指针" class="headerlink" title="简单双指针"></a>简单双指针</h3><p><a href="https://leetcode.cn/problems/rotate-array/">189. 轮转数组</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">rotate</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], k: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Do not return anything, modify nums in-place instead.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    n = <span class="hljs-built_in">len</span>(nums)<br>    k = k % n<br>    count = gcd(k, n)<br>    <span class="hljs-keyword">for</span> start <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(count):<br>        cur = start<br>        pre = nums[start]<br>        <span class="hljs-keyword">while</span>(<span class="hljs-literal">True</span>):<br>            <span class="hljs-built_in">next</span> = (cur + k) % n<br>            nums[<span class="hljs-built_in">next</span>], pre = pre, nums[<span class="hljs-built_in">next</span>]<br>            cur = <span class="hljs-built_in">next</span><br>            <span class="hljs-keyword">if</span> start == cur:<br>                <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><h3 id="快慢双指针"><a href="#快慢双指针" class="headerlink" title="快慢双指针"></a>快慢双指针</h3><p><a href="https://leetcode.cn/problems/linked-list-cycle/">141. 环形链表</a></p><blockquote><p>快指针 <code>fast</code> 先走 <code>n</code> 步，慢指针 <code>slow</code> 再开始移动，当快指针到达链表末尾时，慢指针所在位置即为所求</p></blockquote><p><a href="https://leetcode.cn/problems/remove-nth-node-from-end-of-list/">19. 删除链表的倒数第 N 个结点</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">removeNthFromEnd</span>(<span class="hljs-params">self, head, n</span>):<br>    <span class="hljs-keyword">if</span> head == <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> head.<span class="hljs-built_in">next</span> == <span class="hljs-literal">None</span>:   <span class="hljs-comment"># 链表元素个数小于等于 1</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>    fast, slow = head, head<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):                      <span class="hljs-comment"># 快指针先移动 n 步</span><br>        fast = fast.<span class="hljs-built_in">next</span><br>    <span class="hljs-keyword">if</span> fast == <span class="hljs-literal">None</span>:                        <span class="hljs-comment"># 若快指针遍历至链表末尾,倒数第 n 个则为链表第一个元素  </span><br>        <span class="hljs-keyword">return</span> head.<span class="hljs-built_in">next</span><br>    <span class="hljs-keyword">while</span>(fast.<span class="hljs-built_in">next</span>):                       <span class="hljs-comment"># 快慢指针同时移动,直至快指针移动到链表末尾</span><br>        fast = fast.<span class="hljs-built_in">next</span><br>        slow = slow.<span class="hljs-built_in">next</span><br>    slow.<span class="hljs-built_in">next</span> = slow.<span class="hljs-built_in">next</span>.<span class="hljs-built_in">next</span><br>    <span class="hljs-keyword">return</span> head<br></code></pre></td></tr></table></figure><h3 id="逆向双指针"><a href="#逆向双指针" class="headerlink" title="逆向双指针"></a>逆向双指针</h3><blockquote><p>由于 <code>num1</code> 中末尾均为 <code>0</code>，因此从后往前填</p></blockquote><p><a href="https://leetcode.cn/problems/merge-sorted-array/">88. 合并两个有序数组</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">merge</span>(<span class="hljs-params">self, nums1: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], m: <span class="hljs-built_in">int</span>, nums2: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], n: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Do not return anything, modify nums1 in-place instead.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    i, j, index = m-<span class="hljs-number">1</span>, n-<span class="hljs-number">1</span>, m+n-<span class="hljs-number">1</span><br>    <span class="hljs-keyword">while</span>(index &gt;= <span class="hljs-number">0</span>):<br>        <span class="hljs-keyword">if</span> i == -<span class="hljs-number">1</span>:<br>            nums1[index] = nums2[j]<br>            j -= <span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> j == -<span class="hljs-number">1</span>:<br>            nums1[index] = nums1[i]<br>            i -= <span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> nums1[i] &gt; nums2[j]:<br>            nums1[index] = nums1[i]<br>            i -= <span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> nums1[i] &lt;= nums2[j]:<br>            nums1[index] = nums2[j]<br>            j -= <span class="hljs-number">1</span><br>        index -= <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> nums1<br></code></pre></td></tr></table></figure><h3 id="首尾指针"><a href="#首尾指针" class="headerlink" title="首尾指针"></a>首尾指针</h3><p><a href="https://leetcode.cn/problems/two-sum-ii-input-array-is-sorted/">167. 两数之和 II - 输入有序数组</a></p><blockquote><p>双指针从左右两侧移动，若当前和值偏大则右指针左移，偏小则左指针右移</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">twoSum</span>(<span class="hljs-params">self, numbers, target:</span>):<br>    i, j = <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(numbers)-<span class="hljs-number">1</span><br>    <span class="hljs-keyword">while</span>(i &lt; j):<br>        <span class="hljs-keyword">if</span> numbers[i] + numbers[j] == target:<br>            <span class="hljs-keyword">return</span> [i+<span class="hljs-number">1</span>, j+<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">elif</span> numbers[i] + numbers[j] &gt; target:<br>            j -= <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            i += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><hr><h2 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h2><p><a href="https://leetcode.cn/problems/permutation-in-string/">567. 字符串的排列</a></p><blockquote><p>解题思路：设定一个固定长度（与 <code>s1</code> 等长）的窗口在 <code>s2</code> 上移动，利用哈希表记录 <code>s1</code> 和该滑动窗口内各字母的数量。<br>窗口每向右移动一次，左侧抛弃一个字母，右侧一个字母，维护统计滑动窗口的哈希表，移动过程中判断两个哈希表是否相同即可。</p></blockquote><p>时间复杂度： $O(M \times N)$，$M&#x3D;26$，空间复杂度： $O(M)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">checkInclusion</span>(<span class="hljs-params">self, s1: <span class="hljs-built_in">str</span>, s2: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    n, m = <span class="hljs-built_in">len</span>(s1), <span class="hljs-built_in">len</span>(s2)<br>    cnt1 = [<span class="hljs-number">0</span>]*<span class="hljs-number">26</span><br>    cnt2 = [<span class="hljs-number">0</span>]*<span class="hljs-number">26</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>        cnt1[<span class="hljs-built_in">ord</span>(s1[i])-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;a&#x27;</span>)] += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m-n+<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>                cnt2[<span class="hljs-built_in">ord</span>(s2[i+j])-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;a&#x27;</span>)] += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> cnt1 == cnt2:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">else</span>:<br>            cnt2[<span class="hljs-built_in">ord</span>(s2[i-<span class="hljs-number">1</span>])-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;a&#x27;</span>)] -= <span class="hljs-number">1</span><br>            cnt2[<span class="hljs-built_in">ord</span>(s2[i+n-<span class="hljs-number">1</span>])-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;a&#x27;</span>)] += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> cnt1 == cnt2:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/minimum-size-subarray-sum/">209. 长度最小的子数组</a></p><blockquote><p>究极模板题！！！</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 模板题</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">minSubArrayLen</span>(<span class="hljs-params">self, target, nums</span>)<br>    left, right, curSum, maxlen = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(nums)+<span class="hljs-number">1</span> <span class="hljs-comment"># 初始化左右端点</span><br>    <span class="hljs-keyword">while</span>(right &lt; <span class="hljs-built_in">len</span>(nums)):                          <span class="hljs-comment"># while(右端点可移动): 移动右端点</span><br>        curSum += nums[right]                          <span class="hljs-comment"># 窗口加入右端点元素</span><br>        <span class="hljs-keyword">while</span>(curSum &gt;= target):                       <span class="hljs-comment"># while(满足题目条件): 移动左端点</span><br>            <span class="hljs-keyword">if</span> right - left + <span class="hljs-number">1</span> &lt; maxlen:                 <span class="hljs-comment"># 更新窗口值</span><br>                maxlen = right - left + <span class="hljs-number">1</span><br>            curSum -= nums[left]                       <span class="hljs-comment"># 窗口删除左端点元素</span><br>            left += <span class="hljs-number">1</span>                                  <span class="hljs-comment"># 左端点右移</span><br>        right += <span class="hljs-number">1</span>                                     <span class="hljs-comment"># 右端点右移</span><br>    <span class="hljs-keyword">return</span> maxlen<br></code></pre></td></tr></table></figure><blockquote><p>按照上面的模板 👆👆👆， 做下面这题 👇👇👇，尝尝秒解的快感……（照着模板往里套内容即可）<br><strong>注</strong>：翻了翻 <code>ASCII</code> 码表（<a href="https://www.habaijian.com/">ASCII码中文站</a>），有一个做字符串题目都能用的小 <code>trick</code>，比如这道题，涉及到的打印字符一共 <code>95</code> 个（已经算是最多的情况了），此时用数组来标记出现过的字符比官方题解的哈希表 <code>set()</code> 效率高很多。</p></blockquote><p><a href="https://leetcode.cn/problems/longest-substring-without-repeating-characters/">3. 无重复字符的最长子串</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lengthOfLongestSubstring</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>    n = <span class="hljs-built_in">len</span>(s)<br>    dicts = [<span class="hljs-number">0</span>]*<span class="hljs-number">95</span><br>    left, right = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>    maxLen = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> right &lt; n:<br>        dicts[<span class="hljs-built_in">ord</span>(s[right])-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27; &#x27;</span>)] += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> dicts[<span class="hljs-built_in">ord</span>(s[right])-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27; &#x27;</span>)] &gt; <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">while</span>(left &lt;= right):<br>                <span class="hljs-keyword">if</span> s[left] == s[right]:<br>                    dicts[<span class="hljs-built_in">ord</span>(s[left])-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27; &#x27;</span>)] -= <span class="hljs-number">1</span><br>                    left += <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">break</span><br>                dicts[<span class="hljs-built_in">ord</span>(s[left])-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27; &#x27;</span>)] -= <span class="hljs-number">1</span><br>                left += <span class="hljs-number">1</span><br><br>        curLen = right - left + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> curLen &gt; maxLen:<br>            maxLen = curLen<br>        right += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> maxLen<br></code></pre></td></tr></table></figure><hr><h2 id="深度优先搜索"><a href="#深度优先搜索" class="headerlink" title="深度优先搜索"></a>深度优先搜索</h2><p><a href="https://leetcode.cn/problems/most-frequent-subtree-sum/">508. 出现次数最多的子树元素和</a></p><blockquote><p>很基本的 <code>DFS</code> 模板题……<strong><code>DFS</code> 往往通过递归调用实现</strong></p></blockquote><p><a href="https://leetcode.cn/problems/find-bottom-left-tree-value/">513. 找树左下角的值</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 递归深度搜索</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">findBottomLeftValue</span>(<span class="hljs-params">self, root</span>):<br>    ret = maxdep = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">dfs</span>(<span class="hljs-params">node, depth</span>):<br>        <span class="hljs-keyword">if</span> node <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span><br>        <span class="hljs-keyword">nonlocal</span> ret, maxdep<br>        <span class="hljs-keyword">if</span> depth &gt; maxdep:<br>            ret = node.val<br>            maxdep = depth<br>        dfs(node.left, depth+<span class="hljs-number">1</span>)   <span class="hljs-comment"># 递归搜索左子树</span><br>        dfs(node.right, depth+<span class="hljs-number">1</span>)  <span class="hljs-comment"># 递归搜索右子树</span><br>    dfs(root, <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/find-largest-value-in-each-tree-row/">515. 在每个树行中找最大值</a></p><hr><h2 id="广度优先搜索"><a href="#广度优先搜索" class="headerlink" title="广度优先搜索"></a>广度优先搜索</h2><blockquote><p>又是一个模板题，<code>BFS</code> 中最常见的求最大连通区域问题，**<code>BFS</code> 通常用队列实现**</p></blockquote><p><a href="https://leetcode.cn/problems/max-area-of-island/">695. 岛屿的最大面积</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">maxAreaOfIsland</span>(<span class="hljs-params">self, grid: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>    self.m, self.n = <span class="hljs-built_in">len</span>(grid), <span class="hljs-built_in">len</span>(grid[<span class="hljs-number">0</span>])<br>    self.dire = [[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]]<br>    self.flag = [[<span class="hljs-number">0</span>]*self.n <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.m)]<br>    self.grid = grid<br>    ret = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.m):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.n):<br>            <span class="hljs-keyword">if</span> self.grid[i][j] == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> self.flag[i][j] == <span class="hljs-number">0</span>:<br>                area = self.bfs(i, j)<br>                <span class="hljs-keyword">if</span> area &gt; ret:<br>                    ret = area<br>    <span class="hljs-keyword">return</span> ret<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">bfs</span>(<span class="hljs-params">self, x, y</span>):<br>    area = <span class="hljs-number">0</span><br>    queue = [[x, y]]<br>    <span class="hljs-keyword">while</span> queue:<br>        x, y = queue.pop(<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>            dstX = x + self.dire[i][<span class="hljs-number">0</span>]<br>            dstY = y + self.dire[i][<span class="hljs-number">1</span>]<br>            <span class="hljs-keyword">if</span> dstX &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> dstX &lt; self.m <span class="hljs-keyword">and</span> dstY &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> dstY &lt; self.n:<br>                <span class="hljs-keyword">if</span> self.grid[dstX][dstY] == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> self.flag[dstX][dstY] == <span class="hljs-number">0</span>:<br>                    area += <span class="hljs-number">1</span><br>                    queue.append([dstX, dstY])<br>                    self.flag[dstX][dstY] = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> area <span class="hljs-keyword">if</span> area <span class="hljs-keyword">else</span> <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><blockquote><p><strong>优化一下</strong>：不用 <code>HashTable</code>，直接将 <code>grid</code> 中搜索到过的位置置 <code>0</code> 即可 👇👇👇<br>内存消耗：15 MB, 在所有 Python3 提交中击败了99.14%的用户</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">bfs</span>(<span class="hljs-params">self, x, y</span>):<br>    area = <span class="hljs-number">0</span><br>    queue = [[x, y]]<br>    <span class="hljs-keyword">while</span> queue:<br>        x, y = queue.pop(<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>            dstX = x + self.dire[i][<span class="hljs-number">0</span>]<br>            dstY = y + self.dire[i][<span class="hljs-number">1</span>]<br>            <span class="hljs-keyword">if</span> dstX &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> dstX &lt; self.m <span class="hljs-keyword">and</span> dstY &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> dstY &lt; self.n <span class="hljs-keyword">and</span> self.grid[dstX][dstY] == <span class="hljs-number">1</span>:<br>                area += <span class="hljs-number">1</span><br>                queue.append([dstX, dstY])<br>                self.grid[dstX][dstY] = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">return</span> area <span class="hljs-keyword">if</span> area <span class="hljs-keyword">else</span> <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><blockquote><p>下面是一道经典的题目，可以参考<a href="https://leetcode.cn/problems/rotting-oranges/">994. 腐烂的橘子</a> 👇👇👇<br><code>BFS</code> 解法不是该题的最优解，但 <code>BFS</code> 的思想非常经典</p></blockquote><blockquote><p>解题思路：将所有 <code>0</code> 视作腐烂的橘子，<code>1</code> 视作正常的橘子，每个腐烂的橘子每天会导致其周围四个方向的橘子腐烂，则求每一个 <code>1</code> 到最近的 <code>0</code> 的距离，等价于求该位置的橘子会在第几天腐烂？将所有 <code>0</code> 当作一个整体 <code>0</code>，每一轮往外扩散，搜索与该整体 <code>0</code> 相邻的 <code>1</code>， 初始 <code>step = 0</code>，每一轮 <code>BFS</code> 后 <code>step</code> 加 <code>1</code>，即<strong>“天数加1”</strong>。</p></blockquote><p><a href="https://leetcode.cn/problems/01-matrix/">542. 01 矩阵</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">updateMatrix</span>(<span class="hljs-params">self, mat: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>    m, n = <span class="hljs-built_in">len</span>(mat), <span class="hljs-built_in">len</span>(mat[<span class="hljs-number">0</span>])<br>    ret = [[-<span class="hljs-number">1</span>]*n <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m)]<br>    queue = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            <span class="hljs-keyword">if</span> mat[i][j] == <span class="hljs-number">0</span>:<br>                queue.append([i, j])<br>                ret[i][j] = <span class="hljs-number">0</span><br><br>    step = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">while</span> queue:<br>        k = <span class="hljs-built_in">len</span>(queue)<br>        temp = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>            x, y = queue.pop(<span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">for</span> dx, dy <span class="hljs-keyword">in</span> [[x, y-<span class="hljs-number">1</span>], [x, y+<span class="hljs-number">1</span>], [x-<span class="hljs-number">1</span>, y], [x+<span class="hljs-number">1</span>, y]]:<br>                <span class="hljs-keyword">if</span> dx &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> dx &lt; m <span class="hljs-keyword">and</span> dy &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> dy &lt; n:<br>                    <span class="hljs-keyword">if</span> mat[dx][dy] == <span class="hljs-number">1</span>:<br>                        ret[dx][dy] = step<br>                        mat[dx][dy] = <span class="hljs-number">0</span><br>                        temp.append([dx, dy])<br>        queue = temp<br>        step += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><hr><h2 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h2><blockquote><p>递归的题目一般很简单，关键是找出正确的递推公式……比如<strong>斐波那契数列</strong>（<a href="https://leetcode.cn/problems/fei-bo-na-qi-shu-lie-lcof/">剑指 Offer 10- I. 斐波那契数列</a>）、<strong>汉诺塔问题</strong>（<a href="https://leetcode.cn/problems/hanota-lcci/">面试题 08.06. 汉诺塔问题</a>）、<strong>青蛙跳台阶问题</strong>（<a href="https://leetcode.cn/problems/qing-wa-tiao-tai-jie-wen-ti-lcof/">剑指 Offer 10- II. 青蛙跳台阶问题</a>）<br>二叉树中也常见 👇👇👇</p></blockquote><p><a href="https://leetcode.cn/problems/evaluate-boolean-binary-tree/">6116. 计算布尔二叉树的值</a></p><blockquote><p>上来就想着用迭代法，傻傻的用栈实现，结果不熟练搞半天硬是没过……还是老老实实用递归写吧</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluateTree</span>(<span class="hljs-params">self, root: <span class="hljs-type">Optional</span>[TreeNode]</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    <span class="hljs-keyword">if</span> root.val == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">elif</span> root.val == <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">elif</span> root.val == <span class="hljs-number">2</span>:<br>        <span class="hljs-keyword">return</span> self.evaluateTree(root.left) <span class="hljs-keyword">or</span> self.evaluateTree(root.right)<br>    <span class="hljs-keyword">elif</span> root.val == <span class="hljs-number">3</span>:<br>        <span class="hljs-keyword">return</span> self.evaluateTree(root.left) <span class="hljs-keyword">and</span> self.evaluateTree(root.right)<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/path-sum/">112. 路径总和</a></p><blockquote><p>递归思路：减去当前节点的值，递归查询左子树或右子树</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">hasPathSum</span>(<span class="hljs-params">self, root: <span class="hljs-type">Optional</span>[TreeNode], targetSum: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">if</span> root.left == <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> root.right == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> targetSum == root.val<br>    <span class="hljs-keyword">return</span> self.hasPathSum(root.left, targetSum-root.val) <span class="hljs-keyword">or</span> self.hasPathSum(root.right, targetSum-root.val)<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/invert-binary-tree/">226. 翻转二叉树</a></p><blockquote><p>一眼递归：调换所有节点的左右子树即可</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">invertTree</span>(<span class="hljs-params">self, root: TreeNode</span>) -&gt; TreeNode:<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> root<br>    temp = root.right<br>    root.right = root.left<br>    root.left = temp<br>    self.invertTree(root.left)<br>    self.invertTree(root.right)<br>    <span class="hljs-keyword">return</span> root<br></code></pre></td></tr></table></figure><blockquote><p>优化一下，利用 <code>Python</code> 的多元赋值特性 👇👇👇</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">invertTree</span>(<span class="hljs-params">self, root: TreeNode</span>) -&gt; TreeNode:<br>    <span class="hljs-keyword">if</span> root == <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> root<br>    root.right, root.left = self.invertTree(root.left), self.invertTree(root.right)<br>    <span class="hljs-keyword">return</span> root<br></code></pre></td></tr></table></figure><hr><h2 id="回溯"><a href="#回溯" class="headerlink" title="回溯"></a>回溯</h2><blockquote><p>以下为回溯法的模板</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">result = []<br>def backtrack(路径, 选择列表):<br>    if 满足结束条件:<br>        result.add(路径)<br>        return<br>    <br>    for 选择 in 选择列表:<br>        做选择<br>        backtrack(路径, 选择列表)<br>        撤销选择<br></code></pre></td></tr></table></figure><blockquote><p>照着模板写题 👇👇👇</p></blockquote><p><a href="https://leetcode.cn/problems/permutations/">46. 全排列</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">permute</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>    self.ret = []<br>    self.backtrack(nums, [])<br>    <span class="hljs-keyword">return</span> self.ret<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">backtrack</span>(<span class="hljs-params">self, nums, path</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> nums:<br>        self.ret.append(path)  <span class="hljs-comment"># nums 已被选择完，则返回结果</span><br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)): <span class="hljs-comment"># 利用 for 循环来进行选择和撤销选择的操作</span><br>    <span class="hljs-comment"># 进入循环，选择 nums[i]，从 nums 中移除并加入 path</span><br>        self.backtrack(nums[:i]+nums[i+<span class="hljs-number">1</span>:], path+[nums[i]])<br>        <span class="hljs-comment"># 跳出循环，对下一个元素进行选择操作</span><br></code></pre></td></tr></table></figure><blockquote><p>会了模板题目再写其他题目就很就很简单了<br><strong>回溯三要素：路径、选择列表、结束条件</strong>！！！</p></blockquote><p><a href="https://leetcode.cn/problems/combinations/">77. 组合</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">combine</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span>, k: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>    self.nums = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n+<span class="hljs-number">1</span>)]<br>    self.k = k<br>    self.ret = []<br>    self.backtrack(self.nums, [])<br>    <span class="hljs-keyword">return</span> self.ret<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">backtrack</span>(<span class="hljs-params">self, nums, path</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(path) == self.k:<br>        self.ret.append(path)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>        self.backtrack(nums[i+<span class="hljs-number">1</span>:], path+[nums[i]])<br></code></pre></td></tr></table></figure><blockquote><p>优化一下，其实生成 <code>nums</code> 数组是多此一举，因为本题回溯用不到列表中被选择元素之前的元素<br>然后再剪枝， <code>path</code> 中组合数个数足够则停止</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">combine</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span>, k: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        self.n = n<br>        self.k = k<br>        self.ret = []<br>        self.backtrack(<span class="hljs-number">1</span>, [])<br>        <span class="hljs-keyword">return</span> self.ret<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backtrack</span>(<span class="hljs-params">self, num, path</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(path) == self.k:<br>            self.ret.append(path)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num, self.n+<span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(path) &lt; self.k:            <span class="hljs-comment"># 当前组合数个数小于 k 则继续</span><br>                self.backtrack(i+<span class="hljs-number">1</span>, path+[i])<br></code></pre></td></tr></table></figure><hr><h2 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h2><p><a href="https://leetcode.cn/problems/advantage-shuffle/">870. 优势洗牌</a></p><blockquote><p>这是一道周赛题，拿到手没思路，还是贪心写少了，也可以说没怎么练过，看了题解才会……题目的意思有点田忌赛马的味道，目的是使 <code>nums1</code> 中的元素尽最可能多的大于 <code>nums2</code> 中同位置的元素。</p></blockquote><blockquote><p>解题思路：<br><code>1</code>、将  <code>nums1</code> 和 <code>nums2</code> 排好序；<br><code>2</code>、从小到大遍历 <code>nums1</code>，对于任意最小元素 <code>nums1[i]</code>，有以下情况：<br>（1）若 <code>nums1[i]</code> 比 <code>nums2</code> 中的当前最小元素 <code>nums2[j]</code> 更大，则代表 <code>nums1</code> 中所有未匹配元素都比 <code>nums2[j]</code> 大，此时将 <code>nums1[i]</code> 视为 <code>“上等马”</code>，将其匹配至 <code>nums2[j]</code> 即可，再更新 <code>nums2</code> 中的最小元素为当中的次小元素。<br>（2）若 <code>nums1[i]</code> 比 <code>nums2</code> 中的任意元素都要小，则将其视为 <code>“下等马”</code>，反正都比不过，待后续随机匹配即可。<br><code>3</code>、遍历匹配的集合，若存在 <code>nums2</code> 中元素未被匹配成功的，将 <code>“下等马”</code> 数组的元素弹出与其匹配即可。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">advantageCount</span>(<span class="hljs-params">self, nums1, nums2</span>):<br>    n = <span class="hljs-built_in">len</span>(nums1)<br>    nums1.sort()<br>    sort2 = <span class="hljs-built_in">sorted</span>(nums2)                     <span class="hljs-comment"># nums2需要用另外空间排序，因为要保持其顺序用于对应</span><br><br>    good = &#123;num: [] <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> nums2&#125;         <span class="hljs-comment"># 上等马出战对应~</span><br>    inferior = []                             <span class="hljs-comment"># 下等马集合！</span><br>    index = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> num1 <span class="hljs-keyword">in</span> nums1:<br>        <span class="hljs-keyword">if</span> num1 &gt; sort2[index]:               <span class="hljs-comment"># 找到最小的“上等马”</span><br>            good[sort2[index]].append(num1)   <span class="hljs-comment"># 用数组存储避免 num2 中有相同元素</span><br>            index += <span class="hljs-number">1</span>                        <span class="hljs-comment"># 更新 num2 的最小元素</span><br>        <span class="hljs-keyword">else</span>:<br>            inferior.append(num1)             <span class="hljs-comment"># 否则视为“下等马”</span><br><br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> good.keys():<br>        <span class="hljs-keyword">if</span> good[key] == []:<br>            good[key].append(inferior.pop(<span class="hljs-number">0</span>)) <span class="hljs-comment"># 下等马按顺序弹出匹配</span><br>            <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):                        <span class="hljs-comment"># 按照匹配结果更新 num1 的排序</span><br>        <span class="hljs-keyword">if</span> good[nums2[i]]:                   <br>            nums1[i] = good[nums2[i]].pop(<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">else</span>:<br>            nums1[i] = inferior.pop(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">return</span> nums1<br></code></pre></td></tr></table></figure><hr><h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><h3 id="基础动态规划"><a href="#基础动态规划" class="headerlink" title="基础动态规划"></a>基础动态规划</h3><p>（1）序列型<br><a href="https://leetcode.cn/problems/longest-increasing-subsequence/">300. 最长递增子序列</a></p><blockquote><p>状态转移方程：<br>若 $num[j] &gt; num[i]$，则 $L_{i} &#x3D; \max \limits_{1&lt;&#x3D;j&lt; i}(L_{j}+1, L_{i})$</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lengthOfLIS</span>(<span class="hljs-params">self, nums</span>):<br>    dp = [<span class="hljs-number">0</span>]*(<span class="hljs-built_in">len</span>(nums))<br>    ret = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>        dp[i] = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i):<br>            <span class="hljs-keyword">if</span> nums[i] &gt; nums[j]:<br>                dp[i] = <span class="hljs-built_in">max</span>(dp[j]+<span class="hljs-number">1</span>, dp[i])<br>                <span class="hljs-keyword">if</span> dp[i] &gt; ret:<br>                    ret = dp[i]<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><p>（2）双序列型<br>（3）划分型<br>（4）区间型<br>（5）背包型</p><blockquote><p>以上为常见类型</p></blockquote><p>（6）状态压缩型<br>（7）树型</p><h3 id="记忆化搜索"><a href="#记忆化搜索" class="headerlink" title="记忆化搜索"></a>记忆化搜索</h3><p><a href="https://leetcode.cn/problems/number-of-people-aware-of-a-secret/">2327. 知道秘密的人数</a></p><blockquote><p>常规的动态规划思想是：直接用 <code>dp</code> 数组记录每天知道秘密的人数，最后返回 <code>dp[n]</code>，但由于 <code>forget</code> 的存在实现起来有点困难。</p></blockquote><blockquote><p>尝试改变思路：用 <code>dp</code> 数组 <strong>记录每天新增的人数</strong>，最后再统计第 <code>n</code> 天知道秘密的人数。由此可得状态转移方程：<br>第 i 天新增得知秘密的人数: $N_{i} &#x3D; \sum\limits_{j&#x3D;i-forget+1}^{i-delay}N_j$</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">peopleAwareOfSecret</span>(<span class="hljs-params">self, n, delay, forget</span>):<br>    dp = [<span class="hljs-number">0</span>]*(n+<span class="hljs-number">1</span>) <span class="hljs-comment"># 记录第index天能够得知秘密的人数</span><br>    ret = <span class="hljs-number">0</span><br>    dp[<span class="hljs-number">1</span>] = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n+<span class="hljs-number">1</span>):<br>    <span class="hljs-comment"># 第 i 天知道秘密的人只能在[i+delay, i+forget)的区间内传播秘密</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i+delay, i+forget):  <br>            <span class="hljs-keyword">if</span> j &lt;= n:<br>                dp[j] += dp[i]<br>                dp[j] %= (<span class="hljs-number">10</span> ** <span class="hljs-number">9</span> + <span class="hljs-number">7</span>)<br>    <br>    <span class="hljs-comment"># 统计在第 n 天还没有忘记的人的数量即可</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n+<span class="hljs-number">1</span>-forget, n+<span class="hljs-number">1</span>):<br>        ret += dp[i]<br>        ret %= (<span class="hljs-number">10</span> ** <span class="hljs-number">9</span> + <span class="hljs-number">7</span>)<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><h3 id="其他动态规划"><a href="#其他动态规划" class="headerlink" title="其他动态规划"></a>其他动态规划</h3><p><a href="https://leetcode.cn/problems/maximum-subarray/">53. 最大子数组和</a> （看清楚这道题与滑动窗口的区别）</p><hr><h2 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h2><h3 id="二进制表示"><a href="#二进制表示" class="headerlink" title="二进制表示"></a>二进制表示</h3><p>任意整数 $n$ 都可以表示成 $(a_{1}a_{2}···a_{k})_{2}$的形式，其中 $a_{i}&#x3D;{0, 1}$, $(1 \leq i \leq k)$ </p><blockquote><p>不妨设 $n$ 的二进制串为<br>$$n &#x3D; (a10···0)_{2} \tag{1}$$<br>其中 $a$ 表示若干个高位，$1$ 表示最低位的 $1$，其后为若干个 $0$。</p></blockquote><ul><li><strong>与运算（&amp;）</strong>：遇 <code>0</code> 得 <code>0</code></li></ul><blockquote><p>（1）$n-1$ 的二进制表示为：<br>$$n-1 &#x3D; (a01···1)_{2}, \tag{2}$$<br>该二进制串在式 $(1)$ 的基础上将最低位的 $1$ 及其后的 $0$ 进行了反转，将 $n$ 和 $n-1$ 按位与运算，即将 $(a10···0)_{2}$ 和 $(a01···1)_{2}$ 按位与，可以得到:<br>$$n \And (n-1) &#x3D; (a00···0)_{2}, \tag{3}$$<br>其中 $n$ 的二进制串中最低位的 $1$ 被移除。<br><strong>若 $n$ 是正整数，且 $n \And (n-1) &#x3D; 0$，则 $n$ 为 $2$ 的幂</strong>。<br>（2）$-n$ 的二进制表示：（在计算机中，负数按照补码规则存储，$-n$ 为 $n$ 的二进制表示的每一位取反再加上 $1$）<br>$$-n &#x3D; (\overline{a}01···1)_{2} + (1)_{2} &#x3D; (\overline{a}10···0)_{2}, \tag{4}$$<br>其中 $\overline{a}$ 为取反操作。将 $n$ 和 $n-1$ 按位与运算，即将 $(a10···0)_{2}$ 和 $(a01···1)_{2}$ 按位与，则可以得到:<br>$$n \And (-n) &#x3D; (b10···0)_{2}, \tag{5}$$<br>其中 $a \And \overline{a} &#x3D; b &#x3D; 0$。<br><strong>若 $n$ 是正整数，且 $b &#x3D; 0$，也即 $n \And (-n) &#x3D; n$，则 $n$ 为 $2$ 的幂</strong>。</p></blockquote><p><a href="https://leetcode.cn/problems/number-of-1-bits/">191. 位1的个数</a></p><blockquote><p>容易想到的是利用 $2$ 的<strong>幂</strong>对该32位二进制串进行按位与，若某一位为 $1$，则按位与的结果必不为 $0$。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">hammingWeight</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        ret = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">32</span>):<br>            <span class="hljs-keyword">if</span> n &amp; (<span class="hljs-number">1</span> &lt;&lt; i):<br>                ret += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> ret<br><br></code></pre></td></tr></table></figure><blockquote><p>优化一下，由于 $n-1$ 与 $n$ 的二进制串按位与运算可以移除最低位的 $0$，根据这一运算特点，我们可以：<br>（1）将 $n$ 与 $n-1$ 进行式 $(3)$ 中的与运算操作；<br>（2）消除最低位后将值重新赋给 $n$；<br>（3）重复以上步骤直至 $n$ 为 $0$。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">hammingWeight</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        ret = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">while</span> n:<br>            n = n &amp; n-<span class="hljs-number">1</span><br>            ret += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/reverse-bits/">190. 颠倒二进制位</a></p><blockquote><p>对于二进制串 <code>n = 11000010100101000001111010011101</code>，对 <code>n</code> 的每一位进行以下操作：<br>1）初始化 <code>ret = 0</code>；<br>2）将 <code>ret</code> 左移空出最后一位；<br>3）通过按位 <strong>或</strong> 运算将 <code>n</code> 的最后一位（<code>n &amp; 1</code>）加到 <code>ret</code> 的最后一位上；<br>4）再将 <code>n</code> 右移去掉最后一位。<br>0000000000000000000000000000000<strong>0</strong> or <strong>1</strong> &#x3D; 0000000000000000000000000000000<strong>1</strong><br>000000000000000000000000000000<strong>10</strong> or <strong>0</strong> &#x3D; 000000000000000000000000000000<strong>10</strong><br>00000000000000000000000000000<strong>100</strong> or <strong>1</strong> &#x3D; 00000000000000000000000000000<strong>101</strong><br>0000000000000000000000000000<strong>1010</strong> or <strong>1</strong> &#x3D; 0000000000000000000000000000<strong>1011</strong><br>000000000000000000000000000<strong>10110</strong> or <strong>1</strong> &#x3D; 000000000000000000000000000<strong>10111</strong><br>……<br>0<strong>1011100101111000001010010100000</strong> or <strong>1</strong> &#x3D; 0<strong>1011100101111000001010010100001</strong><br><strong>10111001011110000010100101000010</strong> or <strong>1</strong> &#x3D; <strong>10111001011110000010100101000011</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reverseBits</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        ret = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">32</span>):<br>            lastBit = n &amp; <span class="hljs-number">1</span><br>            ret = ret &lt;&lt; <span class="hljs-number">1</span> | lastBit<br>            n = n &gt;&gt; <span class="hljs-number">1</span>                <br>        <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><blockquote><p><strong>分治法</strong><br>注：在 <code>Python3</code> 中，<code>int</code> 具有任意长度。 因此，如果要获取 <code>32</code> 位数字，则必须使用 <code>0xffffffff</code>（即 <code>32</code> 个设置位，因此是 <code>0b11111111111111111111111111111111</code>）进行掩码：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reverseBits</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        M1 = <span class="hljs-number">1431655765</span> <span class="hljs-comment"># 01010101010101010101010101010101</span><br>        M2 = <span class="hljs-number">858993459</span>  <span class="hljs-comment"># 00110011001100110011001100110011</span><br>        M4 = <span class="hljs-number">252645135</span>  <span class="hljs-comment"># 00001111000011110000111100001111</span><br>        M8 = <span class="hljs-number">16711935</span>   <span class="hljs-comment"># 00000000111111110000000011111111</span><br><br>        n = n &gt;&gt; <span class="hljs-number">1</span> &amp; M1 &amp; <span class="hljs-number">0xffffffff</span> | (n &amp; M1) &lt;&lt; <span class="hljs-number">1</span> &amp; <span class="hljs-number">0xffffffff</span><br>        n = n &gt;&gt; <span class="hljs-number">2</span> &amp; M2 &amp; <span class="hljs-number">0xffffffff</span> | (n &amp; M2) &lt;&lt; <span class="hljs-number">2</span> &amp; <span class="hljs-number">0xffffffff</span><br>        n = n &gt;&gt; <span class="hljs-number">4</span> &amp; M4 &amp; <span class="hljs-number">0xffffffff</span> | (n &amp; M4) &lt;&lt; <span class="hljs-number">4</span> &amp; <span class="hljs-number">0xffffffff</span><br>        n = n &gt;&gt; <span class="hljs-number">8</span> &amp; M8 &amp; <span class="hljs-number">0xffffffff</span> | (n &amp; M8) &lt;&lt; <span class="hljs-number">8</span> &amp; <span class="hljs-number">0xffffffff</span><br><br>        <span class="hljs-keyword">return</span> n &gt;&gt; <span class="hljs-number">16</span> &amp; <span class="hljs-number">0xffffffff</span> | n &lt;&lt; <span class="hljs-number">16</span> &amp; <span class="hljs-number">0xffffffff</span><br></code></pre></td></tr></table></figure><ul><li><p><strong>或（|）运算</strong>：遇 <code>1</code> 得 <code>1</code></p></li><li><p><strong>异或（^）运算</strong>：同得 <code>0</code>，异得 <code>1</code></p></li></ul><blockquote><p>（1）n ^ n &#x3D; 0<br>（2）m ^ n &#x3D; n ^ m<br>（3）0 ^ n &#x3D; n<br>由以上可推知（4）n ^ n ^ n &#x3D; n</p></blockquote><p><a href="https://leetcode.cn/problems/single-number/">136. 只出现一次的数字</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">singleNumber</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        ret = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> nums:<br>            ret ^= num<br>        <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><hr><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>[1] <a href="http://data.biancheng.net/view/5.html">链表（单链表）的基本操作及C语言实现</a> <a id="cite_link_list"></a></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 数据结构 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>For Offersssss! （机器学习篇）</title>
      <link href="/2022/09/25/026-InterviewQs_ML/"/>
      <url>/2022/09/25/026-InterviewQs_ML/</url>
      
        <content type="html"><![CDATA[<p><strong>不重复造轮子了，直接放链接学习……相关描述为直接转载</strong></p><hr><h1 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a><strong>模型评估</strong></h1><h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><h3 id="准确率（查准率）"><a href="#准确率（查准率）" class="headerlink" title="准确率（查准率）"></a>准确率（查准率）</h3><p>所有 <strong>预测为正例</strong> 的样本中 <strong>真正例的占比</strong>，**<code>查准率</code>**，$$Precision&#x3D;\frac{TP}{TP+FP}$$</p><h3 id="召回率（查全率）"><a href="#召回率（查全率）" class="headerlink" title="召回率（查全率）"></a>召回率（查全率）</h3><p>所有 <strong>实际为正例</strong> 的样本中 <strong>真正例的占比</strong>，**<code>查全率</code>**，$$Recall&#x3D;\frac{TP}{TP+FN}$$</p><h3 id="F-score"><a href="#F-score" class="headerlink" title="F-score"></a>F-score</h3><p>基于查全率和查准率的 <strong>调和平均</strong>，$$\frac{1}{F1}&#x3D;\frac{1}{2}(\frac{1}{P}+\frac{1}{R}) \Rightarrow F1&#x3D;\frac{2 \times P \times R}{P+R}$$</p><h3 id="真正例率"><a href="#真正例率" class="headerlink" title="真正例率"></a>真正例率</h3><p><strong>正样本中预测为正例</strong>（预测对的正样本）的占比，$$TPR&#x3D;\frac{TP}{TP+FN}&#x3D;Recall$$</p><h3 id="假正例率"><a href="#假正例率" class="headerlink" title="假正例率"></a>假正例率</h3><p><strong>负样本中预测为正例</strong>（预测错的正样本）的占比，$$FPR&#x3D;\frac{FP}{TN+FP}$$</p><h3 id="P-R曲线与ROC曲线的区别"><a href="#P-R曲线与ROC曲线的区别" class="headerlink" title="P-R曲线与ROC曲线的区别"></a>P-R曲线与ROC曲线的区别</h3><p><a href="https://blog.csdn.net/Mr_health/article/details/96600580">ROC曲线和P-R曲线选择</a></p><blockquote><p><strong>P-R曲线与ROC曲线的区别与选择</strong><br><strong>（1）应用场景</strong><br><strong>ROC曲线</strong>：一般用于分类的场景，二分类多分类皆可。比如我们的测试集右正样本和负样本，通过测试就可以得到 <code>TPR</code> 和 <code>FPR</code>。<br><strong>P-R曲线</strong>：一般用于目标检测领域。通过我们前面的介绍，可以看到 <code>precision</code> 表示预测为正例中有多少是真正的正例，即表示出了预测的准不准。在目标检测领域，例如典型的飞机、船检测，都是从图像中检测出上述目标，那么不可避免的就会有背景 <code>(FP)</code> 出现，进而可以计算 <code>precision</code>。一个好的目标检测模型，<code>P-R</code> 曲线下的面积 <code>(AUC)</code> 越大。<br><strong>（2）正负样本不均衡下的鲁棒性</strong><br><strong>ROC曲线</strong>：正负样本不均衡下更稳定。在实际的数据集中经常会出现类不平衡 <code>(class imbalance)</code> 现象，即负样本比正样本多很多(或者相反)，而且测试数据中的正负样本的分布也可能随着时间变化。例如当负样本增加时，<code>TPR</code> 是不变的，根据下式可知，<code>FP</code> 同在分子和分母，则变化也不大，因此 <code>ROC</code> 的曲线变化不大。<br><strong>P-R曲线</strong>：受到正负样本分布影响较大。仍以上面的例子为例，当负样本增加时，<code>recall</code> 是不变的，根据下式可知，<code>FP</code> 在分母，则 <code>precision</code> 会明显的降低。</p></blockquote><blockquote><p><strong>总结</strong>：<br>（1）一般的分类任务，采用 <code>ROC</code> 曲线；<br>（2）当正负样本比例失调时，则 <code>ROC</code> 曲线变化不大，此时用 <code>PR</code> 曲线更加能反映出分类器性能的好坏。</p></blockquote><h2 id="模型检验方法"><a href="#模型检验方法" class="headerlink" title="模型检验方法"></a>模型检验方法</h2><p><a href="https://blog.csdn.net/qq_40006058/article/details/121548565">模型评估时常用的验证方法有哪些？</a></p><blockquote><p>（1）Holdout<br>（2）K-Fold<br>（3）自助法</p></blockquote><hr><h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a><strong>线性模型</strong></h1><h2 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h2><p><a href="https://zhuanlan.zhihu.com/p/56900935">逻辑回归（Logistic Regression）详解</a></p><blockquote><p><strong>（1）逻辑回归和线性回归的区别？</strong><br>解析：<br><img src="https://s2.loli.net/2022/09/12/1uEjlFBT4NAwXCd.png"><br>（2）<strong>逻辑回归为什么选择对数损失函数？</strong><br>解析：为了优化过程中得到的是全局最优解，一般要求<strong>逻辑回归的损失函数为凸函数</strong>，而相比于平方损失，对数函数的二阶导恒大于 $0$（可严格 <a href="https://blog.csdn.net/weixin_42486139/article/details/106018410">证明</a>）。</p></blockquote><h2 id="线性判别分析（LDA）"><a href="#线性判别分析（LDA）" class="headerlink" title="线性判别分析（LDA）"></a>线性判别分析（<a id="LDA">LDA</a>）</h2><p><code>Linear Discriminant Analysis, LDA</code> </p><p>通俗易懂的描述：采用 <strong>“投影后类内方差最小，类间方差最大”</strong> 的思想将高维空间的数据 <strong>降一维</strong> 投影到一条直线上（或超平面）。</p><blockquote><p><strong>与 <code>PCA</code> 降维的区别</strong><br><img src="https://s2.loli.net/2022/09/12/Fo1dPnZJHjWqT3M.png"></p></blockquote><hr><h1 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a><strong>优化方法</strong></h1><h2 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h2><h2 id="梯度下降法与牛顿法"><a href="#梯度下降法与牛顿法" class="headerlink" title="梯度下降法与牛顿法"></a>梯度下降法与牛顿法</h2><p>梯度下降的三种方式：批量梯度下降（BGD）、随机梯度下降法（SGD）、小批量梯度下降（MBGD）</p><p><a href="https://www.cnblogs.com/lyr2015/p/9010532.html">梯度下降法和牛顿法的总结与比较</a></p><blockquote><p><strong>梯度下降法与牛顿法的对比</strong><br><strong>（1）牛顿法</strong>：是通过求解目标函数的一阶导数为0时的参数，进而求出目标函数最小值时的参数。<br><strong>优点</strong>：收敛速度很快。海森矩阵的逆在迭代过程中不断减小，可以起到逐步减小步长的效果。<br><strong>缺点</strong>：海森矩阵的逆计算复杂，代价比较大，因此有了拟牛顿法（<a href="https://zhuanlan.zhihu.com/p/65987457">拟牛顿法</a>）。<br><strong>（2）梯度下降法</strong>：是通过梯度方向和步长，直接求解目标函数的最小值时的参数。<br>越接近最优值时，步长应该不断减小，否则会在最优值附近来回震荡。<br><img src="https://s2.loli.net/2022/09/12/IxFtrzK5sfXAqPo.png"></p></blockquote><h2 id="欠拟合（高偏差）和过拟合（高方差）"><a href="#欠拟合（高偏差）和过拟合（高方差）" class="headerlink" title="欠拟合（高偏差）和过拟合（高方差）"></a>欠拟合（高偏差）和过拟合（高方差）</h2><p>以下搬运自：<a href="https://juejin.cn/post/7130571832490459143">入坑机器学习：六，逻辑回归</a></p><blockquote><p><strong>从左至右为：</strong><br>（1）线性模型，欠拟合，不能很好地适应我们的训练集；<br>（2）似乎最合适；<br>（3）四次方的模型，过于强调拟合原始数据，而丢失了<strong>算法的本质：预测新数据</strong>。可以看出，若给出一个新的值使之预测，它将表现的很差，即过拟合。</p></blockquote><p><img src="https://s2.loli.net/2022/08/13/f5zLBMKhJAEC6bj.jpg"></p><blockquote><p><strong>如何处理过拟合问题?</strong><br>（1）<strong>特征选择</strong>。丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一 些模型选择的算法来帮忙（例如 PCA）；<br>（2）<strong>正则化</strong>。保留所有的特征，但是减少参数的大小（Magnitude，指参数的数值大小）。高次项导致了过拟合的产生，使高次项的系数接近于 0 即可。</p></blockquote><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>常见的代价函数（以<strong>逻辑回归</strong>为例）：</p><p><strong>均方误差 MSE：</strong>$J(\theta)&#x3D;\frac{1}{2m}[\sum_{i&#x3D;1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}+\lambda\sum_{j&#x3D;1}^{m}\theta_{j}^{2}]$</p><p><strong>交叉熵 CE：</strong>$J(\theta)&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}[-y^{(i)}log(h_{\theta}(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]+\lambda\sum_{j&#x3D;1}^{m}\theta_{j}^{2}$</p><blockquote><p><strong>（1）$\lambda$ 称为正则化参数或惩罚系数（同 SVM 中的系数 C），增加 $\lambda\sum_{j&#x3D;1}^{m}\theta_{j}^{2}$ 一项为什么能减少参数 $\theta$ 的值？</strong><br>解析：一般情况下，正则化系数 $\lambda$ 的设定值会比较大，$J(\theta)$ 在使得代价最小的过程中只能通过减少 $\theta$ 的值来实现。<br><strong>（2）为什么代价函数要具有非负性？</strong><br>解析：保证设计的目标函数有下界，才能确保在优化过程中使其不断减小，直至收敛。</p></blockquote><h2 id="正则化（Regularization）和稀疏性（重点）"><a href="#正则化（Regularization）和稀疏性（重点）" class="headerlink" title="正则化（Regularization）和稀疏性（重点）"></a>正则化（Regularization）和稀疏性（重点）</h2><p><strong>什么叫正则化？</strong><br>正则化，也称规则化，旨在给需要训练的目标函数（作用于模型参数）加上一些规则和限制。</p><p><a href="https://www.cnblogs.com/dataanaly/p/12972315.html">L2正则化、L1正则化与稀疏性</a></p><p><a href="https://blog.csdn.net/u011426016/article/details/119836598">L1正则化及其稀疏性的傻瓜解释</a></p><blockquote><p><strong><code>L1</code> 和 <code>L2</code> 的区别</strong><br>（1）<code>L1</code> 是模型各个参数的<strong>绝对值之和</strong>；<code>L2</code> 是模型各个参数的<strong>平方和的开方</strong>值。<br>（2）<code>L1</code> 会趋向于产生少量的特征，而其他的特征都是 <code>0</code>。因为最优的参数值很大概率出现在坐标轴上，这样就会导致<strong>某一维的权重为 <code>0</code> ，产生稀疏权重矩阵</strong>。<br>（3）<code>L2</code> 会选择更多的特征，这些特征都会接近于 <code>0</code>。最优的参数值很小概率出现在坐标轴上，因此每一维的参数都不会是<code>0</code>。当最小化 $||w||$时，就会使<strong>每一项趋近于 <code>0</code><strong>。<br>（4）<code>L1</code> 的作用是为了</strong>矩阵稀疏化</strong>。假设的是模型的参数取值满足拉普拉斯分布。<br>（5）<code>L2</code> 的作用是为了使<strong>模型更平滑</strong>，得到更好的泛化能力。假设的是参数是满足高斯分布。</p></blockquote><blockquote><p><strong>Dropout</strong><br>通过随机删除⽹络中的神经单元，使得模型拟合不依赖于任何⼀个特征；</p></blockquote><hr><h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a><strong>决策树</strong></h1><h2 id="ID3，C4-5，CART决策树"><a href="#ID3，C4-5，CART决策树" class="headerlink" title="ID3，C4.5，CART决策树"></a>ID3，C4.5，CART决策树</h2><hr><h1 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a><strong>贝叶斯分类器</strong></h1><h2 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h2><h3 id="推导以及假设条件"><a href="#推导以及假设条件" class="headerlink" title="推导以及假设条件"></a>推导以及假设条件</h3><hr><h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a><strong>支持向量机</strong></h1><ul><li>当训练样本线性可分时，通过硬间隔（<code>hard margin</code>）最⼤化，学习⼀个 <strong>线性可分⽀持向量机</strong>； </li><li>当训练样本近似线性可分时，通过软间隔（<code>soft margin</code>）最⼤化，学习⼀个 <strong>线性⽀持向量机</strong>； </li><li>当训练样本线性不可分时，通过核技巧和软间隔最⼤化，学习⼀个 <strong>⾮线性⽀持向量机</strong>；</li></ul><!-- ## 原问题与对偶问题 --><h2 id="原问题"><a href="#原问题" class="headerlink" title="原问题"></a>原问题</h2><p>样本空间中任意点 $x$ 到超平面 $(w, b)$ 的距离可定义为 $$r&#x3D;\frac{|w^{T}x+b|}{||w||},$$ 超平面的分类策略为 $$\begin{cases} w^{T}x+b \geq +1, y_{i}&#x3D;+1; \\ w^{T}x+b \leq -1, y_{i}&#x3D;-1. \end{cases}$$ 两个异类 <strong>支持向量</strong>（使得上式取等号的样本点，即$|w^{T}x+b|&#x3D;1$）到超平面的距离和（即间隔）为 $$margin&#x3D;\frac{2}{||w||},$$ 优化目标即为 <strong>找到一个超平面，使得间隔最大</strong>，问题定义为 $$\max_{w,b} \frac{2}{||w||},$$ 等价于 $$\min_{w,b} \frac{1}{2}||w||^{2},$$ $$\text{s.t. } y_{i}(w^{T}x_{i}+b) \ge 1, i&#x3D;1,2,…,m.$$</p><h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p><a href="https://blog.csdn.net/qq_18846849/article/details/125868760">SVM 对偶形式的推导，以及 Gurobi 求解对偶形式 SVM</a></p><p><strong>注：</strong>优化问题的约束条件一般写成标准形式，即左小右大且右侧为 $0$，因此约束条件转化为 $1-(y_{i}(w^{T}x_{i}+b) \le 0$ 的形式再构造拉格朗日函数。</p><p>显然，原问题是一个凸二次规划问题，其拉格朗日函数可以写成：<br>$$L(w,b,\alpha)&#x3D;\frac{1}{2}||w||^{2}+\sum_{i&#x3D;1}^{m}\alpha_{i}(1-(y_{i}(w^{T}x_{i}+b))),$$ 由 $\alpha_{i}(1-(y_{i}(w^{T}x_{i}+b) \le 0$，可知 $L(w,b,\alpha) \le \frac{1}{2}||w||^{2}$，即 <strong>拉格朗日函数是原问题的一个下界</strong>。</p><p>我们要想找到最接近原问题最优值的一个下界，就需要 <strong>求出下界的最大值</strong>，即 $\max_{\alpha} L(w,b,\alpha)$，则原问题可转化为优化问题： $$\min_{w,b}\max_{\alpha} L(w,b,\alpha),$$ 根据拉格朗日对偶性，原问题（极小极大问题）等价于其对偶问题（极大极小问题）： $$\max_{\alpha}\min_{w,b} L(w,b,\alpha).$$ </p><blockquote><p><strong>为什么要将原问题转换成对偶问题？</strong><br>（1）<strong>问题求解。</strong>$\min_{w,b}\max_{\alpha} L(w,b,\alpha)$ 形式的极小极大问题只能对 $\alpha$ 求导，转化为极大极小问题 $\max_{\alpha}\min_{w,b} L(w,b,\alpha)$ 后才能对求导以求最优解；<br>（2）<strong>降低复杂度。</strong>将求特征向量 $w$（复杂度为 $O(d)$）转化为求比例系数 $\alpha$（复杂度为 $O(m)$），其中 $d$ 为样本的维度，$m$ 为样本数量。 <strong>关键在于：只有支持向量的比例系数 $\alpha$ 才非 $0$<strong>；<br>（3）</strong>约束转换。</strong>将不等式约束转换成等式约束；<br>（4）方便核函数的引入？</p></blockquote><blockquote><p><strong>对偶变换成立的条件是什么？</strong><br>解析：<code>KKT</code> 条件。KKT条件是原问题与对偶问题等价的必要条件，当原问题是凸优化问题时，变为充要条件。</p></blockquote><p>求解的问题明确了，接下来就是简化问题和推导约束条件的过程，将 $L(w,b,\alpha)$ 分别对 $w$ 和 $b$ 求偏导可得 $$\begin{aligned} \frac{\partial L}{\partial w}&amp;&#x3D;w-\sum_{i&#x3D;1}^{m}\alpha_{i}y_{i}x_{i}, \\ \frac{\partial L}{\partial b}&amp;&#x3D;-\sum_{i&#x3D;1}^{m}\alpha_{i}y_{i},\end{aligned}$$ 令上述两式为 $0$，可得 $$w&#x3D;\sum_{i&#x3D;1}^{m}\alpha_{i}y_{i}x_{i},$$ $$\sum_{i&#x3D;1}^{m}\alpha_{i}y_{i}&#x3D;0,\tag{*}$$ 将 $w&#x3D;\sum_{i&#x3D;1}^{m}\alpha_{i}y_{i}x_{i}$ 带入拉格朗日函数 $L(w,b,\alpha)$ 中，可得 $$\begin{aligned} L(w,b,\alpha)&amp;&#x3D;\frac{1}{2}w^{T}w+\sum_{i&#x3D;1}^{m}\alpha_{i}(1-(y_{i}(w^{T}x_{i}+b))) \\ &amp;&#x3D;\frac{1}{2}\sum_{i&#x3D;1}^{n}\sum_{j&#x3D;1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}+\sum_{i&#x3D;1}^{m}\alpha_{i}-\sum_{i&#x3D;1}^{m}\sum_{j&#x3D;1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}-b\sum_{i&#x3D;1}^{m}\alpha_{i}y_{i} \\ &amp;&#x3D;\sum_{i&#x3D;1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i&#x3D;1}^{n}\sum_{j&#x3D;1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}, \end{aligned}$$ 将上述简化后的问题结合 $(*)$ 式的约束即为最后的对偶问题：<br>$$\max_{\alpha}\sum_{i&#x3D;1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i&#x3D;1}^{m}\sum_{j&#x3D;1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j},$$ $$\begin{aligned} \text{s.t. } &amp;\sum_{i&#x3D;1}^{m}\alpha_{i}y_{i}&#x3D;0, \\ &amp;\alpha_{i} \ge 0, i&#x3D;1,2,…,m. \end{aligned}$$</p><h2 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h2><p><a href="https://www.cnblogs.com/hyb221512/p/13661754.html">KKT 条件是非线性问题有最优解的充分必要条件</a></p><p>等式约束问题可按拉格朗日乘数法直接求解。对于不等式约束问题<br>$$\min f(x)$$ $$\text{s.t. } g(x) \le 0$$<br>其最优解必然在可行域内部或边界上，设该问题的拉格朗日函数为 $L(x,\lambda)&#x3D;f(x)+\lambda g(x)$。<br>（1）当最优解 $x^{*}$ 落在可行域内部，即 $g(x^{*})&lt;0$，此时约束条件 $g(x) \le 0$ 无效，等效于令 $$\begin{cases} \lambda &#x3D; 0 \\ \nabla_{x} L(x,\lambda)&#x3D;\nabla f(x)&#x3D;\frac{\partial f(x)}{\partial x}&#x3D;0, \end{cases}$$ 即可求得最优解；<br>（2）当最优解 $x^{*}$ 落在可行域边界，即 $g(x^{*})&#x3D;0$，此时可视为等式优化问题，同样利用拉格朗日函数求解，令 $$\nabla_{x} L(x,\lambda)&#x3D;\nabla f(x^{*})+\lambda \nabla g(x^{*}),$$ 可以证明 $\nabla f(x^{*})$ 与 $\nabla g(x^{*})$ 方向必然相反（数学证明未找到），因此必然存在 $\lambda &gt;0$ 使得 $\nabla f(x^{*})+\lambda \nabla g(x^{*})&#x3D;0$ 有解，该解即为最优解。<br>整合以上两种情况，必然存在以下条件：<br>$$\begin{cases} \nabla_{x} L(x,\lambda)&#x3D;\nabla f(x)+\lambda \nabla g(x)&#x3D;0 \\ g(x) \le 0 \\ \lambda \ge 0 \\ \lambda g(x) &#x3D; 0 \end{cases}$$<br>以上条件合称 <strong>KKT 条件</strong>，分别为定常方程式、原始可行性、对偶可行性，以及互补松弛性；</p><p>由于 <code>SVM</code> 的 <strong>原问题为不等式约束问题</strong>，其对偶问题必须满足 <code>KKT</code> 条件，即：<br>$$\begin{cases} \nabla_{x} L(w,b,\alpha)&#x3D;0 \\ 1-y_{i}(w^{T}x_{i}+b) \le 0 \\ \alpha_{i} \ge 0 \\ \alpha_{i} (1-y_{i}(w^{T}x_{i}+b)) &#x3D; 0. \end{cases}$$</p><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><blockquote><p><strong>为什么引入核函数？</strong><br>解析：实际数据往往是线性不可分的，通过引入核函数将特征映射到更高维的空间实现分类。</p></blockquote><blockquote><p><strong>核函数的引入是否大大增加了计算复杂度？</strong><br>解析：没有。核函数将特征进行低维到高维的转化，但实际上是在低维的空间上计算得到高维空间的效果，不会出现维度爆炸或计算复杂度急剧上升的问题。</p></blockquote><blockquote><p><strong>核函数的本质？</strong><br>解析：核函数是一个低维的计算结果，并没有真正实现低维到高维的映射，而是 <strong>核函数低维运算的结果等价于映射到高维时向量点积的值</strong>。<br>假设高维空间中两个向量 $x_{i},x_{j}$ 的内积为 $I&#x3D;&lt;x_{i},x_{j}&gt;$，映射之后高维空间中的内积为 $I’&#x3D;&lt;\Phi(x_{i}),\Phi(x_{j})&gt;$，尝试找到一个函数使得 **低维空间的向量 $x_{i},x_{j}$ 的运算结果为 $K(x_{i},x_{j})&#x3D;I’$**，其中 $K(x,y)$ 即为核函数。</p></blockquote><p>明确了以上三个问题，核函数的引入动机和作用方式也就很清楚了。</p><h3 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h3><table>    <tr>        <td>名称</td><td>表达式</td><td>参数</td><td>优势</td><td>-</td>    </tr>    <tr>          <td>线性核</td><td>$\kappa(x_{i},x_{j})=x_{i}^{T}x_{j}$</td><td>-</td>    </tr>    <tr>        <td>多项式核</td><td>$\kappa(x_{i},x_{j})=(x_{i}^{T}x_{j})^d$</td><td>$d(\ge 1)$，为多项式的次数</td><td>较线性核而言，可解决非线性问题</td>    </tr>    <tr>        <td>高斯核</td><td>$\kappa(x_{i},x_{j})=\exp(-\frac{||x_{i}-x_{j}||^{2}}{2\sigma^{2}})$</td><td>$\sigma(> 0)$，为高斯核的带宽</td><td>可映射到无限维</td>    </tr>    <tr>        <td>指数核</td><td>$\kappa(x_{i},x_{j})=\exp(-\frac{||x_{i}-x_{j}||}{2\sigma^{2}})$</td><td>$\sigma(> 0)$</td><td>调整向量距离为 $L1$ 距离，对参数的依赖性降低</td>    </tr>    <tr>        <td>拉普拉斯核</td><td>$\kappa(x_{i},x_{j})=\exp(-\frac{||x_{i}-x_{j}||}{\sigma})$</td><td>$\sigma(> 0)$</td><td>基本等价于指数核，对参数的敏感性更低</td>    </tr>    <tr>        <td>Sigmoid核</td><td>$\kappa(x_{i},x_{j})=\tanh(\beta x_{i}^{T}x_{j}+\theta)$</td><td>$\tanh$ 为双曲正切函数，$\beta > 0, \theta < 0$</td><td>-</td>    </tr></table><h2 id="软间隔和正则化"><a href="#软间隔和正则化" class="headerlink" title="软间隔和正则化"></a>软间隔和正则化</h2><p>软间隔：<strong>允许某些样本不满足约束</strong> $y_{i}(w^{T}x_{i}+b) \ge 1$。</p><h3 id="优化目标的改进"><a href="#优化目标的改进" class="headerlink" title="优化目标的改进"></a>优化目标的改进</h3><p>在 <strong>最大化间隔的同时，使得不满足约束的样本尽可能少</strong>，优化目标写成：$$\min_{w,b} \frac{1}{2}||w||^{2}+C \sum_{i&#x3D;1}^{m}\mathcal{L}(z),$$ 以上问题可称为正则化问题，其中 $C$ 为惩罚系数，$C$ 越大，上式会迫使更多的样本满足约束，反之更少。</p><blockquote><p><strong>软间隔SVM的欠拟合和过拟合解决办法？</strong><br>解析：欠拟合-增大 $C$，过拟合-减小 $C$。</p></blockquote><h2 id="SVM的优劣势"><a href="#SVM的优劣势" class="headerlink" title="SVM的优劣势"></a>SVM的优劣势</h2><blockquote><p>SVM的优点：<br>（1）解决了 <strong>小样本情况下</strong> 的机器学习分类问题。<br>（2）由于使用核函数方法 <strong>克服了维数灾难和非线性可分的问题</strong>，所以向高维空间映射时没有增加计算的复杂度。（由于支持向量机算法的最终决策函数只由少数的支持向量所确定，所以计算的复杂性取决于支持向量的数目，而不是整个样本空间的维数）。</p></blockquote><blockquote><p>SVM的缺点：<br>（1）支持向量机算法对大规模训练样本难以实施，这是因为支持向量算法借助二次规划求解支持向量，这其中会设计 $m$ 阶矩阵的计算，所以 <strong>矩阵阶数很大时将耗费大量的机器内存和运算时间</strong>。<br>（2）经典的 <code>SVM</code> 只给出二分类的算法，而在数据挖掘中，一般要解决多分类的分类问题，而支持向量机 <strong>对于多分类问题解决效果并不理想</strong>。<br>（3）现在常用的 <code>SVM</code> 理论都是使用 **固定惩罚系数 $C$**，但是正负样本的两种错误造成的损失是不一样的。</p></blockquote><h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><hr><h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><h2 id="BP神经网络"><a href="#BP神经网络" class="headerlink" title="BP神经网络"></a>BP神经网络</h2><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p><a href="https://svyj.github.io/2022/07/02/027-InterviewQs_DL/">For Offersssss! （深度学习篇）</a></p><hr><h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a><strong>聚类</strong></h1><h2 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h2><h3 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h3><blockquote><p><strong>算法步骤：</strong><br>假设样本总数为 <code>N</code>，<code>K-means</code> 聚类法可以简单表示为一下几个步骤：<br>（1）在样本中随机选取 <code>K</code> 个点，作为每一类的中心点。<br>（2）计算剩下 <code>n-K</code> 个样本点到每个聚类中心的距离（距离有很多种，假设这里采用欧式距离）。对于每一个样本点，将它归到和他距离最近的聚类中心所属的类。<br>（3）将每一个类内的所有点取平均值，计算出新的聚类中心。<br>（4）重复步骤 <code>2</code> 和 <code>3</code> 的操作，直到所有的聚类中心不再改变。</p></blockquote><blockquote><p><strong>K值的选择？</strong><br>（1）<strong>拍脑袋法：</strong>$K \approx \sqrt{n&#x2F;2}$<br>（2）<strong>肘部法则：</strong>纵坐标 <code>cost</code> 一般可以定义为 <code>Sum of the Squared Srrors（SSE）</code>，$SSE&#x3D;\sum_{p \in C_{i}}|p-m_{i}|^{2}$，其中 $C_{i}$ 代表第 $i$ 个簇，$p$ 是簇 $C_{i}$ 里的样本点，$m_{i}$ 是簇的中心。如下左图为一个比较完美的肘部图，但不是所有的问题都能画出这样的图，因此对于 <strong>右图无法确认的情况则需要借助其他的方法</strong>。<br><img src="https://s2.loli.net/2022/09/13/CYuRPFq5laKmJ67.png"><br>（3）<strong>轮廓系数法：</strong>定义样本点 $X_{i}$ 的轮廓系数（<code>Silhouette Coefficient</code>）为 $$S_{i}&#x3D;\frac{b(i)-a(i)}{max(a(i),b(i))}&#x3D;\begin{cases} 1-\frac{a_{i}}{b_{i}}, a_{i}&lt;b_{i}, \\ 0, a_{i}&#x3D;b_{i}, \\ \frac{b_{i}}{a_{i}}-1, a_{i}&gt;b_{i}, \end{cases}$$ 易知 $S_{i} \in [-1,1]$，其中 $a_{i}$ 是 $X_{i}$ 和同簇的其他样本的平均距离，称为 <strong>簇内差异度</strong>。 $b_{i}$ 是 $X_{i}$ 和最近簇中所有样本的平均距离，称为 <strong>簇间差异度</strong>。 由上式可知：<br><strong>i）</strong>若 $a_{i}&lt;b_{i}$，则 $S_{i} \rightarrow 1$，表示簇内相似度足够高，样本 $X_{i}$ 的聚类结果合理；<br>**ii）**若 $a_{i}&gt;b_{i}$，则 $S_{i} \rightarrow -1$，表示簇内相似度不足，样本 $X_{i}$ 应该聚类到另外的簇。<br><strong>iii）</strong>若 $a_{i}&#x3D;b_{i}$，则 $S_{i} \approx 0$，说明样本 $X_{i}$ 在两个簇的边界上。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">K</span>-Means:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, k, iter_num</span>):<br>        self.k = k<br>        self.iter_num = iter_num<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, data</span>):<br>        data = np.asarray(data)<br>        np.random.seed(<span class="hljs-number">0</span>)<br><br>        self.cluster_centers = data[np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(data), self.k)]<br>        self.classes = np.zeroes(<span class="hljs-built_in">len</span>(data))<br><br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">iter</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iter_num):<br>            <span class="hljs-comment"># 计算每个样本 x 距离聚类中心的距离，更新样本类别为最近的聚类中心所属类别</span><br>            <span class="hljs-keyword">for</span> index, x <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data):<br>                distance = np.sqrt(np.<span class="hljs-built_in">sum</span>((x - self.cluester_center)**<span class="hljs-number">2</span>, axis=<span class="hljs-number">1</span>))<br>                self.classes[index] = distance.argmin()<br><br>            <span class="hljs-comment"># 更新 K 个聚类中心</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.k):<br>                self.cluster_center[i] = np.mean(data[self.classes == i], axis=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, data</span>):<br>        data = np.asarray(data)<br>        pred = np.zeroes(<span class="hljs-built_in">len</span>(data))<br><br>        <span class="hljs-keyword">for</span> index, x <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data):<br>            distance = np.sqrt(np.<span class="hljs-built_in">sum</span>((x - self.cluester_center)**<span class="hljs-number">2</span>, axis=<span class="hljs-number">1</span>))<br>            pred[index] = distance.argmin()<br><br>        <span class="hljs-keyword">return</span> pred<br></code></pre></td></tr></table></figure><h3 id="高斯混合模型（GMM）"><a href="#高斯混合模型（GMM）" class="headerlink" title="高斯混合模型（GMM）"></a>高斯混合模型（GMM）</h3><p><img src="https://s2.loli.net/2022/09/13/RWABFgZ4jfyniYb.png"></p><hr><h1 id="降维"><a href="#降维" class="headerlink" title="降维"></a><strong>降维</strong></h1><h2 id="线性判别分析（LDA）-1"><a href="#线性判别分析（LDA）-1" class="headerlink" title="线性判别分析（LDA）"></a><a href="#LDA">线性判别分析（LDA）</a></h2><h2 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h2><p><code>Principal Component Analysis, PCA</code></p><p>通俗易懂的描述：将数据投影到<strong>方差最大</strong>（方差越大，信息量越多）的几个<strong>相互正交</strong>（线性无关）的方向上。</p><p><img src="https://s2.loli.net/2022/09/13/fzKxRHwjiVXhamI.jpg"></p><h2 id="K近邻"><a href="#K近邻" class="headerlink" title="K近邻"></a>K近邻</h2><hr><h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a><strong>集成学习</strong></h1><h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h2><h2 id="随机森林（理论基础）"><a href="#随机森林（理论基础）" class="headerlink" title="随机森林（理论基础）"></a>随机森林（理论基础）</h2><hr><h1 id="Sklearn"><a href="#Sklearn" class="headerlink" title="Sklearn"></a><strong>Sklearn</strong></h1><p><a href="https://aistudio.baidu.com/aistudio/projectdetail/1888288">『行远见大』 Python 进阶篇：Sklearn 库</a></p><hr>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>For Offersssss! （深度学习篇）</title>
      <link href="/2022/09/17/027-InterviewQs_DL/"/>
      <url>/2022/09/17/027-InterviewQs_DL/</url>
      
        <content type="html"><![CDATA[<h1 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h1><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><blockquote><p>提取图像中的<strong>局部特征</strong>，其原理是通过许多的<strong>卷积核</strong> <code>(filter, kernel)</code> 在图片上进行滑动提取特征。</p></blockquote><h3 id="卷积核"><a href="#卷积核" class="headerlink" title="卷积核"></a>卷积核</h3><ul><li>特性：<strong>权值共享</strong>（一个卷积核滑动提取图像的某一个特征，进而带来平移不变性）、<strong>局部连接</strong>（感知图像的局部信息）</li><li>参数：核大小 <code>Kernel Size</code>、步长 <code>Stride</code>、卷积核数量（通道） <code>Channel</code>、边界填充 <code>Padding</code></li><li>输出大小计算公式：$ H_{out} &#x3D; \frac{H_{in} - K + 2P}{S} + 1 $，$ W_{out} &#x3D; \frac{W_{in} - K + 2P}{S} + 1 $</li></ul><p>注：卷积核通常设定为奇数的原因？（保证padding时候，图像的两边依然相对称），3×3 的卷积核是最优选择。</p><h3 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h3><ul><li>概念：输出特征图上的像素点<strong>对于原图的映射区域的大小</strong></li></ul><!-- ### 卷积的类型 --><h3 id="1-×-1-卷积"><a href="#1-×-1-卷积" class="headerlink" title="1 × 1 卷积"></a><code>1 × 1</code> 卷积</h3><blockquote><p><strong>作用：</strong><br>（1）实现信息的跨通道交互和整合；<br>（2）对卷积核通道数进行降维或升维，减小参数量。</p></blockquote><h3 id="K-×-K-卷积"><a href="#K-×-K-卷积" class="headerlink" title="K × K 卷积"></a><code>K × K</code> 卷积</h3><blockquote><p><em>⼤多数情况下</em>，通过 <strong>堆叠较⼩的卷积核</strong> ⽐直接采⽤ <strong>单个更⼤的卷积核</strong> 会更加有效。**<br>如：两层 <code>3×3</code> 卷积会比一层 <code>5×5</code> 卷积有效（<strong>深度更大</strong>），且 <strong>参数量更小</strong>（$3\times3\times2&lt;5\times5$）</p></blockquote><p><strong><code>3 × 3</code> 卷积的实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Conv2d</span>(<span class="hljs-params">img, in_channels, out_channels, kernels, bias=<span class="hljs-literal">False</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span></span>):<br>    N, C, H, W = img.shape<br>    kh, kw = kernels.shape<br>    <span class="hljs-keyword">assert</span> C == in_channels<br> <br>    <span class="hljs-keyword">if</span> padding:<br>        img = np.pad(img, ((<span class="hljs-number">0</span>, <span class="hljs-number">0</span>),(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>),(padding, padding),(padding, padding)), <span class="hljs-string">&#x27;constant&#x27;</span>)<br> <br>    out_h = (H + <span class="hljs-number">2</span>*padding - kh) // stride + <span class="hljs-number">1</span><br>    out_w = (W + <span class="hljs-number">2</span>*padding - kw) // stride + <span class="hljs-number">1</span><br> <br>    outputs = np.zeros([N, out_channels, out_h, out_w])<br><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>        <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(out_channels):<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(in_channels):<br>                <span class="hljs-keyword">for</span> h <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(out_h):<br>                    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(out_w):<br>                        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(kh):<br>                            <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(kw):<br>                                outputs[n][c][h][w] += img[n][i][h*stride+x][w*stride+y] * kernels[x][y]<br>    <span class="hljs-keyword">return</span> outputs<br><br>img = np.random.random(size=(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>))<br>kernel = np.random.random(size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>outputs = Conv2d(img, in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">3</span>, kernels=kernel, bias=<span class="hljs-literal">False</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(img, kernel, outputs)<br></code></pre></td></tr></table></figure><h3 id="分组卷积"><a href="#分组卷积" class="headerlink" title="分组卷积"></a>分组卷积</h3><h3 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h3><h3 id="可变形卷积"><a href="#可变形卷积" class="headerlink" title="可变形卷积"></a>可变形卷积</h3><h3 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h3><p><strong>棋盘效应</strong></p><blockquote><p><strong>产生原因</strong>：卷积计算后差异被放大所导致的明暗相间的网格状。</p></blockquote><table>    <tr>        <td><center><img src="https://s2.loli.net/2022/09/11/lxOh9f7HPikeaXC.png" width=350></center>stride=2, kernel size=2</td>        <td><center><img src="https://s2.loli.net/2022/09/11/7brA8lPyew43BfN.png" width=350></center>stride=2, kernel size=3</td>        <td><center><img src="https://s2.loli.net/2022/09/11/SsV4YNE1tpPA9RH.png" width=350></center>stride=2, kernel size=4</td>    </tr></table><blockquote><p><strong>解决办法</strong>：（推荐使用第二种）<br>（1）<strong>设置的 kernel size 能够被 stride 整除</strong>；（无法从根本上解决问题）<br>（2）<strong>采用双线性插值+卷积</strong>，即先利用双线性插值上采样，以减小插值的像素和原输入像素值的大小差异，再进行卷积可以避免棋盘效应的产生。</p></blockquote><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><blockquote><p>主要用来缩小特征图的大小，<strong>减少计算量和参数量</strong>(降维)、<strong>减少过拟合</strong>问题、缓解卷积层对位置的敏感度。</p></blockquote><h3 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h3><h4 id="反卷积（转置卷积）"><a href="#反卷积（转置卷积）" class="headerlink" title="反卷积（转置卷积）"></a>反卷积（转置卷积）</h4><p><img src="https://s2.loli.net/2022/08/22/ZUdfGSpN1zYBaiL.jpg"></p><h4 id="插值法（双线性插值）"><a href="#插值法（双线性插值）" class="headerlink" title="插值法（双线性插值）"></a>插值法（双线性插值）</h4><p><img src="https://s2.loli.net/2022/08/22/jZamER6BcAiTuQ5.jpg"></p><h4 id="反池化（Unpooling-x2F-Unsampling）"><a href="#反池化（Unpooling-x2F-Unsampling）" class="headerlink" title="反池化（Unpooling &#x2F; Unsampling）"></a>反池化（Unpooling &#x2F; Unsampling）</h4><p><img src="https://s2.loli.net/2022/08/22/AKgez8c4MyXDOfl.jpg"></p><h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><blockquote><p>全连接层中的每个神经元与其前一层的所有神经元进行中全连接，全连接层可以整合卷积层或者池化层中具有类别区分性的<strong>局部信息</strong></p></blockquote><h3 id="FC、MLP-和-DNN"><a href="#FC、MLP-和-DNN" class="headerlink" title="FC、MLP 和 DNN"></a>FC、MLP 和 DNN</h3><blockquote><p><strong><code>MLP</code> （多层感知机）</strong>：<strong>至少两层</strong> 全连接层（FC）堆叠得到的模型，能够解决单层感知机无法解决的 <strong>非线性问题</strong>；<br><strong><code>DNN</code>（深度神经网络）</strong>：网络深度较大的 <code>MLP</code>。</p></blockquote><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><blockquote><p><code>Dropout</code> 被大量利用于全连接网络，而由于卷积自身的稀疏化以及稀疏化的 <code>ReLU</code> 函数（置零特性）的大量使用等原因，<code>Dropout</code> 策略也逐渐被淘汰。</p></blockquote><ul><li><strong>训练过程中 <code>Dropout</code> 函数的 <code>re-scale</code></strong></li></ul><p>设 <code>dropout</code> 在训练过程中以一定概率 <code>p</code> 使神经元 <strong>失活</strong>，则为了保证随机失活后被保留的神经元的期望保持不变，则需要对神经元的数值进行 <code>re-scale</code>，不妨设输入序列为 $X&#x3D;[1, 1, 1, 1, 1]$，设以失活概率 <code>p=0.2</code> 对该序列进行 <code>mask</code> 后的序列为 $X’&#x3D;[1, 0, 1, 1, 1]$，易知 $E(X)&#x3D;1, E(X’)&#x3D;0.8$，因此为了 <strong>保证期望不变</strong>，需要对 $X’$ 的数值进行 $\frac{1}{1-p}$ 倍的放大（<code>re-scale</code>），本例中 $E(\frac{X’}{0.8})&#x3D;1$。</p><h2 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a>归一化层</h2><p><img src="https://s2.loli.net/2022/08/21/pDJPXOkGY9brFwh.png"></p><blockquote><p>将输入的特征图 $x \in \mathbb{R}^{N \times C \times H \times W}$ 比喻成一摞书，这摞书总共有 $N$ 本，每本有 $C$ 页，每页有 $H$ 行，每行有 $W$ 个字符；<br>（1）<code>BN</code> 是在 <code>batch</code> 上，对 <code>N、H、W</code> 做归一化，而保留通道 <code>C</code> 的维度。<code>BN</code> 对较小的 <code>batch size</code> 效果不好。**<code>BN</code> 适用于固定深度的前向神经网络**，如 <code>CNN</code>，不适用于 <code>RNN</code>；<br><strong>注：</strong><code>BN</code> 相当于把这些书按页码一一对应地加起来，再除以每个页码下的字符总数：$N×H×W$；<br>（2）<code>LN</code> 在通道方向上，对 <code>C、H、W</code> 归一化，主要 <strong>对 <code>RNN</code> 和 <code>Transformer</code> 效果明显</strong>；<br><strong>注：</strong><code>LN</code>  相当于把每一本书的所有字加起来，再除以这本书的字符总数：$C×H×W$；<br>（3）<code>IN</code> 在图像像素上，对 <code>H、W</code> 做归一化，用在 <strong>风格化迁移</strong>；<br><strong>注：</strong><code>IN</code> 相当于把一页书中所有字加起来，再除以该页的总字数：$H×W$；<br>（4）<code>GN</code> 将 <code>channel</code> 分组，然后再做归一化；<br><strong>注：</strong><code>GN</code> 相当于把一本 $C$ 页的书平均分成 $G$ 份，每份成为有 $C&#x2F;G$ 页的小册子，对每个小册子做 <code>Norm</code>。</p></blockquote><h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><ul><li><strong>算法步骤</strong></li></ul><p>（0）给定 <code>1</code> 个 $batch&#x3D;m$ 的 <strong>输入</strong> $X&#x3D;[x^{(1)},x^{(2)},…,x^{(m)}]$，在神经网络某一层对应的中间隐藏值为 $Z&#x3D;[z^{(1)},z^{(2)},…,z^{(m)}]$；</p><p>（1）<strong>计算均值</strong>： $\mu &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m}z^{(i)}$</p><p>（2）<strong>计算方差</strong>： $\sigma^{2}&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}(z^{(i)}-\mu)^2$</p><p>（3）<strong>归一化</strong>： $z_{norm}^{(i)}&#x3D;\frac{z^{(i)}-\mu}{\sqrt{\sigma^2 + \epsilon}}$ ,其中 $\epsilon$ 是为了防止除 <code>0</code>。</p><p>（4）<strong>缩放和平移</strong>： $\tilde{z}^{(i)}&#x3D;\gamma z_{norm}^{(i)}+\beta$ ,其中 $\gamma$ 和 $\beta$ 是要训练的超参数。</p><img src="https://s2.loli.net/2022/08/21/qdv4ZFoj61n98hK.png" height=300px><ul><li><strong>Why BN works?</strong></li></ul><blockquote><p><strong>BN 的作用</strong><br>（1）<strong>加速训练</strong>，可以使用更大的学习率；<br>（2）解决 <strong>梯度消失和梯度爆炸</strong>；<br>（3）有 <strong>轻微的正则化作用</strong>（相当于给隐藏层加入噪声），神经网络无需再使用 <code>dropout</code> 和 <code>L2</code> 正则。</p></blockquote><blockquote><p><strong>有效的原因？</strong>（说法有很多种，以下为个人见解）<br>（1）<strong>加速训练：</strong>归一化使得每层输入的特征都处于同一分布或尺度，这种情况下的数据拟合更高效；（理论解释：对特征进⾏了归⼀化，其对应的等⾼线会显得很圆（下右图），在梯度下降进⾏求解时能较快的收敛。）<br>（2）<strong>梯度问题：</strong>数据归一化到区间 [-1,1] 内，能够有效避免梯度弥散，且减少了链式求导法则带来的梯度依赖；<br>（3）<strong>正则化：</strong>每次使用的是当前 <code>batch</code> 的均值和方差而不是整个数据集的均值和方差，引入了些微噪声。因此，<code>Batch size</code> 越大，正则化效果越弱。</p></blockquote><p><img src="https://s2.loli.net/2022/09/06/C5UtNPxuw1gdO4F.png"></p><ul><li><strong>Python实现</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyBN</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, momentum, eps, num_features</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        初始化参数值</span><br><span class="hljs-string">        :param momentum: 追踪样本整体均值和方差的动量</span><br><span class="hljs-string">        :param eps: 防止数值计算错误</span><br><span class="hljs-string">        :param num_features: 特征数量</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        self.running_mean = <span class="hljs-number">0</span><br>        self.running_var = <span class="hljs-number">1</span><br><br>        self.momentum = momentum<br><br>        self.eps = eps<br>        self.beta = np.zeros(shape=(num_features, ))<br>        self.gamma = np.ones(shape=(num_features, ))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">batch_norm</span>(<span class="hljs-params">self, x</span>):<br>        x_mean = x.mean(axis=<span class="hljs-number">0</span>)<br>        x_var = x.var(axis=<span class="hljs-number">0</span>)<br><br>        self.running_mean = (<span class="hljs-number">1</span>-self.momentum)*x_mean + self._momentum*self.running_mean<br>        self.running_var = (<span class="hljs-number">1</span>-self.momentum)*x_var + self._momentum*self.running_var<br><br>        x_hat = (x-x_mean) / np.sqrt(x_var+self.eps)<br>        y = self.gamma*x_hat + self.beta<br><br>        <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure><h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><blockquote><p>在 <code>RNNs</code> 网络中，不同的 <code>mini-batch</code> 可能具有 <strong>不同的输入序列长度（深度）</strong>，计算统计信息比较困难，而且测试序列长度不能大于最大训练序列长度；</p></blockquote><ul><li><strong>算法步骤</strong></li></ul><p>（0）对于前向神经网络的<strong>第 $l$ 个隐藏层</strong>（等价于 <code>RNNs</code> 中 $x^{l}$ 时刻对应的隐藏层），令 $a^{l}$ 表示输入向量（前层网络输出加权后的向量），$H$ 表示隐藏单元数量；</p><p>（1）<strong>计算均值</strong>：$\mu^{l}&#x3D;\frac{1}{H}\sum_{i&#x3D;1}^{H}a_{i}^{l}$，在 <strong>通道维度</strong> 上求每一个 <code>token</code> 的均值（方差同理）；</p><p>（2）<strong>计算方差</strong>：$\sigma^{l}&#x3D;\sqrt{\frac{1}{H}\sum_{i&#x3D;1}^{H}(a_{i}^{l}-\mu^{l})^{2}}$；</p><p>（3）<strong>归一化</strong>：$z_{norm}^{l}&#x3D;\frac{z^{l}-\mu^{l}}{\sqrt{(\sigma^{l})^{2} + \epsilon}}$；</p><p>（4）<strong>缩放和平移</strong>：$\tilde{z}^{l}&#x3D;\gamma z_{norm}^{l}+\beta$。</p><blockquote><p><strong>LN 的优势：</strong>：无需批训练，适用于 <code>batch size</code> 为 <code>1</code> 的数据输入</p></blockquote><blockquote><p><strong>一个很好的解释：</strong><br>深度学习里的正则化方法就是 <strong>“通过把一部分不重要的复杂信息损失掉，以此来降低拟合难度以及过拟合的风险，从而加速了模型的收敛”<strong>。<br><code>Normalization</code> 目的就是让分布稳定下来（降低各维度数据的方差）。<br>&amp;nbsp;<br>不同正则化方法的区别只是操作的信息维度不同，即选择损失信息的维度不同。<br>在 <code>CV</code> 场景中，各个 <code>Channel</code> 维度中的信息原封不动，因为数据在不同 <code>channel</code> 中的信息很重要，如果</strong>对 <code>channle</code> 维度进行归一化将会损失不同 <code>channel</code> 的差异信息。</strong><br>而 <code>NLP</code> 中不同 <code>batch</code> 样本的信息关联性不大，而且由于不同的句子长度不同，强行归一化会损失不同样本间的差异信息，所以就没在 <code>batch</code> 维度进行归一化，而是选择 <code>Layer Normalization</code>，只考虑的句子内部维度的归一化。 可以认为 <code>NLP</code> 应用场景中一个样本内部维度间是有关联的，所以在信息归一化时，<strong>对样本内部差异信息进行一些损失，反而能降低方差。</strong><br>&amp;nbsp;<br>总结：<strong>选择什么样的归一化方式，取决于你关注数据的哪部分信息。如果某个维度信息的差异性很重要，需要被拟合，那就别在那个维度进行归一化。</strong></p></blockquote><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p><strong>激活函数和归一化的顺序问题？</strong><br>若激活函数为 ReLU，则必须将 BN 置于激活前，否则经过 ReLU 后的失活神经元会造成归一化的偏移和抖动。若激活函数为 Sigmoid 或 tanh，则顺序根据实验效果而定。</p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>$$f(x)&#x3D;\frac{1}{1+e^{-x}}$$</p><blockquote><p><strong>劣势：</strong>（1）梯度弥散（消失或爆炸）；（2）求导耗时；（3）不关于原点对称；（4）收敛速度慢；</p></blockquote><p><strong>激活函数关于原点对称的重要性：</strong><br>Sigmoid 的非原点对称的特点导致所有参数的梯度方向都是同向的；而深度学习模型的参数在更新时，不一定都朝着同一（正向&#x2F;反向）梯度方向更新，因此 sigmoid 这一特点使得模型收敛速度减慢了；</p><h3 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h3><p>$$f(x)&#x3D;\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$</p><blockquote><p><strong>优势：</strong>（1）关于原点对称；（2）收敛速度比 sigmoid 更快；<br><strong>劣势：</strong>（1）梯度弥散（消失或爆炸）；</p></blockquote><p><strong>为什么 tanh 比 sigmoid 收敛快？</strong><br>tanh 激活函数关于原点对称，训练过程中梯度可以朝着两个方向呈现 zigzag 形式的更新，更容易达到最优值。</p><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>$$f(x)&#x3D;\begin{cases} x, x&gt;0, \\ 0, x&lt;0 \end{cases}$$</p><blockquote><p><strong>优势：</strong>（1）求导运算量小；（2）有效避免梯度消失；（3）置零能够缓解过拟合。<br><strong>劣势：</strong>（1）神经元死亡且不会复活；</p></blockquote><hr><h1 id="循环神经网络（RNN）"><a href="#循环神经网络（RNN）" class="headerlink" title="循环神经网络（RNN）"></a>循环神经网络（RNN）</h1><p><img src="https://s2.loli.net/2022/08/19/AZ94NPjQwvSmVTH.png"></p><blockquote><p><strong><code>RNN</code> 和 <code>CNN</code> 中的梯度问题，为什么 <code>RNN</code> 更容易梯度消失或爆炸？</strong><br>（1）**<code>RNN</code><strong>：长时间序列的传播过程会用到前一时刻的信息，导致 <code>n</code> 个 <strong>相同的</strong> 雅可比矩阵 W <strong>连续相乘</strong>；（</strong>解决方法：梯度截断<strong>）<br>（2）</strong><code>CNN</code><strong>：每层的</strong>参数矩阵** W 不同；（<strong>解决方法：激活函数，正则化等</strong>）</p></blockquote><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p><img src="https://s2.loli.net/2022/08/19/T9l3UcGDe7qmFAP.png"></p><blockquote><p><strong>三个门</strong>：<br>（1）<strong>遗忘门（<code>Forget</code>）</strong>：最左侧 <code>Sigmoid</code>，选择保留（遗忘）<strong>旧的信息</strong>；<br>（3）<strong>输入门（<code>Input</code>）</strong>：中间的 <code>Sigmoid + tanh</code>，选择保留（遗忘）<strong>新的信息</strong>；<br>（3）<strong>输出门（<code>Output</code>）</strong>：最右侧 <code>Sigmoid + tanh</code>，<strong>选择输出</strong>内容。</p></blockquote><blockquote><p><strong>LSTM 如何解决梯度问题？</strong><br>当前的 <code>cell informaton</code> 是通过 <code>input gate</code> 控制之后叠加的，<code>RNN</code> 是叠乘。</p></blockquote><blockquote><p><strong>为什么既存在tanh和sigmoid，而不统一采用一样的?</strong><br><code>sigmoid</code> 用于<strong>门控处理</strong>（一般不可替换），<code>tanh</code> 用于<strong>状态输出</strong>（可替换）</p></blockquote><h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p><img src="https://s2.loli.net/2022/08/19/otYMmSWRE3yZcpw.png"></p><blockquote><p>进一步<strong>解决长期记忆</strong>和<strong>反向传播中的梯度等问题</strong></p></blockquote><blockquote><p><strong>两个门</strong>：<br>（1）<strong>更新门（<code>Update</code>）</strong>：两个 <code>Sigmoid</code>，<strong>合并了 <code>LSTM</code> 中的输入门和遗忘门</strong>，分别选择保留（遗忘）新&#x2F;旧的信息；<br>（2）<strong>重置门（<code>Reset</code>）</strong>：一个 <code>tanh</code>，选择输出内容。</p></blockquote><hr><h1 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h1><p>根据<strong>真实分布</strong> <code>p-data(x)</code> 的训练数据，通过训练学习得到<strong>近似于真实的分布</strong> <code>p-model(x)</code> 的新样本数据。</p><h2 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h2><h2 id="Pixel-RNN-x2F-CNN"><a href="#Pixel-RNN-x2F-CNN" class="headerlink" title="Pixel RNN &#x2F; CNN"></a>Pixel RNN &#x2F; CNN</h2><blockquote><p>从左上角开始定义 <strong>之前的像素</strong>，生成图像 <code>x</code> 的 <strong>条件概率分布</strong> 为：<br>$$p(x)&#x3D;\prod_{i&#x3D;1}^{n}p(x_{i}|x_{1},…,x_{i-1})$$<br><strong>注</strong>：对于 <code>Pixel RNN</code> 而言，公式中的每一项的输出概率 $p(x_{i})$ 都依赖于之前所有概率 $p(x_{1}),…,p(x_{i-1})$；<br>而 <code>Pixel CNN</code> 在训练时可以 <strong>并行计算</strong> 公式中的每一项，然后进行参数更新，因此 <strong>训练速度远快于 <code>Pixel RNN</code></strong></p></blockquote><blockquote><p><strong>训练方式</strong>：<br>（1）<strong>前向传播</strong>：根据上面的公式求出似然<br>（<code>Pixel RNN</code> 需要 <strong>从左上到右下串行</strong> 走一遍，而 <code>Pixel CNN</code> 并行计算所有像素点）；<br>（2）<strong>最大化似然</strong> 以对参数 <strong>做一轮更新</strong>。<br><strong>测试方式</strong>：<br><code>Pixel RNN</code> 和 <code>Pixel CNN</code> 都要 <strong>从左上角开始逐个像素点地生成图片</strong></p></blockquote><blockquote><p><code>Pixel RNN / CNN</code> 的优劣势：<br><strong>优势</strong>：是一种 <strong>可优化的显式密度模型</strong>，可以通过似然评估生成质量；<br><strong>劣势</strong>：由于需要逐像素点生成图像，实际应用中的 <strong>速度很慢</strong>。</p></blockquote><h2 id="变分自编码器（VAE）"><a href="#变分自编码器（VAE）" class="headerlink" title="变分自编码器（VAE）"></a>变分自编码器（VAE）</h2><p><strong>待补充……</strong></p><h2 id="生成对抗网络（GAN）"><a href="#生成对抗网络（GAN）" class="headerlink" title="生成对抗网络（GAN）"></a>生成对抗网络（GAN）</h2><h3 id="核心思路"><a href="#核心思路" class="headerlink" title="核心思路"></a>核心思路</h3><blockquote><p><code>GAN</code> 中我们定义了两个网络：<strong>生成器</strong> 和 <strong>判别器</strong><br><strong>生成器</strong>：利用随机噪声 $z$ <strong>生成尽可能真的样本</strong> 以骗过判别器。<br><strong>判别器</strong>：辨别区分 <strong>生成器生成的假样本</strong> 和 <strong>训练集中抽出来的真样本</strong>。</p></blockquote><blockquote><p><strong>对抗形式：</strong><br>$$\min_{\theta_{g}}\max_{\theta_{d}}[\mathbb{E}_{x \sim p_{data}} \log D_{\theta_{d}}(x) + \mathbb{E}_{z \sim p_{z}} \log (1-D_{\theta_{d}}(G_{\theta_{g}}(z)))]$$<br>其中第一项为输入真实数据 $x \sim p_{data}$，第二项为输入生成数据 $G_{\theta_{g}}(z)$，优化过程：<br><strong>对判别器网络训练 K 次，使目标函数在判别器参数 $\theta_{d}$ 下最大化，</strong><br>（1）使得 $D_{\theta_{d}}(x)$ 接近 1，即将真实样本 $x$ 判定为真；<br>（2）使得 $D_{\theta_{d}}(G_{\theta_{g}}(z))$ 接近 0，即将生成样本 $G_{\theta_{g}}(z)$ 判定为假；<br><strong>对生成器网络训练 1 次，使目标函数在生成器参数 $\theta_{g}$ 下最小化，</strong><br>（3）使得 $D_{\theta_{d}}(G_{\theta_{g}}(z))$ 接近 1，即骗过判别器将生成样本 $G_{\theta_{g}}(z)$ 判定为真；</p></blockquote><blockquote><p><strong>生成分布和真实分布之间的差异衡量</strong>：<br>（1）<code>KL</code> 散度:<br>$$KL(P(x)||Q(x)) &#x3D; \sum_{x \in X}[P(x) \log\frac{P(x)}{Q(x)}]&#x3D;\mathbb{E}_{x \sim P(x)}[\log\frac{P(x)}{Q(x)}],$$<br>然而，<code>KL</code> 散度具有不对称性，即 $KL(P||Q) \neq KL(Q||P)$<br>（2）<code>JS</code> 散度:<br>$$JS(P(x)||Q(x)) &#x3D; \frac{1}{2}KL(P(x)||\frac{P(x)+Q(x)}{2}) + \frac{1}{2}KL(Q(x)||\frac{P(x)+Q(x)}{2}),$$<br><code>JS</code> 散度则具有对称性，即 $JS(P||Q) &#x3D; JS(Q||P)$，且值域范围为 $[0, 1]$</p></blockquote><h3 id="原始-GAN-存在的问题"><a href="#原始-GAN-存在的问题" class="headerlink" title="原始 GAN 存在的问题"></a>原始 GAN 存在的问题</h3><p><strong>（1）梯度消失</strong><br>判别器网络的训练令其过于强大，导致生成分布和真实分布不重叠或者重叠较小，</p><table>    <tr>        <td><center><img src="https://s2.loli.net/2022/08/21/TicY8DWJeyP1bvK.png" width=350></center></td>        <td><center><img src="https://s2.loli.net/2022/08/21/siZRotHApnczJeG.png" width=350></center></td>    </tr></table><blockquote><p><strong>当两个分布没有重叠部分时，</strong>此时的 <code>JS</code> 散度为<br>$$JS(P_{data}(x)||P_{g}(x))&#x3D;\frac{1}{2}\mathbb{E}_{x \sim P_{data}(x)}[\log\frac{2P_{data}(x)}{P_{data}(x)+P_{g}(x)}] + \frac{1}{2}\mathbb{E}_{x \sim P_{g}(x)}[\log\frac{2P_{g}(x)}{P_{data}(x)+P_{g}(x)}],$$<br>不妨设 $P_{data}(x)&#x3D;0, P_{g}(x)\neq0$ 或 $P_{data}(x)\neq0, P_{g}(x)&#x3D;0$，则 $JS(P_{data}(x)||P_{g}(x))&#x3D;\log2$，此时生成器的 <strong>梯度消失</strong>，无法更新。</p></blockquote><blockquote><p><strong>当两个分布有重叠部分时，</strong>有文献指出，两个分布在高维空间是很难相交的，<strong>即使相交，其相交部分其实是高维空间中的一个低维流形，可以忽略不计</strong>，则此时的 <code>JS</code> 散度依然为常数，生成器依然 <strong>梯度消失</strong>，无法更新。</p></blockquote><p><strong>（2）模式崩塌（Mode Collapse）</strong><br>崩塌的概念是针对 <strong>生成</strong> 和 <strong>判别</strong> 的 <strong>对抗模式</strong> 而言的，是指 GAN 生成了与真实样本相同的样本，但 <strong>生成样本不具多样性（或说存在大量重复的样本）</strong>，但在这种情况下 <strong>判别器却认为生成器的性能优越</strong>。</p><p>如左下图中的绿色样本，右下图中的重复样本。</p><table>    <tr>        <td><center><img src="https://s2.loli.net/2022/08/21/GfF1DmuV58W6nLZ.png" width=350></center></td>        <td><center><img src="https://s2.loli.net/2022/08/21/vje6TaShCUuLB1F.png" width=350></center></td>    </tr></table><h3 id="Wasserstein-GAN（W-GAN）"><a href="#Wasserstein-GAN（W-GAN）" class="headerlink" title="Wasserstein GAN（W-GAN）"></a>Wasserstein GAN（W-GAN）</h3><p>引入了 <code>Wasserstein</code> 距离（也称 <code>Earth-Mover (EM)</code> 距离），<strong>无论两个分布多远，都有梯度</strong>，相对 <code>KL</code> 散度与 <code>JS</code> 散度具有优越的平滑特性，理论上可以解决 <strong>真实分布和生成分布不重叠或重叠部分可忽略时</strong> 出现的<strong>梯度消失问题</strong>。</p><p>将两个分布 $p$ 和 $q$ 看成两堆土，如下图所示，希望把其中的一堆土移成另一堆土的位置和形状，有很多种可能的方案。推土代价被定义为移动土的量乘以土移动的距离，在所有的方案中，存在一种推土代价最小的方案，这个代价就称为两个分布的 <code>Wasserstein</code> 距离。</p><p><img src="https://s2.loli.net/2022/09/01/Dt9acjyQOHkdmfX.png"></p><p><code>Wasserstein</code> 距离的形式化的表达式如下：$$W(p,q)&#x3D;\inf\limits_{\gamma\sim\prod(p,q)}E_{x,y\sim\gamma}[||x-y||]$$其中， $\prod(p,q)$ 表示​分布 $p$ 和 $q$ 组合起来的所有可能的<strong>联合分布</strong>的集合。</p><p>对于每一个可能的联合分布 $\gamma$，可以从中<strong>采样一对样本​</strong> $x$ 和​​​ $y$，$(x,y)\sim\gamma$，并计算出这对样本的距离 $||x-y||$，计算该联合分布 $\gamma$ 下，<strong>样本对距离的期望值</strong> $E_{x,y\sim\gamma}[||x-y||]$。</p><p>用推土的方式理解就是，$E_{x,y\sim\gamma}[||x-y||]$ 是<strong>在 $\gamma$ 路径规划下，把 $p$ 移动成​ $q$ 的消耗</strong>，而 <code>Wasserstein</code> 距离就是在<strong>“最优路径规划”下的最小消耗</strong>，也即<strong>所有可能的联合分布中能够对这个期望值取到的下界</strong>。</p><h3 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h3><p><strong>待补充……</strong></p><hr><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><table>    <tr>        <td><center><img src="https://s2.loli.net/2022/09/03/94btVRgkqF2GpHJ.png" width=350>Transformer（Encoder（左），Decoder（右））</center></td>        <td><center><img src="https://s2.loli.net/2022/09/03/8WQqKJyeM63lULj.png" width=350>Multi-head Self-attention</center></td>    </tr></table><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><blockquote><p><code>Encoder</code> 由 <code>N=6</code> 个相同的 <code>layers</code> 组成, 每个 <code>layers</code> 的结构：<br>（1）<strong>多头注意力层</strong>（Multi-head attention layer）；<br>（2）<strong>前馈层</strong>（Feed Forward Layer）；<br>每个 <code>sub-layer</code> 都加了残差连接和归一化。</p></blockquote><h3 id="Positional-Embedding"><a href="#Positional-Embedding" class="headerlink" title="Positional Embedding"></a>Positional Embedding</h3><h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h3><h4 id="多头注意力计算过程"><a href="#多头注意力计算过程" class="headerlink" title="多头注意力计算过程"></a>多头注意力计算过程</h4><p>以 <code>Head</code> 数等于 <code>4</code> 为例<br><img src="https://s2.loli.net/2022/09/02/v3ME1jfh2zWV5eL.png"></p><h4 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h4><blockquote><p><strong>（1）求出 $Q, K, V$</strong></p></blockquote><p>这里是求 <code>MultiHead</code> 的 $Q,K,V$，所以 <code>Shape</code> 为 (batch, head数, 词数，d_model&#x2F;head数)</p><ul><li>首先，通过定义的 $W^q,W^k,W^v$ 求出 <code>SelfAttention</code> 的 $Q,K,V$，此时 $Q,K,V$ 的 <code>Shape</code> 为 (batch, 词数, d_model)，对应代码为 <code>linear(x)</code>；</li><li>分成多头，即将 <code>Shape</code> 由 (batch, 词数, d_model) 变为 (batch, 词数, head数，d_model&#x2F;head数)。对应代码为 <code>view(nbatches, -1, self.h, self.d_k)</code>；</li><li>最终交换 “词数” 和 “head数” 这两个维度，将 head数 放在前面，最终 <code>shape</code> 变为 (batch, head数, 词数，d_model&#x2F;head数)。对应代码为 <code>transpose(1, 2)</code>。</li></ul><blockquote><p><strong>（2）通过 <code>attention</code> 函数计算出 <code>Attention</code> 结果</strong></p></blockquote><p>$$Attention(Q,K,V)&#x3D;softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$$</p><p>这里 <code>x</code> 的 <code>shape</code> 为 (batch, head数, 词数，d_model&#x2F;head数)，<code>self.attn</code> 的 <code>shape</code> 为 (batch, head数, 词数，词数)</p><blockquote></blockquote><p><strong>（重点）注：<code>self-attention</code> 为什么要除以 $\sqrt{d_{k}}$<strong>（<a href="https://blog.csdn.net/samuelzhoudev/article/details/120417922">参考</a>）<br><strong>目的：</strong>使得 <code>attention</code> 的结果处于一个合适的区间（数值不大不小），结果中个别数值偏大或整体偏小都会导致梯度消失（解释如下），同时也起到归一化的作用；<br><strong>解释：个别数值偏大</strong> 会导致数据数量级差距过大，<code>softmax</code> 会将概率分布（<code>p=1</code>）全部分配给最大值的标签，导致其他类别对应的偏导向量值为 $0$；</strong>整体数值偏小</strong> 则会导致梯度过小，会导致在前馈或反馈的过程中梯度消失。<br><strong>解决方案：</strong>对 <code>attention</code> 的结果进行 <code>scale</code>，缩小数据范围，扩大数据差异；<br><strong>数学证明：</strong>假设 $Q, K$ 均服从期望为 $0$，方差为 $1$ 的概率分布，则 $$E(Q)&#x3D;E(K)&#x3D;E(QK)&#x3D;0, D(Q)&#x3D;D(K)&#x3D;1,$$可得<br>$$D(QK)&#x3D;D(\sum_{i&#x3D;0}^{d_{k}}q_{i}k_{i})&#x3D;d_{k}\times1&#x3D;d_{k},$$由 $D(\frac{X}{a})&#x3D;\frac{D(X)}{a^{2}}$ 可得$$D(\frac{QK}{\sqrt{q_{k}}})&#x3D;\frac{d_{k}}{(\sqrt{d^{k}})^{2}}&#x3D;1$$</p><blockquote></blockquote><blockquote><p><strong>（3）合并多个 <code>head</code> 的结果</strong></p></blockquote><p>即将 <code>x</code> 的 <code>shape</code> 由 (batch, head数, 词数，d_model&#x2F;head数)，再变为 (batch, 词数，d_model)</p><ul><li>首先，交换 “<code>head数</code>”和“词数”，这两个维度，结果为(batch, 词数, head数, d_model&#x2F;head数)，对应代码为：<code>x.transpose(1, 2).contiguous()</code>；</li><li>然后将 “head数” 和 “d_model&#x2F;head数” 这两个维度合并，结果为(batch, 词数，d_model)。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 按注意力公式计算结果</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">attention</span>(<span class="hljs-params">query, key, value</span>):<br>    d_k = query.size(-<span class="hljs-number">1</span>)<br>    scores = torch.matmul(query, key.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) / math.sqrt(d_k)<br>    p_attn = scores.softmax(dim=-<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> torch.matmul(p_attn, value)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadedAttention</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, h, d_model</span>):<br>        <span class="hljs-built_in">super</span>(MultiHeadedAttention, self).__init__()<br>        <span class="hljs-keyword">assert</span> d_model % h == <span class="hljs-number">0</span><br><br>        self.d_k = d_model // h<br>        self.h = h<br><br>        self.linears = [<br>            nn.Linear(d_model, d_model),<br>            nn.Linear(d_model, d_model),<br>            nn.Linear(d_model, d_model),<br>            nn.Linear(d_model, d_model),<br>        ]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        nbatches = x.size(<span class="hljs-number">0</span>)<br><br>        query, key, value = [<br>            linear(x).view(nbatches, -<span class="hljs-number">1</span>, self.h, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>            <span class="hljs-keyword">for</span> linear, x <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.linears, (x, x, x))<br>        ]<br><br>        x = attention(query, key, value)<br><br>        x = (x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(nbatches, -<span class="hljs-number">1</span>, self.h * self.d_k))<br><br>        <span class="hljs-keyword">return</span> self.linears[-<span class="hljs-number">1</span>](x)<br><br><br><span class="hljs-comment"># 定义8个head，词向量维度为512的多头注意力层</span><br>model = MultiHeadedAttention(<span class="hljs-number">8</span>, <span class="hljs-number">512</span>)<br><br><span class="hljs-comment"># 传入一个batch_size为2， 7个单词，每个单词为512维度</span><br>x = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">7</span>, <span class="hljs-number">512</span>)<br><span class="hljs-built_in">print</span>(model(x).size())<br></code></pre></td></tr></table></figure><h3 id="Layer-Normalization-1"><a href="#Layer-Normalization-1" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>见 1.4.2。</p><h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed Forward"></a>Feed Forward</h3><blockquote><p><strong>前馈全连接层的作用:</strong><br>考虑注意力机制可能对复杂过程的拟合程度不够，通过增加两层网络来增强模型的能力。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositionwiseFeedForward</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, d_ff, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-built_in">super</span>(PositionwiseFeedForward, self).__init__()<br>        <br>        self.w1 = nn.Linear(d_model, d_ff)<br>        self.w2 = nn.Linear(d_ff, d_model)<br>        self.dropout = nn.Dropout(p=dropout)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.w1(x)<br>        x = F.relu(x)<br>        x = self.dropout(x)<br>        x = self.w2(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><blockquote><p><code>Dncoder</code> 由 <code>N=6</code> 个相同的 <code>layers</code> 组成, 每个 <code>layers</code> 的结构：<br>（1）<strong>带mask的多头注意力层</strong><br>（2）<strong>多头注意力层</strong>（Multi-head attention layer）；<br>（3）<strong>前馈层</strong>（Feed Forward Layer）；<br>每个 sub-layer 都加了残差连接和归一化。</p></blockquote><h3 id="Masked-Multi-head-Attention"><a href="#Masked-Multi-head-Attention" class="headerlink" title="Masked Multi-head Attention"></a><strong>Masked</strong> Multi-head Attention</h3><p><a href="https://blog.csdn.net/zhaohongfei_358/article/details/125858248">MultiHead-Attention和Masked-Attention的机制和原理</a></p><hr><h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><h2 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h2><p>指 <strong>分类任务中不同类别的训练样例数⽬差别很⼤的情况。</strong></p><blockquote><p><strong>解决方案：</strong><br><strong>（1）扩大数据集；</strong><br><strong>（2）采样策略：</strong>对大类数据欠采样或对小类数据过采样；<br><strong>（3）训练策略：</strong>代价加权，新的评价指标或算法；<br><strong>（4）问题转化：</strong>子问题划分或将问题转换成小类样本检测问题（如异常点检测、变化趋势检测等）。</p></blockquote><hr><h1 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h1><p>各种优化器的计算公式（GD，SGD，batch GD，SGD+momentum，NAG，AdaGrad，RMSProp，Adam）<br><strong>待补充……</strong></p><hr><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="Mean-Square-Error（均方误差，MSE）"><a href="#Mean-Square-Error（均方误差，MSE）" class="headerlink" title="Mean Square Error（均方误差，MSE）"></a>Mean Square Error（均方误差，MSE）</h2><h2 id="mathcal-L-MSE-x3D-frac-1-n-sum-i-x3D-1-n-p-i-y-i-2-Root-Mean-Square-Error（均方根误差，RMSE）-mathcal-L-RMSE-x3D-sqrt-frac-1-n-sum-i-x3D-1-n-p-i-y-i-2-Mean-Absolute-Error（平均绝对误差，MAE）-mathcal-L-MAE-x3D-frac-1-n-sum-i-x3D-1-n-p-i-y-i-Cross-Entropy（交叉熵，CE）-mathcal-L-CE-x3D-frac-1-n-sum-i-x3D-1-n-y-i-log-p-i-Weighted-Cross-Entropy（带权交叉熵，WCE）-mathcal-L-WCE-x3D-frac-1-n-sum-i-x3D-1-n-w-i-y-i-log-p-i-Binary-Cross-Entropy（二类交叉熵，BCE）-mathcal-L-BCE-x3D-frac-1-n-sum-i-x3D-1-n-y-i-log-p-i-1-y-i-log-1-p-i-Dice-Loss-IOU-Loss-Focal-Loss"><a href="#mathcal-L-MSE-x3D-frac-1-n-sum-i-x3D-1-n-p-i-y-i-2-Root-Mean-Square-Error（均方根误差，RMSE）-mathcal-L-RMSE-x3D-sqrt-frac-1-n-sum-i-x3D-1-n-p-i-y-i-2-Mean-Absolute-Error（平均绝对误差，MAE）-mathcal-L-MAE-x3D-frac-1-n-sum-i-x3D-1-n-p-i-y-i-Cross-Entropy（交叉熵，CE）-mathcal-L-CE-x3D-frac-1-n-sum-i-x3D-1-n-y-i-log-p-i-Weighted-Cross-Entropy（带权交叉熵，WCE）-mathcal-L-WCE-x3D-frac-1-n-sum-i-x3D-1-n-w-i-y-i-log-p-i-Binary-Cross-Entropy（二类交叉熵，BCE）-mathcal-L-BCE-x3D-frac-1-n-sum-i-x3D-1-n-y-i-log-p-i-1-y-i-log-1-p-i-Dice-Loss-IOU-Loss-Focal-Loss" class="headerlink" title="$$\mathcal{L}_{MSE}&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}(p_{i}-y_{i})^{2}$$## Root Mean Square Error（均方根误差，RMSE）$$\mathcal{L}_{RMSE}&#x3D;\sqrt{\frac{1}{n}\sum_{i&#x3D;1}^{n}(p_{i}-y_{i})^{2}}$$## Mean Absolute Error（平均绝对误差，MAE）$$\mathcal{L}_{MAE}&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}|p_{i}-y_{i}|$$## Cross Entropy（交叉熵，CE）$$\mathcal{L}_{CE}&#x3D;-\frac{1}{n}\sum_{i&#x3D;1}^{n}y_{i}\log(p_{i})$$Weighted Cross Entropy（带权交叉熵，WCE）$$\mathcal{L}_{WCE}&#x3D;-\frac{1}{n}\sum_{i&#x3D;1}^{n}w_{i}y_{i}\log(p_{i})$$Binary Cross Entropy（二类交叉熵，BCE）$$\mathcal{L}_{BCE}&#x3D;-\frac{1}{n}\sum_{i&#x3D;1}^{n}(y_{i}\log(p_{i})+(1-y_{i})\log(1-p_{i}))$$## Dice Loss## IOU Loss## Focal Loss"></a>$$\mathcal{L}_{MSE}&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}(p_{i}-y_{i})^{2}$$<br>## Root Mean Square Error（均方根误差，RMSE）<br>$$\mathcal{L}_{RMSE}&#x3D;\sqrt{\frac{1}{n}\sum_{i&#x3D;1}^{n}(p_{i}-y_{i})^{2}}$$<br>## Mean Absolute Error（平均绝对误差，MAE）<br>$$\mathcal{L}_{MAE}&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}|p_{i}-y_{i}|$$<br>## Cross Entropy（交叉熵，CE）<br>$$\mathcal{L}_{CE}&#x3D;-\frac{1}{n}\sum_{i&#x3D;1}^{n}y_{i}\log(p_{i})$$<br><strong>Weighted Cross Entropy（带权交叉熵，WCE）</strong><br>$$\mathcal{L}_{WCE}&#x3D;-\frac{1}{n}\sum_{i&#x3D;1}^{n}w_{i}y_{i}\log(p_{i})$$<br><strong>Binary Cross Entropy（二类交叉熵，BCE）</strong><br>$$\mathcal{L}_{BCE}&#x3D;-\frac{1}{n}\sum_{i&#x3D;1}^{n}(y_{i}\log(p_{i})+(1-y_{i})\log(1-p_{i}))$$<br>## Dice Loss<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class SoftDiceLoss(nn.Module):<br>    def __init__(self, weight=None, size_average=True):<br>        super(SoftDiceLoss, self).__init__()<br> <br>    def forward(self, logits, targets):<br>        num = targets.size(0)<br>        // 为了防止除0的发生<br>        smooth = 1<br>        <br>        probs = F.sigmoid(logits)<br>        m1 = probs.view(num, -1)<br>        m2 = targets.view(num, -1)<br>        intersection = (m1 * m2)<br> <br>        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)<br>        score = 1 - score.sum() / num<br>        return score<br></code></pre></td></tr></table></figure><br>## IOU Loss<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">#PyTorch<br>class IoULoss(nn.Module):<br>    def __init__(self, weight=None, size_average=True):<br>        super(IoULoss, self).__init__()<br><br>    def forward(self, inputs, targets, smooth=1):<br>        inputs = F.sigmoid(inputs)       <br>        <br>        inputs = inputs.view(-1)<br>        targets = targets.view(-1)<br>        <br>        intersection = (inputs * targets).sum()<br>        total = (inputs + targets).sum()<br>        union = total - intersection <br>        <br>        IoU = (intersection + smooth)/(union + smooth)<br>                <br>        return 1 - IoU<br></code></pre></td></tr></table></figure><br>## Focal Loss<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">#PyTorch<br>ALPHA = 0.8<br>GAMMA = 2<br><br>class FocalLoss(nn.Module):<br>    def __init__(self, weight=None, size_average=True):<br>        super(FocalLoss, self).__init__()<br><br>    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):<br>        inputs = F.sigmoid(inputs)       <br>        <br>        inputs = inputs.view(-1)<br>        targets = targets.view(-1)<br>        <br>        BCE = F.binary_cross_entropy(inputs, targets, reduction=&#x27;mean&#x27;)<br>        BCE_EXP = torch.exp(-BCE)<br>        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE<br>                       <br>        return focal_loss<br></code></pre></td></tr></table></figure></h2><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><p><a href="https://www.showmeai.tech/article-detail/271">深度学习与计算机视觉教程(12) | 目标检测 (两阶段, R-CNN系列)</a><br><a href="https://www.showmeai.tech/article-detail/272">深度学习与计算机视觉教程(13) | 目标检测 (SSD, YOLO系列)</a></p><h3 id="传统检测方法"><a href="#传统检测方法" class="headerlink" title="传统检测方法"></a>传统检测方法</h3><blockquote><p><strong>三个步骤：</strong><br>（1）滑动窗口选择候选区域；<br>（2）提取候选区域的视觉特征；<br>（3）分类器识别；<br><strong>可以视为 穷举法</strong></p></blockquote><h3 id="深度学习方法"><a href="#深度学习方法" class="headerlink" title="深度学习方法"></a>深度学习方法</h3><blockquote><p><strong>两阶段和一阶段的区别：</strong><br>（1）两阶段：生成一系列<strong>候选框</strong>，再通过 <code>CNN</code> 分类；（R-CNN、Fast R-CNN、Faster R-CNN 等）<br>（2）一阶段：直接将<strong>目标框的定位问题转化为回归问题</strong>；（YOLO、SSD 等）<br><strong>注：两阶段方法精度更高，一阶段方法速度更快</strong></p></blockquote><h3 id="R-CNN-系列"><a href="#R-CNN-系列" class="headerlink" title="R-CNN 系列"></a>R-CNN 系列</h3><h4 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h4><p><strong><code>R-CNN</code>（<code>Region with CNN features</code>） 尝试将爆火的 <code>CNN</code>（<code>AlexNet</code>）应用到目标检测的特征提取过程中</strong></p><blockquote><p><strong>Insights:</strong><br>（1）用 CNN 做特征提取；<br>（2）迁移学习的策略应对检测数据量少的问题，也就是预训练</p></blockquote><blockquote><p><strong>检测方案：</strong><br>（1）直接回归锚框坐标；<br>（2）滑动窗口；<br>（3）<strong>候选框策略。（<code>RCNN</code>）</strong></p></blockquote><h4 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h4><h4 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h4><h3 id="YOLO-系列"><a href="#YOLO-系列" class="headerlink" title="YOLO 系列"></a>YOLO 系列</h3><h4 id="YOLO-V1（Paper）"><a href="#YOLO-V1（Paper）" class="headerlink" title="YOLO V1（Paper）"></a>YOLO V1（<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf">Paper</a>）</h4><h4 id="YOLO-V2（Paper）"><a href="#YOLO-V2（Paper）" class="headerlink" title="YOLO V2（Paper）"></a>YOLO V2（<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf">Paper</a>）</h4><h4 id="YOLO-V3（Paper）"><a href="#YOLO-V3（Paper）" class="headerlink" title="YOLO V3（Paper）"></a>YOLO V3（<a href="https://arxiv.org/pdf/1804.02767.pdf">Paper</a>）</h4><h4 id="YOLO-V4（Paper）"><a href="#YOLO-V4（Paper）" class="headerlink" title="YOLO V4（Paper）"></a>YOLO V4（<a href="https://arxiv.org/pdf/2004.10934">Paper</a>）</h4><h4 id="YOLO-V5"><a href="#YOLO-V5" class="headerlink" title="YOLO V5"></a>YOLO V5</h4><p><strong>待补充……</strong></p><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><ul><li>聚类</li><li>主成分分析（PCA）</li></ul><h2 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h2><p><strong>待补充……</strong></p><h2 id="自监督学习"><a href="#自监督学习" class="headerlink" title="自监督学习"></a>自监督学习</h2><blockquote><p>自监督中的崩溃解问题的理解</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>For Offersssss! （Linux与Python篇）</title>
      <link href="/2022/09/06/029-InterviewQs_Python/"/>
      <url>/2022/09/06/029-InterviewQs_Python/</url>
      
        <content type="html"><![CDATA[<h2 id="ACM-模式下的-python"><a href="#ACM-模式下的-python" class="headerlink" title="ACM 模式下的 python"></a>ACM 模式下的 python</h2><h3 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h3><p>万能输入语句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">int</span>(x), <span class="hljs-built_in">input</span>().split())<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">Function</span>(<span class="hljs-params">self, ...</span>):<br>    <span class="hljs-built_in">print</span>(...)<br>    <span class="hljs-keyword">return</span><br><br><span class="hljs-keyword">if</span> __name__==<span class="hljs-string">&quot;__main__&quot;</span>:<br><span class="hljs-comment"># 单行输入两个数</span><br>    n, m = <span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">int</span>(x), <span class="hljs-built_in">input</span>().split())<br>    <br>    <span class="hljs-comment"># m行输入，保存为列表即可</span><br>    lst = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        lst.append(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">int</span>(x), <span class="hljs-built_in">input</span>().split())))<br><br>    <span class="hljs-comment"># 输出</span><br>    Solution().Function(...)<br></code></pre></td></tr></table></figure><blockquote></blockquote><p>以下为举例：【模板】 拓扑排序</p><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>给定一个包含 $n$ 个点 $m$ 条边的有向无环图，求出该图的拓扑序。若图的拓扑序不唯一，输出任意合法的拓扑序即可。若该图不能拓扑排序，输出 $−1$。<br><strong>输入描述：</strong><br>第一行输入两个整数 $n,m(1 \leq n,m \leq 2 \times 10^{5})$，表示点的个数和边的条数。<br>接下来的 $m$ 行，每行输入两个整数 $u_{i},v{i}(1 \leq u,v \leq n)$，表示 $u_{i}$ 到 $v_{i}$ 之间有一条有向边。<br><strong>输出描述：</strong><br>若图存在拓扑序，输出一行nn个整数，表示拓扑序。否则输出-1−1。</p><blockquote><p>示例1<br><strong>输入：</strong><br>5 4<br>1 2<br>2 3<br>3 4<br>4 5<br><strong>输出：</strong><br>1 2 3 4 5</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">TopologicalSort</span>(<span class="hljs-params">self, n, m, edges</span>):<br>        indegrees = [<span class="hljs-number">0</span>]*(n+<span class="hljs-number">1</span>)<br>        visited = [<span class="hljs-number">0</span>]*(n+<span class="hljs-number">1</span>)<br>        dst = [[] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n+<span class="hljs-number">1</span>)]<br>        path = []<br>        queue = []<br>        <span class="hljs-keyword">for</span> edge <span class="hljs-keyword">in</span> edges:<br>            indegrees[edge[<span class="hljs-number">1</span>]] += <span class="hljs-number">1</span><br>            dst[edge[<span class="hljs-number">0</span>]].append(edge[<span class="hljs-number">1</span>])<br>        <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n+<span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">if</span> indegrees[node] == <span class="hljs-number">0</span>:<br>                path.append(node)<br>                visited[node] = <span class="hljs-number">1</span><br>                queue.append(node)<br>        <br>        <span class="hljs-keyword">while</span> queue:<br>            node = queue.pop(<span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> dst[node]:<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> visited[d]:<br>                    queue.append(d)<br>                indegrees[d] -= <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> indegrees[d]:<br>                    path.append(d)<br>                    visited[d] = <span class="hljs-number">1</span><br>        <span class="hljs-comment"># 输出</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            <span class="hljs-built_in">print</span>(path[i], end=<span class="hljs-string">&#x27; &#x27;</span>)<br>        <span class="hljs-keyword">return</span><br><br><br><span class="hljs-keyword">if</span> __name__==<span class="hljs-string">&quot;__main__&quot;</span>:<br><span class="hljs-comment"># 单行输入</span><br>    n, m = <span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">int</span>(x), <span class="hljs-built_in">input</span>().split())<br>    <br>    <span class="hljs-comment"># 多行输入</span><br>    edges = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        edges.append(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">int</span>(x), <span class="hljs-built_in">input</span>().split())))<br><br>    <span class="hljs-comment"># 输出</span><br>    Solution().TopologicalSort(n, m, edges)<br></code></pre></td></tr></table></figure><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><h3 id="Python基础"><a href="#Python基础" class="headerlink" title="Python基础"></a>Python基础</h3><p>python多线程，多进程<br>python的GIL<br>生成器与迭代器<br>装饰器<br>深浅拷贝<br>do while与while do循环语句的区别<br>多元赋值机制</p><h3 id="Python3"><a href="#Python3" class="headerlink" title="Python3"></a>Python3</h3><p>nonlocal、global区别</p><h2 id="linux常用命令"><a href="#linux常用命令" class="headerlink" title="linux常用命令"></a>linux常用命令</h2><p>查找文件命令（whereis，which，find，grep，locate）<br>查看进程<br>杀死进程<br>ssh，scp<br>nohup &amp;<br>文件权限</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>For Offersssss! （图像处理篇）</title>
      <link href="/2022/09/01/028-InterviewQs_IP/"/>
      <url>/2022/09/01/028-InterviewQs_IP/</url>
      
        <content type="html"><![CDATA[<h2 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h2><h3 id="图像处理基础"><a href="#图像处理基础" class="headerlink" title="图像处理基础"></a>图像处理基础</h3><ul><li><p><strong>图像预处理方法及原理</strong><br>低通高通滤波器<br>高斯滤波器<br>均值、中值滤波</p></li><li><p><strong>图像的增强和降噪</strong><br>几何变换<br>颜色变换<br>区域置零</p></li><li><p><strong>常用的图像特征提取算法</strong><br>方向梯度直方图（HOG）<br>尺度不变特征变换（SIFT）<br>加速稳健特征（SURF）<br>高斯函数差分（DOG）<br>局部二值模式（LBP）<br>哈尔变换（HAAR）</p></li><li><p><strong>常用的图像插值算法</strong></p></li><li><p><strong>直方图</strong><br>直方图的概念<br>直方图均衡化</p></li></ul><h3 id="图像变换"><a href="#图像变换" class="headerlink" title="图像变换"></a>图像变换</h3><h3 id="图像编码和压缩"><a href="#图像编码和压缩" class="headerlink" title="图像编码和压缩"></a>图像编码和压缩</h3><h3 id="图像增强和复原"><a href="#图像增强和复原" class="headerlink" title="图像增强和复原"></a>图像增强和复原</h3><h3 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h3><h3 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h3><ul><li><p><strong>传统的边缘检测算子</strong><br>Sobel<br>Canny<br>Prewitt<br>Laplace<br>Robert<br>各算法优缺点</p></li><li><p><strong>极大值抑制</strong></p></li></ul><h3 id="形态学处理"><a href="#形态学处理" class="headerlink" title="形态学处理"></a>形态学处理</h3><ul><li><strong>腐蚀</strong></li><li><strong>膨胀</strong></li><li><strong>开运算</strong></li><li><strong>闭运算</strong></li></ul><hr><h2 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h2><p>注：以下转自<a href="https://blog.csdn.net/qq_45445740/article/details/120354641">面试问题总结——关于OpenCV</a><br><a href="https://aistudio.baidu.com/aistudio/projectdetail/2360571">【地表最全】零基础！计算机视觉OpenCV从入门到入土</a></p><h3 id="OpenCV-中-cv-Mat-的深拷贝和浅拷贝问题"><a href="#OpenCV-中-cv-Mat-的深拷贝和浅拷贝问题" class="headerlink" title="OpenCV 中 cv::Mat 的深拷贝和浅拷贝问题"></a>OpenCV 中 cv::Mat 的深拷贝和浅拷贝问题</h3><p><code>深拷贝</code>：分配新内存的同时拷贝数据，当被赋值的容器被修改时，原始容器数据不会改变。<br><code>浅拷贝</code>：仅拷贝数据，当被赋值容器修改时，原始容器数据也会做同样改变。</p><p>OpenCV 的 Mat 时，有几种赋值方式分别为：<br>  深拷贝 是 <code>b = a.clone();</code> 和 <code>a.copyTo(b);</code><br>  浅拷贝 是 <code>b = a;</code> 和 <code>b(a);</code></p><p>  关于这个问题，我还测试了 OpenCV 中的 ROI，就是图像截图，发现ROI也是浅拷贝。</p><p>  C++ 中利用opencv存储图像的结构体是 Mat。</p><h3 id="OpenCV-中-RGB2GRAY-是怎么实现的"><a href="#OpenCV-中-RGB2GRAY-是怎么实现的" class="headerlink" title="OpenCV 中 RGB2GRAY 是怎么实现的"></a>OpenCV 中 RGB2GRAY 是怎么实现的</h3><p>  本质上就是寻找一个三维空间到一维空间的映射，以 <code>R、G、B</code> 为轴建立空间直角坐标系，则图片上的每一个像素点都可以用一个点表示，则可以通过一个公式 $Gray &#x3D; 0.29900×R + 0.58700×G + 0.11400×B$，来完成一维空间的映射。</p><h3 id="连续图像转化为数字图像需要进行哪些操作？"><a href="#连续图像转化为数字图像需要进行哪些操作？" class="headerlink" title="连续图像转化为数字图像需要进行哪些操作？"></a>连续图像转化为数字图像需要进行哪些操作？</h3><p>  取样 量化</p><h3 id="数字图像中有哪些基本特征？"><a href="#数字图像中有哪些基本特征？" class="headerlink" title="数字图像中有哪些基本特征？"></a>数字图像中有哪些基本特征？</h3><p>  颜色特征、纹理特征、形状特征、空间关系特征等。</p><h3 id="图像边缘检测中常用的边缘检测算子有哪些？"><a href="#图像边缘检测中常用的边缘检测算子有哪些？" class="headerlink" title="图像边缘检测中常用的边缘检测算子有哪些？"></a>图像边缘检测中常用的边缘检测算子有哪些？</h3><p>  <code>Roberts</code> 算子、<code>Prewitt</code> 算子、<code>Sobel</code> 算子、<code>Canny</code> 算子、<code>Laplacian</code> 算子等。</p><h3 id="对霍夫变换的理解"><a href="#对霍夫变换的理解" class="headerlink" title="对霍夫变换的理解"></a>对霍夫变换的理解</h3><p>  霍夫变换常用来提取图像中的直线和圆等几何形状。它通过一种投票算法检测具有特定形状的物体，就是<strong>通过计算累计结果的局部最大值得到一个符合该几何形状的集合</strong>作为结果。</p><p>  算法原理：针对每个像素点，使得 $\theta$ 在 <code>-90</code> 度到 <code>180</code> 度之间，用极坐标 $p &#x3D; xcos\theta + ysin\theta$ 计算得到共 <code>270</code> 组 $(p,\theta)$ 代表着霍夫空间的 <code>270</code> 条直线。将这 <code>270</code> 组值存储到 $H$ 中。如果一组点共线，则这组点中的每个值，都会使得 $H(p,\theta)$ 加 <code>1</code>，因此找到最大的 $H$ 值，就是共线的点最多的直线，也可以通过设定阈值来判定。</p><h3 id="对-HOG-特征的理解"><a href="#对-HOG-特征的理解" class="headerlink" title="对 HOG 特征的理解"></a>对 HOG 特征的理解</h3><p>  <strong>主要思想</strong>：在边缘具体位置未知的情况下，边缘方向的分布也可以很好的表示行人目标的外形轮廓，即梯度的统计信息，而梯度主要位于边缘的地方很好地描述。<br>  <strong><code>HOG</code> 特征检测算法的几个步骤</strong>：颜色空间归一化 —&gt; 梯度计算 —&gt; 梯度方向直方图 —&gt; 重叠块直方图归一化 —&gt; <code>HOG</code> 特征。</p><h3 id="图像的插值方法有哪些？"><a href="#图像的插值方法有哪些？" class="headerlink" title="图像的插值方法有哪些？"></a>图像的插值方法有哪些？</h3><p>  最近邻法<br>  双线性内插法<br>  三次内插法</p><h3 id="Grabcut-的基本原理和应用"><a href="#Grabcut-的基本原理和应用" class="headerlink" title="Grabcut 的基本原理和应用"></a>Grabcut 的基本原理和应用</h3><p>  <code>Graph Cut</code> 和 <code>Grab Cut</code> 算法，两者都是基于图论的分割方法。<br>  <code>Graph Cut</code> 在计算机视觉领域普遍应用于前背景分割、抠图等。<br>  基本原理：根据待分割的图像，确定图的节点与边，即图的形状已确定下来,是给图中所有边赋值相应的权值，然后找到权值和最小的边的组合，就完成了图像分割。</p><h3 id="SIFT-x2F-SURF的特征提取方法，是如何保持尺度不变性的？"><a href="#SIFT-x2F-SURF的特征提取方法，是如何保持尺度不变性的？" class="headerlink" title="SIFT &#x2F; SURF的特征提取方法，是如何保持尺度不变性的？"></a>SIFT &#x2F; SURF的特征提取方法，是如何保持尺度不变性的？</h3><p>  <code>SIFT (Scale-Invariant Feature Transform)</code> 算法的实质是在不同的尺度空间上查找关键点(特征点)，并计算出关键点的方向。<code>SIFT</code> 所查找到的关键点是一些十分突出，不会因光照，仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点等。</p><p>  <strong>如何保持尺度不变性？</strong><br>  ① <strong>尺度空间极值检测</strong>：搜索所有尺度上的图像位置。通过高斯差分函数来识别潜在的对于尺度和旋转不变的关键点。（第一步获取图片的高斯金字塔，高斯金字塔指的是图片在不同尺寸下进行的高斯模糊处理后的图片，为了寻找图片的特征点，我们要找到图片在不同尺寸空间里的极值，这些极值通常存在于图片的边缘或者灰度突变的地方，所以要对高斯模糊后的图片进行高斯差分，然后到寻找极值点。）<br>  ② <strong>关键点定位</strong>：由于图片的坐标是离散，所以要精确定位的话，作者采用了曲面拟合的方法，通过插值找到真正极值点的位置，位置找到之后，我们要找到这个极值点的主方向。<br>  ③ <strong>关键点方向确定</strong>：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向。所有后面的对图像数据的操作都相对于关键点的方向、尺度和位置进行变换，从而保证了对于这些变换的不变性。<br>  ④ <strong>关键点描述</strong>：在每个关键点周围的邻域内，在选定的尺度上测量图像局部的梯度。这些梯度作为关键点的描述符，它允许比较大的局部形状的变形或光照变化。</p><p>  <code>SURF</code> 是在 <code>SIFT</code> 的基础上改进的，原理基本相似，在特征点检测这儿，<code>SIFT</code> 是先进行非极大值抑制，再去除低对比度的点，最后再通过 <code>Hessian</code> 矩阵去除边缘的点；而 <code>SURF</code> 是先用 <code>Hessian</code> 矩阵确定候选点，然后再进行非极大值抑制，该算法提高了 <code>SIFT</code> 的速度和鲁棒性，且理论上 <code>SURF</code> 是 <code>SIFT</code> 速度的 <code>3</code> 倍。（见书《OpenCV3编程入门》P417）</p><h3 id="关于-FLANN-算法"><a href="#关于-FLANN-算法" class="headerlink" title="关于 FLANN 算法"></a>关于 FLANN 算法</h3><p>  <code>FLANN</code> 算法也属于关键点匹配算法，算法步骤整体来说分为两步：一是建立索引，而是搜索。<br>  建立索引：其实就是要两部分参数，一是数据也就是 <code>mat</code> 矩阵，二是一些具体参数，这个参数要根据建立的索引类型来设置。而有哪些索引类型呢？共有：线性索引、<code>KD-Tree</code> 索引、K均值索引、复合索引、<code>LSH</code> 方法索引、自动索引六种。<br>  进行搜索：有两种搜索方式 ：搜索 k 邻近 （具体点的个数由用户设定，设 <code>n</code> 个就一定返回 <code>n</code> 个）、搜索半径最近 （即可能不存在符合条件的点，则返回空的）。<br>  <code>SIFT / SURF</code> 是基于浮点数的匹配，<code>ORB</code> 是二值匹配，速度更快。对于 <code>FLANN</code> 匹配算法，当使用 <code>ORB</code> 匹配算法的时候，需要重新构造 <code>HASH</code>。</p><h3 id="Canny边缘检测的流程"><a href="#Canny边缘检测的流程" class="headerlink" title="Canny边缘检测的流程"></a>Canny边缘检测的流程</h3><p>  图像降噪——计算图像梯度，得到可能边缘——非极大值抑制——双阈值筛选</p><h3 id="图像锐化-–-sharpen"><a href="#图像锐化-–-sharpen" class="headerlink" title="图像锐化 – sharpen"></a>图像锐化 – sharpen</h3><p>  平滑：把图像变模糊；<br>  锐化：把图像变清晰；<br>  图像锐化主要用于增强图像的灰度跳变部分，这和图像平滑对灰度跳变的抑制正好相反。</p><h3 id="图像对比度"><a href="#图像对比度" class="headerlink" title="图像对比度"></a>图像对比度</h3><p>  对比度：指一幅图像中灰度反差的大小；<br>  对比度 &#x3D; 最大亮度 &#x2F; 最小亮度</p><h3 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h3><p>  <strong>定义</strong>：把滤波想象成一个包含加权系数的窗口，当使用这个滤波器平滑处理图像时，就把这个窗口放到图像上，透过该窗口查看。<br>  <strong>滤波的作用</strong>：去除图像中的噪声<br>  <strong>低通滤波器</strong>：去除图像中的高频部分——模糊化<br>  <strong>高通滤波器</strong>：去除图像中的低频部分——锐化<br>  <strong>线性滤波器</strong>：方框滤波、均值滤波、高斯滤波<br>  <strong>非线性滤波</strong>：中值滤波、双边滤波</p><h3 id="OpenCV-中图像加法（cv2-add-）与图像融合（cv2-addWeighted-）有何区别？"><a href="#OpenCV-中图像加法（cv2-add-）与图像融合（cv2-addWeighted-）有何区别？" class="headerlink" title="OpenCV 中图像加法（cv2.add()）与图像融合（cv2.addWeighted()）有何区别？"></a>OpenCV 中图像加法（cv2.add()）与图像融合（cv2.addWeighted()）有何区别？</h3><p>  <strong>图像加法</strong>：目标图像 &#x3D; 图像1 + 图像2<br>  <strong>图像融合</strong>：目标图像 &#x3D; 图像1 × 系数1 + 图像2 × 系数2 + 亮度调节量</p><h3 id="如何检测图片中的汽车，并识别车型，如果有遮挡怎么办？"><a href="#如何检测图片中的汽车，并识别车型，如果有遮挡怎么办？" class="headerlink" title="如何检测图片中的汽车，并识别车型，如果有遮挡怎么办？"></a>如何检测图片中的汽车，并识别车型，如果有遮挡怎么办？</h3><p>  首先这是一个细粒度的分类（Fine-Grained Classification）问题，和普通的分类不一样，要分类的类别往往只是有细微的差异。<br>  思路：<br>  ① 人工框定局部图像，然后识别。比如我去框定，汽车的车灯，汽车的前脸，汽车轮毂等，然后用 <code>cnn</code> 或 <code>deep cnn</code> 其他的分类器做这些的分类，对于分类器来说输入是汽车的车灯+汽车的前脸+汽车轮毂，而不是整张图片，分类器再从车灯前脸等提取高级特征，从而得到一个分类模型。<br>  ② 不做局部变换，做图片整体识别。但是做分级或是分层。首先，训练第一个分类器，它只针对汽车和非汽车进行分类，标记了 <code>car</code> 和 <code>other</code>。这一步要求尽可能的广，涵盖生活中常见的图片，力求 98% 以上的准确率，每个分类用了2w张图片，实际上能达到 99.5% 的准确率。接着，对于汽车做品牌分类器，只对汽车所属的品牌进行分类，不对细分的子品牌分类。这一步每一类人工标记5000张图片，输入是第一步的输出，准确率能达到96%以上。<br>最后，对每个品牌的汽车进行车型分类，这一层识别率在 94% 左右。这样会得到一个 0.98×0.96×0.94~0.88 的识别率。</p><h3 id="常用图像增强算法"><a href="#常用图像增强算法" class="headerlink" title="常用图像增强算法"></a>常用图像增强算法</h3><p>  ① <strong>直方图均衡化</strong>：直方图均衡化是图像处理领域中利用图像直方图对对比度进行调整的方法。<br>这种方法通常用来增加许多图像的局部对比度，这种方法对于背景和前景都太亮或者太暗的图像非常有用，这种方法尤其是可以带来X光图像中更好的骨骼结构显示以及曝光过度或者曝光不足照片中更好的细节。</p><p>  ② <strong>对数图像增强算法</strong>：对数图像增强是图像增强的一种常见方法，其公式为： $S &#x3D; c \times log(r+1)$，其中 $c$ 是常数（以下算法 $c&#x3D;255&#x2F;(log(256)$），这样可以实现整个画面的亮度增大。</p><p>  ③ <strong>指数图像增强算法</strong>：指数图像增强的表达为：$S &#x3D; cR^{r}$，通过合理的选择c和r可以压缩灰度范围，算法以 $c&#x3D;1.0&#x2F;255.0, r&#x3D;2$ 实现。</p><p>  ④ <strong>加Masaic算法（马赛克）</strong>：原理：用中心像素来表示邻域像素</p><p>  ⑤ <strong>曝光过度问题处理</strong>：对于曝光过度问题，可以通过计算当前图像的反相（$255-image$)，然后取当前图像和反相图像的较小者为当前像素位置的值。</p><p>  ⑥ <strong>高反差保留</strong>：高反差保留主要是将图像中颜色、明暗反差较大两部分的交界处保留下来，比如图像中有一个人和一块石头，那么石头的轮廓线和人的轮廓线以及面部、服装等有明显线条的地方会变被保留，而其他大面积无明显明暗变化的地方则生成棕灰色。其表达形式为：$dst &#x3D; r \times(img - Blur(img))$。</p><p>  ⑦ <strong>拉普拉斯算子图像增强</strong>：使用中心为 <code>5</code> 的 <code>8</code> 邻域拉普拉斯算子与图像卷积可以达到锐化增强图像的目的。</p><p>  ⑧ <strong>Gamma校正</strong>：伽马变换主要用于图像的校正，将灰度过高或者灰度过低的图片进行修正，增强对比度。伽马变换对图像的修正作用其实就是通过增强低灰度或高灰度的细节实现的。</p><h3 id="数字图像处理中常用图像分割算法有哪些？"><a href="#数字图像处理中常用图像分割算法有哪些？" class="headerlink" title="数字图像处理中常用图像分割算法有哪些？"></a>数字图像处理中常用图像分割算法有哪些？</h3><p>  ① <strong>多数的图像分割算法</strong>：均是基于灰度值的不连续和相似的性质。</p><p>  ② <strong>图像边缘分割</strong>：边缘是图像中灰度突变像素的集合，一般用微分进行检测。基本的边缘检测算法有：<code>Roberts</code> 算子、<code>Sobel</code> 算子。稍高级的算法有：<code>Canny</code> 边缘检测器。</p><p>  ③ <strong>图像阈值分割</strong>：由于阈值处理直观、实现简单且计算速度快，因此阈值处理在分割应用中处于核心地位。如 <code>Otsu</code>（最大类间方差算法）算法。</p><p>  ④ <strong>基于区域的分割</strong>：区域生长算法和区域分裂与聚合都是属于基于区域的分割算法。</p><hr>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>（持续更新中...）Leetcode 周赛记录</title>
      <link href="/2022/08/14/032-Leetcode/"/>
      <url>/2022/08/14/032-Leetcode/</url>
      
        <content type="html"><![CDATA[<ul><li>慢慢刷题慢慢补充~</li><li>写在最前面的小笔记（持续补充）：<br>（1）<code>ASCII</code> 码表：<a href="https://www.habaijian.com/">ASCII码中文站</a><br>（2）字符串数组用 <code>strings.sort()</code> 排序，<code>sorted</code> 不适用<br>（3）<code>Python</code> 字符串 <code>string</code> 类型不支持更改<br>（4）初始化一个字典用 <code>collections.defaultdict</code><br>（5）有序列表可以用 <code>sortedcontainers.SortedList</code></li></ul><hr><h1 id="2022-08-14-第-306-场周赛"><a href="#2022-08-14-第-306-场周赛" class="headerlink" title="2022-08-14 第 306 场周赛"></a>2022-08-14 第 306 场周赛</h1><h2 id="T1-6148-矩阵中的局部最大值"><a href="#T1-6148-矩阵中的局部最大值" class="headerlink" title="T1 6148. 矩阵中的局部最大值"></a>T1 <a href="https://leetcode.cn/contest/weekly-contest-306/problems/largest-local-values-in-a-matrix/">6148. 矩阵中的局部最大值</a></h2><blockquote><p><strong>模拟最大池化，Deep Learning 人狂喜</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">largestLocal</span>(<span class="hljs-params">self, grid: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        n = <span class="hljs-built_in">len</span>(grid)<br>        self.grid = grid<br>        maxLocal = [[<span class="hljs-number">0</span>]*(n-<span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n-<span class="hljs-number">2</span>)]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n-<span class="hljs-number">2</span>):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n-<span class="hljs-number">2</span>):<br>                maxLocal[i][j] = self.findMax(i, i+<span class="hljs-number">2</span>, j, j+<span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">return</span> maxLocal<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">findMax</span>(<span class="hljs-params">self, x1, x2, y1, y2</span>):<br>        ret = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(x1, x2+<span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(y1, y2+<span class="hljs-number">1</span>):<br>                <span class="hljs-keyword">if</span> self.grid[i][j] &gt; ret:<br>                    ret = self.grid[i][j]<br>        <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><h2 id="T2-6149-边积分最高的节点"><a href="#T2-6149-边积分最高的节点" class="headerlink" title="T2 6149. 边积分最高的节点"></a>T2 <a href="https://leetcode.cn/contest/weekly-contest-306/problems/node-with-highest-edge-score/">6149. 边积分最高的节点</a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">edgeScore</span>(<span class="hljs-params">self, edges: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        n = <span class="hljs-built_in">len</span>(edges)<br>        ret = [<span class="hljs-number">0</span>]*n<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            ret[edges[i]] += i<br>        <span class="hljs-keyword">return</span> ret.index(<span class="hljs-built_in">max</span>(ret))<br></code></pre></td></tr></table></figure><h2 id="T3-6150-根据模式串构造最小数字"><a href="#T3-6150-根据模式串构造最小数字" class="headerlink" title="T3 6150. 根据模式串构造最小数字"></a>T3 <a href="https://leetcode.cn/contest/weekly-contest-306/problems/construct-smallest-number-from-di-string/">6150. 根据模式串构造最小数字</a></h2><p>我以为只有 T4 是原题，没想到 T3 居然也是原题 <a href="https://leetcode.cn/problems/find-permutation/">484. 寻找排列</a>，而且还欺负我是穷逼不是会员看不到原题。</p><p><img src="https://s2.loli.net/2022/08/14/bkprtLgSMixRBEA.jpg"></p><h2 id="T4-6151-统计特殊整数"><a href="#T4-6151-统计特殊整数" class="headerlink" title="T4 6151. 统计特殊整数"></a>T4 <a href="https://leetcode.cn/contest/weekly-contest-306/problems/count-special-integers/">6151. 统计特殊整数</a></h2><p>看了赛后讨论区才发现是原题 <a href="https://leetcode.cn/problems/numbers-with-repeated-digits/">1012. 至少有 1 位重复的数字</a>，没意思。我还纳闷大家 Hard 题 A 得这么快。</p><hr><h1 id="2022-08-07-第-305-场周赛"><a href="#2022-08-07-第-305-场周赛" class="headerlink" title="2022-08-07 第 305 场周赛"></a>2022-08-07 第 305 场周赛</h1><p><strong>动态规划专场</strong>了属于是，刚好我 <code>DP</code> 比较菜</p><h2 id="T1-6136-算术三元组的数目"><a href="#T1-6136-算术三元组的数目" class="headerlink" title="T1 6136. 算术三元组的数目"></a>T1 <a href="https://leetcode.cn/contest/weekly-contest-305/problems/number-of-arithmetic-triplets/">6136. 算术三元组的数目</a></h2><p>给你一个下标从 <code>0</code> 开始、<strong>严格递增</strong> 的整数数组 <code>nums</code> 和一个正整数 <code>diff</code> 。如果满足下述全部条件，则三元组 <code>(i, j, k)</code> 就是一个 <strong>算术三元组</strong> ：</p><ul><li><code>i &lt; j &lt; k</code> ，</li><li><code>nums[j] - nums[i] == diff</code> 且</li><li><code>nums[k] - nums[j] == diff</code></li></ul><p>返回不同 <strong>算术三元组</strong> 的数目。</p><blockquote><p><strong>输入</strong>：nums &#x3D; [0,1,4,6,7,10], diff &#x3D; 3<br><strong>输出</strong>：2<br><strong>解释</strong>：<br>(1, 2, 4) 是算术三元组：7 - 4 &#x3D;&#x3D; 3 且 4 - 1 &#x3D;&#x3D; 3 。<br>(2, 4, 5) 是算术三元组：10 - 7 &#x3D;&#x3D; 3 且 7 - 4 &#x3D;&#x3D; 3 。</p></blockquote><p>提示：<br>$3 \leq$ <code>nums.length</code> $\leq 200$<br>$0 \leq$ <code>nums[i]</code> $\leq 200$<br>$1 \leq$ <code>diff</code> $\leq 50$<br><code>nums</code> <strong>严格</strong> 递增</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">arithmeticTriplets</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], diff: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        self.nums = nums<br>        self.n = <span class="hljs-built_in">len</span>(self.nums)<br>        ret = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.n):<br>            j = self.helper(i+<span class="hljs-number">1</span>, self.nums[i]+diff)<br>            k = self.helper(i+<span class="hljs-number">1</span>, self.nums[i]+<span class="hljs-number">2</span>*diff)<br>            <span class="hljs-keyword">if</span> j &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> k &gt;= <span class="hljs-number">0</span>:<br>                ret += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> ret<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">helper</span>(<span class="hljs-params">self, startIndex, target</span>):<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(startIndex, self.n):<br>            <span class="hljs-keyword">if</span> self.nums[i] == target:<br>                <span class="hljs-keyword">return</span> i<br>        <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h2 id="T2-6139-受限条件下可到达节点的数目"><a href="#T2-6139-受限条件下可到达节点的数目" class="headerlink" title="T2 6139. 受限条件下可到达节点的数目"></a>T2 <a href="https://leetcode.cn/contest/weekly-contest-305/problems/reachable-nodes-with-restrictions/">6139. 受限条件下可到达节点的数目</a></h2><p>现有一棵由 <code>n</code> 个节点组成的无向树，节点编号从 <code>0</code> 到 <code>n - 1</code> ，共有 <code>n - 1</code> 条边。</p><p>给你一个二维整数数组 <code>edges</code> ，长度为 <code>n - 1</code> ，其中 <code>edges[i] = [ai, bi]</code> 表示树中节点 <code>ai</code> 和 <code>bi</code> 之间存在一条边。另给你一个整数数组 <code>restricted</code> 表示 <strong>受限</strong> 节点。</p><p>在不访问受限节点的前提下，返回你可以从节点 <code>0</code> 到达的 <strong>最多</strong> 节点数目。</p><p>注意，节点 <code>0</code> <strong>不</strong> 会标记为受限节点。</p><blockquote><p><strong>输入</strong>：n &#x3D; 7, edges &#x3D; [[0,1],[1,2],[3,1],[4,0],[0,5],[5,6]], restricted &#x3D; [4,5]<br><strong>输出</strong>：4<br><strong>解释</strong>：上图所示正是这棵树。<br>在不访问受限节点的前提下，只有节点 [0,1,2,3] 可以从节点 0 到达。<br><img src="https://s2.loli.net/2022/08/07/Je27Wnlz8X9siRr.png"></p></blockquote><p>提示：<br>$2 \leq$ <code>n</code> $\leq 10^{5}$<br><code>edges.length</code> $&#x3D;&#x3D; n - 1$<br><code>edges[i].length</code> $&#x3D;&#x3D; 2$<br>$0 \leq a_{i}, b_{i} &lt; n$<br>$a_{i} !&#x3D; b_{i}$<br><code>edges</code> 表示一棵有效的树<br>$1 \leq$ <code>restricted.length</code> $&lt; n$<br>$1 \leq$ <code>restricted[i]</code> $&lt; n$<br><code>restricted</code> 中的所有值 <strong>互不相同</strong></p><blockquote><p>从节点 <code>0</code> 开始 <code>BFS</code> 即可。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reachableNodes</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span>, edges: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]], restricted: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        restr_flag = [<span class="hljs-number">0</span>]*n<br>        paths = [[] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]<br>        visited = [<span class="hljs-number">0</span>]*n<br>        <br>        <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> restricted:<br>            restr_flag[node] = <span class="hljs-number">1</span><br>            <br>        <span class="hljs-keyword">for</span> edge <span class="hljs-keyword">in</span> edges:<br>            <span class="hljs-keyword">if</span> restr_flag[edge[<span class="hljs-number">0</span>]] <span class="hljs-keyword">or</span> restr_flag[edge[<span class="hljs-number">1</span>]]:<br>                <span class="hljs-keyword">continue</span><br>            paths[edge[<span class="hljs-number">0</span>]].append(edge[<span class="hljs-number">1</span>])<br>            paths[edge[<span class="hljs-number">1</span>]].append(edge[<span class="hljs-number">0</span>])<br>            <br>        queue = [<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">while</span> queue:<br>            node = queue.pop(<span class="hljs-number">0</span>)<br>            visited[node] = <span class="hljs-number">1</span><br>            <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> paths[node]:<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> visited[nd]:<br>                    queue.append(nd)<br>        <span class="hljs-keyword">return</span> visited.count(<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h2 id="T3-6137-检查数组是否存在有效划分"><a href="#T3-6137-检查数组是否存在有效划分" class="headerlink" title="T3 6137. 检查数组是否存在有效划分"></a>T3 <a href="https://leetcode.cn/contest/weekly-contest-305/problems/check-if-there-is-a-valid-partition-for-the-array/">6137. 检查数组是否存在有效划分</a></h2><p>给你一个下标从 <code>0</code> 开始的整数数组 <code>nums</code> ，你必须将数组划分为一个或多个 <strong>连续</strong> 子数组。</p><p>如果获得的这些子数组中每个都能满足下述条件 <strong>之一</strong> ，则可以称其为数组的一种 <strong>有效</strong> 划分：</p><ul><li>子数组 <strong>恰</strong> 由 <code>2</code> 个相等元素组成，例如，子数组 <code>[2,2]</code> 。</li><li>子数组 <strong>恰</strong> 由 <code>3</code> 个相等元素组成，例如，子数组 <code>[4,4,4]</code> 。</li><li>子数组 <strong>恰</strong> 由 <code>3</code> 个连续递增元素组成，并且相邻元素之间的差值为 <code>1</code> 。例如，子数组 <code>[3,4,5]</code> ，但是子数组 <code>[1,3,5]</code> 不符合要求。</li></ul><p>如果数组 <strong>至少</strong> 存在一种有效划分，返回 <code>true</code> ，否则，返回 <code>false</code> 。</p><blockquote><p><strong>输入</strong>：nums &#x3D; [4,4,4,5,6]<br><strong>输出</strong>：true<br><strong>解释</strong>：数组可以划分成子数组 [4,4] 和 [4,5,6] 。<br>这是一种有效划分，所以返回 true 。</p></blockquote><p>提示：<br>$2 \leq$ <code>nums.length</code> $\leq 10^{5}$<br>$1 \leq$ <code>nums[i]</code> $\leq 10^{6}$</p><blockquote><p>数据量小的情况下可以用递归，但本题用递归会 <code>T</code>……</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">validPartition</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(nums) == <span class="hljs-number">2</span> <span class="hljs-keyword">and</span> nums[<span class="hljs-number">0</span>] == nums[<span class="hljs-number">1</span>]:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">len</span>(nums) == <span class="hljs-number">3</span>:<br>            <span class="hljs-keyword">if</span> nums[<span class="hljs-number">0</span>] == nums[<span class="hljs-number">1</span>] <span class="hljs-keyword">and</span> nums[<span class="hljs-number">0</span>] == nums[<span class="hljs-number">2</span>]:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>            <span class="hljs-keyword">elif</span> nums[<span class="hljs-number">0</span>] == nums[<span class="hljs-number">1</span>]-<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> nums[<span class="hljs-number">1</span>] == nums[<span class="hljs-number">2</span>]-<span class="hljs-number">1</span>:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">len</span>(nums) &gt; <span class="hljs-number">3</span>:<br>            <span class="hljs-keyword">if</span> nums[<span class="hljs-number">0</span>] == nums[<span class="hljs-number">1</span>] <span class="hljs-keyword">and</span> nums[<span class="hljs-number">0</span>] == nums[<span class="hljs-number">2</span>]:<br>                <span class="hljs-keyword">return</span> self.validPartition(nums[<span class="hljs-number">2</span>:]) <span class="hljs-keyword">or</span> self.validPartition(nums[<span class="hljs-number">3</span>:])<br>            <span class="hljs-keyword">if</span> nums[<span class="hljs-number">0</span>] == nums[<span class="hljs-number">1</span>] <span class="hljs-keyword">and</span> nums[<span class="hljs-number">0</span>] != nums[<span class="hljs-number">2</span>]:<br>                <span class="hljs-keyword">return</span> self.validPartition(nums[<span class="hljs-number">2</span>:])<br>            <span class="hljs-keyword">elif</span> nums[<span class="hljs-number">0</span>] == nums[<span class="hljs-number">1</span>]-<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> nums[<span class="hljs-number">1</span>] == nums[<span class="hljs-number">2</span>]-<span class="hljs-number">1</span>:<br>                <span class="hljs-keyword">return</span> self.validPartition(nums[<span class="hljs-number">3</span>:])<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><blockquote><p>动态规划：遍历数组中的所有元素，判断该元素及其前的所有元素是否合法，更新当前的状态即可，当全部元素遍历完后即为最后结果。<br>（1）<code>dp</code> 数组全部初始化为 <code>False</code>：<br><code>dp[0] = True, dp[1] = False</code>；<br>若 <code>nums[0] == nums[1]</code>（<strong>条件一</strong>）, 则 <code>dp[2] = True</code>，否则为 <code>False</code>；<br>若 <code>nums[0] == nums[1] == nums[2]</code>（<strong>条件二</strong>）或 <code>nums[0]+1 == nums[1] == nums[2]-1</code>（<strong>条件三</strong>）, 则 <code>dp[3] = True</code>，否则为 <code>False</code>；<br>（2）对于 <code>nums[i]</code>，有以下三种方式处理，状态转移规则为：<br>a. 若 <code>nums[i]</code> 与 <code>nums[i-1]</code> 按<strong>条件一</strong>组成子数组，则 <code>dp[i] = dp[i-2] &amp; nums[i-1] == nums[i]</code>；<br>b. 若 <code>nums[i]</code> 与 <code>nums[i-1]</code>，<code>nums[i-2]</code> 按<strong>条件二</strong>组成子数组，则 <code>dp[i] = dp[i-3] &amp; nums[i-2] == nums[i-1] == nums[i]</code>；<br>c. 若 <code>nums[i]</code> 与 <code>nums[i-1]</code>，<code>nums[i-2]</code> 按<strong>条件三</strong>组成子数组，则 <code>dp[i] = dp[i-3] &amp; nums[i-2]+1 == nums[i-1] == nums[i]-1</code>。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">validPartition</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        <span class="hljs-keyword">if</span> n == <span class="hljs-number">2</span>:<br>            <span class="hljs-keyword">return</span> nums[<span class="hljs-number">0</span>] == nums[<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">elif</span> n == <span class="hljs-number">3</span>:<br>            <span class="hljs-keyword">return</span> nums[<span class="hljs-number">0</span>] == nums[<span class="hljs-number">1</span>] == nums[<span class="hljs-number">2</span>] <span class="hljs-keyword">or</span> nums[<span class="hljs-number">0</span>]+<span class="hljs-number">1</span> == nums[<span class="hljs-number">1</span>] == nums[<span class="hljs-number">2</span>]-<span class="hljs-number">1</span><br>        <br>        dp = [<span class="hljs-literal">False</span>]*n<br>        dp[<span class="hljs-number">1</span>] = nums[<span class="hljs-number">0</span>] == nums[<span class="hljs-number">1</span>]<br>        dp[<span class="hljs-number">2</span>] = nums[<span class="hljs-number">0</span>] == nums[<span class="hljs-number">1</span>] == nums[<span class="hljs-number">2</span>] <span class="hljs-keyword">or</span> nums[<span class="hljs-number">0</span>]+<span class="hljs-number">1</span> == nums[<span class="hljs-number">1</span>] == nums[<span class="hljs-number">2</span>]-<span class="hljs-number">1</span><br>        <br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>, n):<br>            <span class="hljs-keyword">if</span> dp[i-<span class="hljs-number">2</span>] <span class="hljs-keyword">and</span> nums[i-<span class="hljs-number">1</span>] == nums[i]:<br>                dp[i] = <span class="hljs-literal">True</span><br>            <span class="hljs-keyword">if</span> dp[i-<span class="hljs-number">3</span>] <span class="hljs-keyword">and</span> nums[i-<span class="hljs-number">2</span>] == nums[i-<span class="hljs-number">1</span>] == nums[i]:<br>                dp[i] = <span class="hljs-literal">True</span><br>            <span class="hljs-keyword">if</span> dp[i-<span class="hljs-number">3</span>] <span class="hljs-keyword">and</span> nums[i-<span class="hljs-number">2</span>]+<span class="hljs-number">1</span> == nums[i-<span class="hljs-number">1</span>] == nums[i]-<span class="hljs-number">1</span>:<br>                dp[i] = <span class="hljs-literal">True</span><br><br>        <span class="hljs-keyword">return</span> dp[n-<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><h2 id="T4-6138-最长理想子序列"><a href="#T4-6138-最长理想子序列" class="headerlink" title="T4 6138. 最长理想子序列"></a>T4 <a href="https://leetcode.cn/contest/weekly-contest-305/problems/longest-ideal-subsequence/">6138. 最长理想子序列</a></h2><p>给你一个由小写字母组成的字符串 <code>s</code> ，和一个整数 <code>k</code> 。如果满足下述条件，则可以将字符串 <code>t</code> 视作是 <strong>理想字符串</strong> ：</p><p><code>t</code> 是字符串 <code>s</code> 的一个子序列。<br><code>t</code> 中每两个 <strong>相邻</strong> 字母在字母表中位次的绝对差值小于或等于 <code>k</code> 。<br>返回 <strong>最长</strong> 理想字符串的长度。</p><p>字符串的子序列同样是一个字符串，并且子序列还满足：可以经由其他字符串删除某些字符（也可以不删除）但不改变剩余字符的顺序得到。</p><p><strong>注意</strong>：字母表顺序不会循环。例如，<code>&#39;a&#39;</code> 和 <code>&#39;z&#39;</code> 在字母表中位次的绝对差值是 <code>25</code> ，而不是 <code>1</code> 。</p><blockquote><p><strong>输入</strong>：s &#x3D; “acfgbd”, k &#x3D; 2<br><strong>输出</strong>：4<br><strong>解释</strong>：最长理想字符串是 “acbd” 。该字符串长度为 4 ，所以返回 4 。<br>注意 “acfgbd” 不是理想字符串，因为 ‘c’ 和 ‘f’ 的字母表位次差值为 3 。</p></blockquote><p>提示：<br>$1 \leq$ <code>s.length</code> $\leq 10^{5}$<br>$0 \leq$ <code>k</code> $\leq 25$<br><code>s</code> 由小写英文字母组成</p><blockquote><p>动态规划：<code>dp</code> 数组记录以某一字母结尾的最长理想字符串长度；对于 <code>s[i]</code>，其在字母表中的索引为 <code>t</code>，以其结尾的最大长度，仅需考虑其 <code>[t-k, t+k]</code> 范围内的字母即可，而无需考虑 <code>s[i]</code> 之前的所有字母。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">longestIdealString</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span>, k: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        dp = [<span class="hljs-number">0</span>]*<span class="hljs-number">26</span><br>        n = <span class="hljs-built_in">len</span>(s)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            t = <span class="hljs-built_in">ord</span>(s[i]) - <span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;a&#x27;</span>)<br>            dp[t] = <span class="hljs-number">1</span> + <span class="hljs-built_in">max</span>(dp[<span class="hljs-built_in">max</span>(t-k, <span class="hljs-number">0</span>): t+k+<span class="hljs-number">1</span>])<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(dp)<br></code></pre></td></tr></table></figure><hr><h1 id="2022-07-31-第-304-场周赛"><a href="#2022-07-31-第-304-场周赛" class="headerlink" title="2022-07-31 第 304 场周赛"></a>2022-07-31 第 304 场周赛</h1><h2 id="T1-6132-使数组中所有元素都等于零"><a href="#T1-6132-使数组中所有元素都等于零" class="headerlink" title="T1 6132. 使数组中所有元素都等于零"></a>T1 <a href="https://leetcode.cn/contest/weekly-contest-304/problems/make-array-zero-by-subtracting-equal-amounts/">6132. 使数组中所有元素都等于零</a></h2><p>给你一个非负整数数组 <code>nums</code> 。在一步操作中，你必须：</p><p>选出一个正整数 <code>x</code> ，<code>x</code> 需要小于或等于 <code>nums</code> 中 <strong>最小</strong> 的 <strong>非零</strong> 元素。<br><code>nums</code> 中的每个正整数都减去 <code>x</code>。<br>返回使 <code>nums</code> 中所有元素都等于 <code>0</code> 需要的 <strong>最少</strong> 操作数。</p><blockquote><p><strong>输入</strong>：nums &#x3D; [1,5,0,3,5]<br><strong>输出</strong>：3<br><strong>解释</strong>：<br>第一步操作：选出 x &#x3D; 1 ，之后 nums &#x3D; [0,4,0,2,4] 。<br>第二步操作：选出 x &#x3D; 2 ，之后 nums &#x3D; [0,2,0,0,2] 。<br>第三步操作：选出 x &#x3D; 2 ，之后 nums &#x3D; [0,0,0,0,0] 。</p></blockquote><p>提示：<br>$1 \leq$ <code>nums.length</code> $\leq 100$<br>$0 \leq$ <code>nums[i]</code> $\leq 100$</p><blockquote><p><strong>解题思路</strong>：<br><strong>模拟</strong>：每次减掉数组中的最小非零元素即可，直到全部置零</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">minimumOperations</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        nums.sort()<br>        k = self.helper(nums)<br>        ret = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">while</span> k:<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>                <span class="hljs-keyword">if</span> nums[i] &gt; <span class="hljs-number">0</span>:<br>                    nums[i] -= k<br>            k = self.helper(nums)<br>            ret += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> ret<br>        <br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">helper</span>(<span class="hljs-params">self, nums</span>):<br>        <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> nums:<br>            <span class="hljs-keyword">if</span> num &gt; <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">return</span> num<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><h2 id="T2-6133-分组的最大数量"><a href="#T2-6133-分组的最大数量" class="headerlink" title="T2 6133. 分组的最大数量"></a>T2 <a href="https://leetcode.cn/contest/weekly-contest-304/problems/maximum-number-of-groups-entering-a-competition/">6133. 分组的最大数量</a></h2><p>给你一个正整数数组 <code>grades</code> ，表示大学中一些学生的成绩。你打算将 <strong>所有</strong> 学生分为一些 <strong>有序</strong> 的非空分组，其中分组间的顺序满足以下全部条件：</p><p>第 <code>i</code> 个分组中的学生总成绩 <strong>小于</strong> 第 <code>(i + 1)</code> 个分组中的学生总成绩，对所有组均成立（除了最后一组）。<br>第 <code>i</code> 个分组中的学生总数 <strong>小于</strong> 第 <code>(i + 1)</code> 个分组中的学生总数，对所有组均成立（除了最后一组）。<br>返回可以形成的 <strong>最大</strong> 组数。</p><blockquote><p><strong>输入</strong>：grades &#x3D; [10,6,12,7,3,5]<br><strong>输出</strong>：3<br><strong>解释</strong>：下面是形成 3 个分组的一种可行方法：<br>-&gt; 第 1 个分组的学生成绩为 grades &#x3D; [12] ，总成绩：12 ，学生数：1<br>-&gt; 第 2 个分组的学生成绩为 grades &#x3D; [6,7] ，总成绩：6 + 7 &#x3D; 13 ，学生数：2<br>-&gt; 第 3 个分组的学生成绩为 grades &#x3D; [10,3,5] ，总成绩：10 + 3 + 5 &#x3D; 18 ，学生数：3<br>可以证明无法形成超过 3 个分组。</p></blockquote><p>提示：<br>$1 \leq$ <code>grades.length</code> $\leq 10^{5}$<br>$1 \leq$ <code>grades[i]</code> $\leq 10^{5}$</p><blockquote><p>又是脑筋急转弯，大无语<br><strong>解题思路</strong>：<br>分组的形式可以是：将 <code>n</code> 个学生分成 <code>k</code> 组，每组人数为 <code>1、2、...、k</code>，若 $\frac{k \times (k+1)}{2} &lt; n$，即多出来 <code>m(&lt; k+1)</code> 个学生，则将多出来的学生随机分配到前面的组内。按成绩有序分组则可以满足总成绩递增的原则。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">maximumGroups</span>(<span class="hljs-params">self, grades: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        i = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">while</span> i * (i+<span class="hljs-number">1</span>) // <span class="hljs-number">2</span> &lt;= <span class="hljs-built_in">len</span>(grades):<br>            i += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> i-<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h2 id="T3-6134-找到离给定两个节点最近的节点"><a href="#T3-6134-找到离给定两个节点最近的节点" class="headerlink" title="T3 6134. 找到离给定两个节点最近的节点"></a>T3 <a href="https://leetcode.cn/contest/weekly-contest-304/problems/find-closest-node-to-given-two-nodes/">6134. 找到离给定两个节点最近的节点</a></h2><p>给你一个 <code>n</code> 个节点的 <strong>有向图</strong> ，节点编号为 <code>0</code> 到 <code>n - 1</code> ，每个节点 <strong>至多</strong> 有一条出边。</p><p>有向图用大小为 <code>n</code> 下标从 <code>0</code> 开始的数组 <code>edges</code> 表示，表示节点 <code>i</code> 有一条有向边指向 <code>edges[i]</code> 。如果节点 <code>i</code> 没有出边，那么 <code>edges[i] == -1</code> 。</p><p>同时给你两个节点 <code>node1</code> 和 <code>node2</code> 。</p><p>请你返回一个从 <code>node1</code> 和 <code>node2</code> 都能到达节点的编号，使节点 <code>node1</code> 和节点 <code>node2</code> 到这个节点的距离 较大值最小化。如果有多个答案，请返回 <strong>最小</strong> 的节点编号。如果答案不存在，返回 <code>-1</code> 。</p><p>注意 <code>edges</code> 可能包含环。</p><blockquote><p><strong>输入</strong>：edges &#x3D; [2,2,3,-1], node1 &#x3D; 0, node2 &#x3D; 1<br><strong>输出</strong>：2<br><strong>解释</strong>：从节点 0 到节点 2 的距离为 1 ，从节点 1 到节点 2 的距离为 1 。<br>两个距离的较大值为 1 。我们无法得到一个比 1 更小的较大值，所以我们返回节点 2 。<br><img src="https://s2.loli.net/2022/08/01/ouWwCn3yR51z4G8.png"></p></blockquote><p>提示：<br><code>n</code> &#x3D;&#x3D; <code>edges.length</code><br>$2 \leq$ <code>n</code> $\leq 10^{5}$<br>$-1 \leq$ <code>edges[i]</code> $&lt;$ <code>n</code><br><code>edges[i]</code> !&#x3D; <code>i</code><br>$0 \leq$ <code>node1</code>, <code>node2</code> $&lt;$ <code>n</code></p><blockquote><p><strong>解题思路</strong>：<br>从两个节点开始 <code>BFS</code>，记录到每个节点的距离，最后选择二者均能到达且距离最小的即可。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">closestMeetingNode</span>(<span class="hljs-params">self, edges: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], node1: <span class="hljs-built_in">int</span>, node2: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        n = <span class="hljs-built_in">len</span>(edges)<br>        dst = [[-<span class="hljs-number">1</span>]*n <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]<br>        starts = [node1, node2]<br>        <br>        <span class="hljs-keyword">for</span> i, start <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(starts):<br>            dst[i][start] = <span class="hljs-number">0</span><br>            queue = [start]<br>            dist = <span class="hljs-number">1</span><br>            flag = [<span class="hljs-number">0</span>]*n<br>            <span class="hljs-keyword">while</span> queue:<br>                node = queue.pop(<span class="hljs-number">0</span>)<br>                <span class="hljs-keyword">if</span> edges[node] &gt;= <span class="hljs-number">0</span>:<br>                    <span class="hljs-keyword">if</span> dst[i][edges[node]] &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> dist &lt; dst[i][edges[node]] <span class="hljs-keyword">or</span> dst[i][edges[node]] &lt; <span class="hljs-number">0</span>:<br>                        dst[i][edges[node]] = dist<br>                    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> flag[edges[node]]:<br>                        queue.append(edges[node])<br>                        flag[edges[node]] = <span class="hljs-number">1</span><br>                dist += <span class="hljs-number">1</span><br>        <br>        minD = inf<br>        retNode = -<span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            <span class="hljs-keyword">if</span> dst[<span class="hljs-number">0</span>][i] &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> dst[<span class="hljs-number">1</span>][i] &gt;= <span class="hljs-number">0</span>:<br>                dst = <span class="hljs-built_in">max</span>(dst[<span class="hljs-number">0</span>][i], dst[<span class="hljs-number">1</span>][i])<br>                <span class="hljs-keyword">if</span> dst &lt; minD:<br>                    retNode = i<br>                    minD = dst<br>        <span class="hljs-keyword">return</span> retNode<br>                <br></code></pre></td></tr></table></figure><h2 id="T4-6135-图中的最长环"><a href="#T4-6135-图中的最长环" class="headerlink" title="T4 6135. 图中的最长环"></a>T4 <a href="https://leetcode.cn/contest/weekly-contest-304/problems/longest-cycle-in-a-graph/">6135. 图中的最长环</a></h2><p>给你一个 <code>n</code> 个节点的 <strong>有向图</strong> ，节点编号为 <code>0</code> 到 <code>n - 1</code> ，其中每个节点 <strong>至多</strong> 有一条出边。</p><p>图用一个大小为 <code>n</code> 下标从 <code>0</code> 开始的数组 <code>edges</code> 表示，节点 <code>i</code> 到节点 <code>edges[i]</code> 之间有一条有向边。如果节点 <code>i</code> 没有出边，那么 <code>edges[i] == -1</code> 。</p><p>请你返回图中的 <strong>最长</strong> 环，如果没有任何环，请返回 <code>-1</code> 。</p><p>一个环指的是起点和终点是 <strong>同一个</strong> 节点的路径。</p><blockquote><p><strong>输入</strong>：edges &#x3D; [3,3,4,2,3]<br><strong>输出</strong>：3<br><strong>解释</strong>：图中的最长环是：2 -&gt; 4 -&gt; 3 -&gt; 2 。<br>这个环的长度为 3 ，所以返回 3 。<br><img src="https://s2.loli.net/2022/08/01/mwDHV6cRz8EoiPa.png"></p></blockquote><p>提示：<br><code>n</code> &#x3D;&#x3D; <code>edges.length</code><br>$2 \leq$ <code>n</code> $\leq 10^{5}$<br>$-1 \leq$ <code>edges[i]</code> $&lt;$ <code>n</code><br><code>edges[i]</code> !&#x3D; <code>i</code></p><blockquote><p><strong>解题思路</strong>：<br>做完 <code>T3</code>，一种<strong>容易想到的思路</strong>是：从每个节点开始遍历整个有向图，若能到达自身节点则说明其处于环内，且距离为环的长度，遍历所有节点，更新最长环的长度即为答案。时间复杂度为 $O(N^{2}+N)$，超时无疑。<br><strong>优化思路</strong>：<br>通过<strong>拓扑排序</strong>依次<strong>去除入度为 <code>0</code> 的节点</strong>后，仅<strong>保留处于闭环内的节点</strong>，再通过上述方式遍历，且每个节点仅需访问一遍， 可将 <code>visit</code> 数组置于循环外。时间复杂度为 $O(N)$ 。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">longestCycle</span>(<span class="hljs-params">self, edges: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        n = <span class="hljs-built_in">len</span>(edges)<br>        inDegrees = [<span class="hljs-number">0</span>]*n   <span class="hljs-comment"># 入度</span><br>        ret = -<span class="hljs-number">1</span><br>        <br>        <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> edges:<br>            <span class="hljs-keyword">if</span> node &gt;= <span class="hljs-number">0</span>:<br>                inDegrees[node] += <span class="hljs-number">1</span><br>        <br>        <span class="hljs-comment"># 拓扑排序，依次去除入度为0的节点</span><br>        queue = [node <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n) <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> inDegrees[node]]<br>        <span class="hljs-keyword">while</span> queue:<br>            node = queue.pop() <br>            dst_node = edges[node]<br>            <span class="hljs-keyword">if</span> dst_node &gt;= <span class="hljs-number">0</span>:<br>                inDegrees[dst_node] -= <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> inDegrees[dst_node]:<br>                    queue.append(dst_node)<br><br>        visit = [<span class="hljs-number">0</span>]*n<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> inDegrees[i] <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> visit[i]:<br>                <span class="hljs-keyword">continue</span><br>            queue = [i]<br>            dist = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">while</span> queue:<br>                node = queue.pop() <span class="hljs-comment"># 注意这里用 pop()，pop(0) 复杂度为 O(N)，会超时                 </span><br>                <span class="hljs-keyword">if</span> edges[node] &gt;= <span class="hljs-number">0</span>:<br>                    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> visit[edges[node]]:      <span class="hljs-comment"># 未访问</span><br>                        queue.append(edges[node])<br>                        visit[edges[node]] = <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">else</span>:                           <span class="hljs-comment"># 已访问，闭环结束，更新最长环</span><br>                        <span class="hljs-keyword">if</span> dist &gt; ret:<br>                            ret = dist<br>                dist += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><hr><h1 id="2022-07-24-第-303-场周赛"><a href="#2022-07-24-第-303-场周赛" class="headerlink" title="2022-07-24 第 303 场周赛"></a>2022-07-24 第 303 场周赛</h1><h2 id="T1-6124-第一个出现两次的字母"><a href="#T1-6124-第一个出现两次的字母" class="headerlink" title="T1 6124. 第一个出现两次的字母"></a>T1 <a href="https://leetcode.cn/contest/weekly-contest-303/problems/first-letter-to-appear-twice/">6124. 第一个出现两次的字母</a></h2><p>给你一个由小写英文字母组成的字符串 s ，请你找出并返回第一个出现 两次 的字母。</p><p><strong>注意</strong>：</p><ul><li>如果 a 的 第二次 出现比 b 的 第二次 出现在字符串中的位置更靠前，则认为字母 a 在字母 b 之前出现两次。</li><li>s 包含至少一个出现两次的字母。</li></ul><blockquote><p><strong>输入</strong>：s &#x3D; “abccbaacz”<br><strong>输出</strong>：”c”<br><strong>解释</strong>：<br>字母 ‘a’ 在下标 0 、5 和 6 处出现。<br>字母 ‘b’ 在下标 1 和 4 处出现。<br>字母 ‘c’ 在下标 2 、3 和 7 处出现。<br>字母 ‘z’ 在下标 8 处出现。<br>字母 ‘c’ 是第一个出现两次的字母，因为在所有字母中，’c’ 第二次出现的下标是最小的。</p></blockquote><p>提示：<br>$2 \leq$ <code>s.length</code> $\leq 100$<br><code>s</code> 由小写英文字母组成<br><code>s</code> 包含至少一个重复字母</p><blockquote><p><strong>解题思路</strong>：<br>哈希表＋模拟</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">repeatedCharacter</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">str</span>:<br>        hashmap = [<span class="hljs-number">0</span>]*<span class="hljs-number">26</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(s)):<br>            t = <span class="hljs-built_in">ord</span>(s[i])-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;a&#x27;</span>)<br>            hashmap[t] += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> hashmap[t] == <span class="hljs-number">2</span>:<br>                <span class="hljs-keyword">return</span> s[i]<br></code></pre></td></tr></table></figure><h2 id="T2-6125-相等行列对"><a href="#T2-6125-相等行列对" class="headerlink" title="T2 6125. 相等行列对"></a>T2 <a href="https://leetcode.cn/contest/weekly-contest-303/problems/equal-row-and-column-pairs/">6125. 相等行列对</a></h2><p>给你一个下标从 <code>0</code> 开始、大小为 <code>n x n</code> 的整数矩阵 <code>grid</code> ，返回满足 <code>Ri</code> 行和 <code>Cj</code> 列相等的行列对 <code>(Ri, Cj)</code> 的数目。</p><p>如果行和列以相同的顺序包含相同的元素（即相等的数组），则认为二者是相等的。</p><blockquote><p><strong>输入</strong>：grid &#x3D; [[3,1,2,2],[1,4,4,5],[2,4,2,2],[2,4,2,2]]<br><strong>输出</strong>：3<br><strong>解释</strong>：存在三对相等行列对：<br>-&gt; (第 0 行，第 0 列)：[3,1,2,2]<br>-&gt; (第 2 行, 第 2 列)：[2,4,2,2]<br>-&gt; (第 3 行, 第 2 列)：[2,4,2,2]<br><img src="https://s2.loli.net/2022/07/24/DOSv9eRTg2JaFXt.jpg" width=128 height=128 left=0/></p></blockquote><p>提示：<br><code>n</code> &#x3D;&#x3D; <code>grid.length</code> &#x3D;&#x3D; <code>grid[i].length</code><br>$1 \leq$ <code>n</code> $\leq 200$<br>$1 \leq$ <code>grid[i][j]</code> $\leq 10^{5}$</p><blockquote><p><strong>解题思路</strong>：<br>暴力即可，对比每一行和每一列</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">equalPairs</span>(<span class="hljs-params">self, grid: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        n = <span class="hljs-built_in">len</span>(grid)<br>        ret = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>                <span class="hljs-keyword">if</span> grid[i] == [grid[k][j] <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]:<br>                    ret += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><h2 id="T3-6126-设计食物评分系统"><a href="#T3-6126-设计食物评分系统" class="headerlink" title="T3 6126. 设计食物评分系统"></a>T3 <a href="https://leetcode.cn/contest/weekly-contest-303/problems/design-a-food-rating-system/">6126. 设计食物评分系统</a></h2><p>设计一个支持下述操作的食物评分系统：</p><p>修改 系统中列出的某种食物的评分。<br>返回系统中某一类烹饪方式下评分最高的食物。<br>实现 <code>FoodRatings</code> 类：</p><ul><li><code>FoodRatings(String[] foods, String[] cuisines, int[] ratings)</code> 初始化系统。食物由 <code>foods</code>、<code>cuisines</code> 和 <code>ratings</code> 描述，长度均为 <code>n</code> 。</li><li><code>foods[i]</code> 是第 <code>i</code> 种食物的名字。</li><li><code>cuisines[i]</code> 是第 <code>i</code> 种食物的烹饪方式。</li><li><code>ratings[i]</code> 是第 <code>i</code> 种食物的最初评分。</li><li><code>void changeRating(String food, int newRating)</code> 修改名字为 <code>food</code> 的食物的评分。</li><li><code>String highestRated(String cuisine)</code> 返回指定烹饪方式 <code>cuisine</code> 下评分最高的食物的名字。如果存在并列，返回 <strong>字典序较小</strong> 的名字。<br><strong>注意</strong>：字符串 <code>x</code> 的字典序比字符串 <code>y</code> 更小的前提是：<code>x</code> 在字典中出现的位置在 <code>y</code> 之前，也就是说，要么 <code>x</code> 是 <code>y</code> 的前缀，或者在满足 <code>x[i] != y[i]</code> 的第一个位置 <code>i</code> 处，<code>x[i]</code> 在字母表中出现的位置在 <code>y[i]</code> 之前。</li></ul><blockquote><p><strong>输入</strong>:<br>[“FoodRatings”, “highestRated”, “highestRated”, “changeRating”, “highestRated”, “changeRating”, “highestRated”]<br>[[[“kimchi”, “miso”, “sushi”, “moussaka”, “ramen”, “bulgogi”], [“korean”, “japanese”, “japanese”, “greek”, “japanese”, “korean”], [9, 12, 8, 15, 14, 7]], [“korean”], [“japanese”], [“sushi”, 16], [“japanese”], [“ramen”, 16], [“japanese”]]<br><strong>输出</strong>:<br>[null, “kimchi”, “ramen”, null, “sushi”, null, “ramen”]<br><strong>解释</strong>:<br>FoodRatings foodRatings &#x3D; new FoodRatings([“kimchi”, “miso”, “sushi”, “moussaka”, “ramen”, “bulgogi”], [“korean”, “japanese”, “japanese”, “greek”, “japanese”, “korean”], [9, 12, 8, 15, 14, 7]);<br>foodRatings.highestRated(“korean”); &#x2F;&#x2F; 返回 “kimchi”，kimchi” 是分数最高的韩式料理，评分为 9 。<br>foodRatings.highestRated(“japanese”); &#x2F;&#x2F; 返回 “ramen”，”ramen” 是分数最高的日式料理，评分为 14 。<br>foodRatings.changeRating(“sushi”, 16); &#x2F;&#x2F; “sushi” 现在评分变更为 16 。<br>foodRatings.highestRated(“japanese”); &#x2F;&#x2F; 返回 “sushi”，”sushi” 是分数最高的日式料理，评分为 16 。<br>foodRatings.changeRating(“ramen”, 16); &#x2F;&#x2F; “ramen” 现在评分变更为 16 。<br>foodRatings.highestRated(“japanese”); &#x2F;&#x2F; 返回 “ramen”，”sushi” 和 “ramen” 的评分都是 16，但是，”ramen” 的字典序比 “sushi” 更小。</p></blockquote><p>提示：<br>$1 \leq$ <code>n</code> $\leq 2 * 10^{4}$<br><code>n</code> &#x3D;&#x3D; <code>foods.length</code> &#x3D;&#x3D; <code>cuisines.length</code> &#x3D;&#x3D; <code>ratings.length</code><br>$1 \leq$ <code>foods[i].length, cuisines[i].length</code> $\leq 10$<br><code>foods[i]、cuisines[i]</code> 由小写英文字母组成<br>$1 \leq$ <code>ratings[i]</code> $\leq 10^{8}$<br><code>foods</code> 中的所有字符串 <strong>互不相同</strong><br>在对 <code>changeRating</code> 的所有调用中，<code>food</code> 是系统中食物的名字。<br>在对 <code>highestRated</code> 的所有调用中，<code>cuisine</code> 是系统中 <strong>至少一种</strong> 食物的烹饪方式。<br>最多调用 <code>changeRating</code> 和 <code>highestRated</code> 总计 $2 * 10^{4}$ 次</p><blockquote><p><strong>解题思路</strong>：<br>建立映射： 将 <code>food[i]</code> 的烹饪方式 <code>cuisines[i]</code> 和 最初评分 <code>ratings[i]</code> 存入一个字典中。将 <code>cuisines[i]</code> 的下所有食物和最初评分存入一个字典中，并按照评分排序。<br><strong>实现方式</strong>：<br>利用 <code>defaultdict</code> 建立字典，相比于 <code>dict</code>，其可以避免字典中 <code>key</code> 值不存在时的 <code>KeyError</code> 异常，<code>SortedList</code> 用于评分排序，其插入元素的时间复杂度为 $O(logn)$<br><strong><code>Dict + SortedList</code> 可以解决所有这类问题</strong></p></blockquote><blockquote><p><code>defaultdict</code> 使用详解：<code>defaultdict</code> 在初始化时可以提供一个 <code>default_factory</code> 的参数，<code>default_factory</code> 接收一个工厂函数作为参数， 可以是 <code>int</code>、<code>str</code>、<code>list</code> 等内置函数，也可以是 <strong>自定义函数</strong>。其他方法与 <code>dict</code> 一致。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> sortedcontainers <span class="hljs-keyword">import</span> SortedList<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FoodRatings</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, foods: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], cuisines: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], ratings: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>):<br>        self.food_cuisine = defaultdict(<span class="hljs-built_in">str</span>)<br>        self.food_rating = defaultdict(<span class="hljs-built_in">int</span>)<br>        self.cuisine = defaultdict(<span class="hljs-keyword">lambda</span>: SortedList(key=<span class="hljs-keyword">lambda</span> x: (-x[<span class="hljs-number">1</span>], x[<span class="hljs-number">0</span>])))<br>        n = <span class="hljs-built_in">len</span>(foods)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            self.food_cuisine[foods[i]] = cuisines[i]              <span class="hljs-comment"># food -&gt; cuisine</span><br>            self.food_rating[foods[i]] = ratings[i]                <span class="hljs-comment"># food -&gt; rating</span><br>            self.cuisine[cuisines[i]].add([foods[i], ratings[i]])  <span class="hljs-comment"># cuisine -&gt; [food, rating]</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">changeRating</span>(<span class="hljs-params">self, food: <span class="hljs-built_in">str</span>, newRating: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        cuisine = self.food_cuisine[food]<br>        rating = self.food_rating[food]<br>        self.cuisine[cuisine].discard([food, rating])   <span class="hljs-comment"># 删除已有评分记录，这里用 discard 以防报错</span><br>        self.food_rating[food] = newRating              <span class="hljs-comment"># 更新 food -&gt; rating</span><br>        self.cuisine[cuisine].add([food, newRating])    <span class="hljs-comment"># 添加评分记录 cuisine -&gt; [food, rating]</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">highestRated</span>(<span class="hljs-params">self, cuisine: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-comment"># 设定了按评分的负值排序，当最大评分有多个 food 时，字典序更小的会在前</span><br>        <span class="hljs-keyword">return</span> self.cuisine[cuisine][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br><br><br><span class="hljs-comment"># Your FoodRatings object will be instantiated and called as such:</span><br><span class="hljs-comment"># obj = FoodRatings(foods, cuisines, ratings)</span><br><span class="hljs-comment"># obj.changeRating(food,newRating)</span><br><span class="hljs-comment"># param_2 = obj.highestRated(cuisine)</span><br></code></pre></td></tr></table></figure><h2 id="T4-6127-优质数对的数目"><a href="#T4-6127-优质数对的数目" class="headerlink" title="T4 6127. 优质数对的数目"></a>T4 <a href="https://leetcode.cn/contest/weekly-contest-303/problems/number-of-excellent-pairs/">6127. 优质数对的数目</a></h2><p>给你一个下标从 <code>0</code> 开始的正整数数组 <code>nums</code> 和一个正整数 <code>k</code> 。</p><p>如果满足下述条件，则数对 <code>(num1, num2)</code> 是 <strong>优质数对</strong> ：</p><ul><li><code>num1</code> 和 <code>num2</code> <strong>都</strong> 在数组 <code>nums</code> 中存在。</li><li><code>num1 OR num2</code> 和 <code>num1 AND num2</code> 的二进制表示中值为 <code>1</code> 的位数之和大于等于 <code>k</code> ，其中 <code>OR</code> 是按位 <strong>或</strong> 操作，而 <code>AND</code> 是按位 <strong>与</strong> 操作。<br>返回 <strong>不同</strong> 优质数对的数目。</li></ul><p>如果 <code>a != c</code> 或者 <code>b != d</code> ，则认为 <code>(a, b)</code> 和 <code>(c, d)</code> 是不同的两个数对。例如，<code>(1, 2)</code> 和 <code>(2, 1)</code> 不同。</p><p><strong>注意</strong>：如果 <code>num1</code> 在数组中至少出现 <strong>一次</strong> ，则满足 <code>num1 == num2</code> 的数对 <code>(num1, num2)</code> 也可以是优质数对。</p><blockquote><p><strong>输入</strong>：nums &#x3D; [1,2,3,1], k &#x3D; 3<br><strong>输出</strong>：5<br><strong>解释</strong>：有如下几个优质数对：<br>-&gt; (3, 3)：(3 AND 3) 和 (3 OR 3) 的二进制表示都等于 (11) 。值为 1 的位数和等于 2 + 2 &#x3D; 4 ，大于等于 k &#x3D; 3 。<br>-&gt; (2, 3) 和 (3, 2)： (2 AND 3) 的二进制表示等于 (10) ，(2 OR 3) 的二进制表示等于 (11) 。值为 1 的位数和等于 1 + 2 &#x3D; 3 。<br>-&gt; (1, 3) 和 (3, 1)： (1 AND 3) 的二进制表示等于 (01) ，(1 OR 3) 的二进制表示等于 (11) 。值为 1 的位数和等于 1 + 2 &#x3D; 3 。<br>所以优质数对的数目是 5 。</p></blockquote><p>提示：<br>$1 \leq$ <code>nums.length</code> $\leq 10^{5}$<br>$1 \leq$ <code>nums[i]</code> $\leq 10^{9}$<br>$1 \leq$ <code>k</code> $\leq 60$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">countExcellentPairs</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], k: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        nums = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(nums))<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        bits = []<br>        ret = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            bit = self.helper(nums[i])<br>            <span class="hljs-keyword">if</span> <span class="hljs-number">2</span>*bit &gt;= k:<br>                ret += <span class="hljs-number">1</span><br>            bits.append([nums[i], bit])<br><br>        bits.sort(key=<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>])<br>        left, right = <span class="hljs-number">0</span>, n-<span class="hljs-number">1</span><br>        <br>        <span class="hljs-keyword">while</span> left &lt; right:<br>            <span class="hljs-keyword">if</span> bits[left][<span class="hljs-number">1</span>] + bits[right][<span class="hljs-number">1</span>] &gt;= k:<br>                ret += (right-left)*<span class="hljs-number">2</span><br>                right -= <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                left += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> ret<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">helper</span>(<span class="hljs-params">self, n</span>):<br>        ret = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">while</span> n &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">if</span> n % <span class="hljs-number">2</span> == <span class="hljs-number">1</span>:<br>                ret += <span class="hljs-number">1</span><br>            n = n//<span class="hljs-number">2</span><br>        <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><hr><h1 id="2022-07-23-第-83-场双周赛"><a href="#2022-07-23-第-83-场双周赛" class="headerlink" title="2022-07-23 第 83 场双周赛"></a>2022-07-23 第 83 场双周赛</h1><h2 id="T1-6128-最好的扑克手牌"><a href="#T1-6128-最好的扑克手牌" class="headerlink" title="T1 6128. 最好的扑克手牌"></a>T1 <a href="https://leetcode.cn/contest/biweekly-contest-83/problems/best-poker-hand/">6128. 最好的扑克手牌</a></h2><p>给你一个整数数组 <code>ranks</code> 和一个字符数组 <code>suit</code> 。你有 <code>5</code> 张扑克牌，第 <code>i</code> 张牌大小为 <code>ranks[i]</code> ，花色为 <code>suits[i]</code> 。</p><p>下述是从好到坏你可能持有的 <strong>手牌类型</strong> ：</p><p>1、<code>&quot;Flush&quot;</code>：同花，五张相同花色的扑克牌。<br>2、<code>&quot;Three of a Kind&quot;</code>：三条，有 <code>3</code> 张大小相同的扑克牌。<br>3、<code>&quot;Pair&quot;</code>：对子，两张大小一样的扑克牌。<br>4、<code>&quot;High Card&quot;</code>：高牌，五张大小互不相同的扑克牌。<br>请你返回一个字符串，表示给定的 <code>5</code> 张牌中，你能组成的 <strong>最好手牌类型</strong> 。</p><p><strong>注意</strong>：返回的字符串 大小写 需与题目描述相同。</p><blockquote><p><strong>输入</strong>：ranks &#x3D; [4,4,2,4,4], suits &#x3D; [“d”,”a”,”a”,”b”,”c”]<br><strong>输出</strong>：”Three of a Kind”<br><strong>解释</strong>：第一、二和四张牌组成三张相同大小的扑克牌，所以得到 “Three of a Kind” 。<br>注意我们也可以得到 “Pair” ，但是 “Three of a Kind” 是更好的手牌类型。<br>有其他的 3 张牌也可以组成 “Three of a Kind” 手牌类型。</p></blockquote><p>提示：<br><code>ranks.length</code> &#x3D;&#x3D; <code>suits.length</code> &#x3D;&#x3D; 5<br>1 $\leq$ <code>ranks[i]</code> $\leq$ 13<br>‘a’ $\leq$ <code>suits[i]</code> $\leq$  ‘d’<br>任意两张扑克牌不会同时有相同的大小和花色。</p><blockquote><p><strong>解题思路</strong>：<br>直接模拟即可</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">bestHand</span>(<span class="hljs-params">self, ranks: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], suits: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(suits)) == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Flush&quot;</span><br>        ranks.sort()<br>        ranks_set = <span class="hljs-built_in">set</span>(ranks)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(ranks_set) &lt;= <span class="hljs-number">3</span>:<br>            <span class="hljs-keyword">if</span> ranks.count(ranks[<span class="hljs-number">2</span>]) &gt;= <span class="hljs-number">3</span>:<br>                <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Three of a Kind&quot;</span><br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Pair&quot;</span><br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">len</span>(ranks_set) == <span class="hljs-number">4</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Pair&quot;</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;High Card&quot;</span><br></code></pre></td></tr></table></figure><h2 id="T2-6129-全-0-子数组的数目"><a href="#T2-6129-全-0-子数组的数目" class="headerlink" title="T2 6129. 全 0 子数组的数目"></a>T2 <a href="https://leetcode.cn/contest/biweekly-contest-83/problems/number-of-zero-filled-subarrays/">6129. 全 0 子数组的数目</a></h2><p>给你一个整数数组 <code>nums</code> ，返回全部为 <code>0</code> 的 <strong>子数组</strong> 数目。</p><p><strong>子数组</strong> 是一个数组中一段连续非空元素组成的序列。</p><blockquote><p><strong>输入</strong>：nums &#x3D; [0,0,0,2,0,0]<br><strong>输出</strong>：9<br><strong>解释</strong>：<br>子数组 [0] 出现了 5 次。<br>子数组 [0,0] 出现了 3 次。<br>子数组 [0,0,0] 出现了 1 次。<br>不存在长度大于 3 的全 0 子数组，所以我们返回 9 。</p></blockquote><p>提示：<br>$1 \leq$ <code>nums.length</code> $\leq 10^{5}$<br>$-10^{9} \leq$ <code>nums[i]</code> $\leq 10^{9}$</p><blockquote><p><strong>解题思路</strong>：<br><strong>数学题</strong>：记录连续的 <code>0</code> 的个数，然后对于每个连续长度为 <code>n</code> 的 <code>0</code> 序列，其中包含了长度 <code>n-k+1</code> 个为 <code>k</code> 的 <code>0</code> 序列，（如一个长度为 <code>3</code> 的 <code>0</code> 序列，其包含了 <code>3</code> 个长度为 <code>1</code>，<code>2</code> 个长度为 <code>2</code>，<code>1</code> 个长度为 <code>3</code> 的 <code>0</code> 序列）。易知，长度为 <code>n</code> 的 <code>0</code> 序列包含了 $\sum_{i&#x3D;1}^{n}i$ 个全 <code>0</code> 子数组。对每个 <code>0</code> 序列的长度计算以上值累加即为答案。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">zeroFilledSubarray</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        cnt = <span class="hljs-number">0</span><br>        temp = []<br>        ret = <span class="hljs-number">0</span><br>        nums.append(-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>            <span class="hljs-keyword">if</span> nums[i] == <span class="hljs-number">0</span>:<br>                cnt += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">elif</span> nums[i] != <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> cnt &gt; <span class="hljs-number">0</span>:<br>                temp.append(cnt)<br>                cnt = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> temp:<br>            ret += t*(t+<span class="hljs-number">1</span>)//<span class="hljs-number">2</span><br>        <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><h2 id="T3-6130-设计数字容器系统"><a href="#T3-6130-设计数字容器系统" class="headerlink" title="T3 6130. 设计数字容器系统"></a>T3 <a href="https://leetcode.cn/contest/biweekly-contest-83/problems/design-a-number-container-system/">6130. 设计数字容器系统</a></h2><p>设计一个数字容器系统，可以实现以下功能：</p><p>在系统中给定下标处 <strong>插入</strong> 或者 <strong>替换</strong> 一个数字。<br><strong>返回</strong> 系统中给定数字的最小下标。<br>请你实现一个 <code>NumberContainers</code> 类：</p><p><code>NumberContainers()</code> 初始化数字容器系统。<br><code>void change(int index, int number)</code> 在下标 <code>index</code> 处填入 <code>number</code> 。如果该下标 <code>index</code> 处已经有数字了，那么用 <code>number</code> 替换该数字。<br><code>int find(int number)</code> 返回给定数字 <code>number</code> 在系统中的最小下标。如果系统中没有 <code>number</code> ，那么返回 <code>-1</code> 。</p><blockquote><p><strong>输入</strong>：<br>[“NumberContainers”, “find”, “change”, “change”, “change”, “change”, “find”, “change”, “find”]<br>[[], [10], [2, 10], [1, 10], [3, 10], [5, 10], [10], [1, 20], [10]]<br><strong>输出</strong>：<br>[null, -1, null, null, null, null, 1, null, 2]<br><strong>解释</strong>：<br>NumberContainers nc &#x3D; new NumberContainers();<br>nc.find(10); &#x2F;&#x2F; 没有数字 10 ，所以返回 -1 。<br>nc.change(2, 10); &#x2F;&#x2F; 容器中下标为 2 处填入数字 10 。<br>nc.change(1, 10); &#x2F;&#x2F; 容器中下标为 1 处填入数字 10 。<br>nc.change(3, 10); &#x2F;&#x2F; 容器中下标为 3 处填入数字 10 。<br>nc.change(5, 10); &#x2F;&#x2F; 容器中下标为 5 处填入数字 10 。<br>nc.find(10); &#x2F;&#x2F; 数字 10 所在的下标为 1 ，2 ，3 和 5 。因为最小下标为 1 ，所以返回 1 。<br>nc.change(1, 20); &#x2F;&#x2F; 容器中下标为 1 处填入数字 20 。注意，下标 1 处之前为 10 ，现在被替换为 20 。<br>nc.find(10); &#x2F;&#x2F; 数字 10 所在下标为 2 ，3 和 5 。最小下标为 2 ，所以返回 2 。 </p></blockquote><p>提示：<br>$1 \leq$ <code>index, number</code> $\leq 10^{9}$<br>调用 <code>change</code> 和 <code>find</code> 的 <strong>总次数</strong> 不超过 $10^{5}$ 次。</p><blockquote><p><code>Python</code> 数据结构还是掌握得不够，<code>Sortedlist</code> 居然不会用。除以下特殊方法外，其他方法与列表基本一致。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> sortedcontainers <span class="hljs-keyword">import</span> SortedList<br>sl = SortedList([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>])<br><span class="hljs-comment"># 添加元素</span><br>sl.add(<span class="hljs-number">4</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>]<br><span class="hljs-comment">#删除元素 </span><br>sl.remove(<span class="hljs-number">4</span>) <span class="hljs-comment"># 元素不存在时会报错</span><br>sl.discard(<span class="hljs-number">4</span>) <span class="hljs-comment"># 元素不存在时不会报错</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>]<br><span class="hljs-comment"># 查找元素</span><br>pos = test_sl.bisect_left(<span class="hljs-number">2</span>) <span class="hljs-comment"># 查找元素存在的位置</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> sortedcontainers <span class="hljs-keyword">import</span> SortedList<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NumberContainers</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.indexes = &#123;&#125;<br>        self.<span class="hljs-built_in">list</span> = &#123;&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">change</span>(<span class="hljs-params">self, index: <span class="hljs-built_in">int</span>, number: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">if</span> index <span class="hljs-keyword">in</span> self.<span class="hljs-built_in">list</span>:<br>            self.indexes[self.<span class="hljs-built_in">list</span>[index]].remove(index)<br>        <span class="hljs-keyword">if</span> number <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.indexes:<br>            self.indexes[number] = SortedList()<br>        self.indexes[number].add(index)<br>        self.<span class="hljs-built_in">list</span>[index] = number<br>            <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">find</span>(<span class="hljs-params">self, number: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">if</span> number <span class="hljs-keyword">in</span> self.indexes <span class="hljs-keyword">and</span> self.indexes[number]:<br>            <span class="hljs-comment"># print(self.list, self.indexes)</span><br>            <span class="hljs-keyword">return</span> self.indexes[number][<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span><br>        <br><span class="hljs-comment"># Your NumberContainers object will be instantiated and called as such:</span><br><span class="hljs-comment"># obj = NumberContainers()</span><br><span class="hljs-comment"># obj.change(index,number)</span><br><span class="hljs-comment"># param_2 = obj.find(number)</span><br></code></pre></td></tr></table></figure><h2 id="T4-6131-不可能得到的最短骰子序列"><a href="#T4-6131-不可能得到的最短骰子序列" class="headerlink" title="T4 6131. 不可能得到的最短骰子序列"></a>T4 <a href="https://leetcode.cn/contest/biweekly-contest-83/problems/shortest-impossible-sequence-of-rolls/">6131. 不可能得到的最短骰子序列</a></h2><p>给你一个长度为 <code>n</code> 的整数数组 <code>rolls</code> 和一个整数 <code>k</code> 。你扔一个 <code>k</code> 面的骰子 <code>n</code> 次，骰子的每个面分别是 <code>1</code> 到 <code>k</code> ，其中第 <code>i</code> 次扔得到的数字是 <code>rolls[i]</code> 。</p><p>请你返回 <strong>无法</strong> 从 <code>rolls</code> 中得到的 <strong>最短</strong> 骰子子序列的长度。</p><p>扔一个 <code>k</code> 面的骰子 <code>len</code> 次得到的是一个长度为 <code>len</code> 的 <strong>骰子子序列</strong> 。</p><p>注意 ，子序列只需要保持在原数组中的顺序，不需要连续。</p><blockquote><p><strong>输入</strong>：rolls &#x3D; [4,2,1,2,3,3,2,4,1], k &#x3D; 4<br><strong>输出</strong>：3<br><strong>解释</strong>：所有长度为 1 的骰子子序列 [1] ，[2] ，[3] ，[4] 都可以从原数组中得到。<br>所有长度为 2 的骰子子序列 [1, 1] ，[1, 2] ，… ，[4, 4] 都可以从原数组中得到。<br>子序列 [1, 4, 2] 无法从原数组中得到，所以我们返回 3 。<br>还有别的子序列也无法从原数组中得到。</p></blockquote><p>提示：<br><code>n</code> &#x3D;&#x3D; <code>rolls.length</code><br>$1 \leq$ <code>n</code> $\leq 10^{5}$<br>$1 \leq$ <code>rolls[i]</code> $\leq$ <code>k</code> $\leq 10^{5}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python"><br></code></pre></td></tr></table></figure><hr><h1 id="2022-07-17-第-302-场周赛"><a href="#2022-07-17-第-302-场周赛" class="headerlink" title="2022-07-17 第 302 场周赛"></a>2022-07-17 第 302 场周赛</h1><h2 id="T1-6120-数组能形成多少数对"><a href="#T1-6120-数组能形成多少数对" class="headerlink" title="T1 6120. 数组能形成多少数对"></a>T1 <a href="https://leetcode.cn/contest/weekly-contest-302/problems/maximum-number-of-pairs-in-array/">6120. 数组能形成多少数对</a></h2><p>给你一个下标从 <code>0</code> 开始的整数数组 <code>nums</code> 。在一步操作中，你可以执行以下步骤：</p><p>从 <code>nums</code> 选出 <strong>两个 相等的</strong> 整数<br>从 <code>nums</code> 中移除这两个整数，形成一个 数对<br>请你在 <code>nums</code> 上多次执行此操作直到无法继续执行。</p><p>返回一个下标从 <code>0</code> 开始、长度为 <code>2</code> 的整数数组 <code>answer</code> 作为答案，其中 <code>answer[0]</code> 是形成的数对数目，<code>answer[1]</code> 是对 <code>nums</code> 尽可能执行上述操作后剩下的整数数目。</p><blockquote><p><strong>输入</strong>：nums &#x3D; [1,3,2,1,3,2,2]<br><strong>输出</strong>：[3,1]<br><strong>解释</strong>：<br>nums[0] 和 nums[3] 形成一个数对，并从 nums 中移除，nums &#x3D; [3,2,3,2,2] 。<br>nums[0] 和 nums[2] 形成一个数对，并从 nums 中移除，nums &#x3D; [2,2,2] 。<br>nums[0] 和 nums[1] 形成一个数对，并从 nums 中移除，nums &#x3D; [2] 。<br>无法形成更多数对。总共形成 3 个数对，nums 中剩下 1 个数字。</p></blockquote><p><strong>提示</strong>：<br>$1 \leq nums.length \leq 100$<br>$1 \leq nums[i] \leq 100$</p><blockquote><p>愉快的签到题~一上来肾上腺素飙升，WA了两发（🙂🙂🙂</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">numberOfPairs</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        hashmap = [<span class="hljs-number">0</span>]*<span class="hljs-number">101</span><br>        ret = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            hashmap[nums[i]] += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">101</span>):<br>            ret += hashmap[i]//<span class="hljs-number">2</span><br>        <span class="hljs-keyword">return</span> [ret, n-<span class="hljs-number">2</span>*ret]<br></code></pre></td></tr></table></figure><h2 id="T2-6164-数位和相等数对的最大和"><a href="#T2-6164-数位和相等数对的最大和" class="headerlink" title="T2 6164. 数位和相等数对的最大和"></a>T2 <a href="https://leetcode.cn/contest/weekly-contest-302/problems/max-sum-of-a-pair-with-equal-sum-of-digits/">6164. 数位和相等数对的最大和</a></h2><p>给你一个下标从 <code>0</code> 开始的数组 <code>nums</code> ，数组中的元素都是 <strong>正</strong> 整数。请你选出两个下标 <code>i</code> 和 <code>j</code>（<code>i != j</code>），且 <code>nums[i]</code> 的数位和与  <code>nums[j]</code> 的数位和相等。</p><p>请你找出所有满足条件的下标 <code>i</code> 和 <code>j</code> ，找出并返回 <code>nums[i] + nums[j]</code> 可以得到的 <strong>最大值</strong> 。</p><blockquote><p><strong>输入</strong>：nums &#x3D; [18,43,36,13,7]<br><strong>输出</strong>：54<br><strong>解释</strong>：满足条件的数对 (i, j) 为：<br>-&gt; (0, 2) ，两个数字的数位和都是 9 ，相加得到 18 + 36 &#x3D; 54 。<br>-&gt; (1, 4) ，两个数字的数位和都是 7 ，相加得到 43 + 7 &#x3D; 50 。<br>所以可以获得的最大和是 54 。</p></blockquote><p><strong>提示</strong>：<br>$1 \leq nums.length \leq 10^{5}$<br>$1 \leq nums[i] \leq 10^{9}$</p><blockquote><p>看了眼数据量，最大和值为 <code>9×9=81</code>，直接哈希表记录，<code>haspmap[i]</code> 为数位和为 <code>i</code> 的整数列表，返回列表长度大于 <code>2</code> 且列表中最大的两个元素之和，遍历 <code>hashmap</code> 更新结果</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">maximumSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        hashmap = [[] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">82</span>)]<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            hashmap[self.helper(nums[i])].append(nums[i])<br>        ret = -<span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(hashmap)):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(hashmap[i]) &gt;= <span class="hljs-number">2</span>:<br>                hashmap[i].sort()<br>                <span class="hljs-keyword">if</span> ret &lt; hashmap[i][-<span class="hljs-number">1</span>] + hashmap[i][-<span class="hljs-number">2</span>]:<br>                    ret = hashmap[i][-<span class="hljs-number">1</span>] + hashmap[i][-<span class="hljs-number">2</span>]<br>        <span class="hljs-keyword">return</span> ret<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">helper</span>(<span class="hljs-params">self, n</span>):<br>        ret = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">while</span> n &gt; <span class="hljs-number">0</span>:<br>            ret += n%<span class="hljs-number">10</span><br>            n = n//<span class="hljs-number">10</span><br>        <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><h2 id="T3-6121-裁剪数字后查询第-K-小的数字"><a href="#T3-6121-裁剪数字后查询第-K-小的数字" class="headerlink" title="T3 6121. 裁剪数字后查询第 K 小的数字"></a>T3 <a href="https://leetcode.cn/contest/weekly-contest-302/problems/query-kth-smallest-trimmed-number/">6121. 裁剪数字后查询第 K 小的数字</a></h2><p>给你一个下标从 <code>0</code> 开始的字符串数组 <code>nums</code> ，其中每个字符串 <strong>长度相等</strong> 且只包含数字。</p><p>再给你一个下标从 <code>0</code> 开始的二维整数数组 <code>queries</code> ，其中 <code>queries[i] = [ki, trimi]</code> 。对于每个 <code>queries[i]</code> ，你需要：</p><p>将 <code>nums</code> 中每个数字 <strong>裁剪</strong> 到剩下 <strong>最右边</strong> <code>trimi</code> 个数位。<br>在裁剪过后的数字中，找到 <code>nums</code> 中第 <code>ki</code> 小数字对应的 <strong>下标</strong> 。如果两个裁剪后数字一样大，那么下标 <strong>更小</strong> 的数字视为更小的数字。<br>将 <code>nums</code> 中每个数字恢复到原本字符串。<br>请你返回一个长度与 <code>queries</code> 相等的数组 <code>answer</code>，其中 <code>answer[i]</code> 是第 <code>i</code> 次查询的结果。</p><blockquote><p><strong>输入</strong>：nums &#x3D; [“102”,”473”,”251”,”814”], queries &#x3D; [[1,1],[2,3],[4,2],[1,2]]<br><strong>输出</strong>：[2,2,1,0]<br><strong>解释</strong>：<br>查询 1. 裁剪到只剩 1 个数位后，nums &#x3D; [“2”,”3”,”1”,”4”] 。最小的数字是 1 ，下标为 2 。<br>查询 2. 裁剪到剩 3 个数位后，nums 没有变化。第 2 小的数字是 251 ，下标为 2 。<br>查询 3. 裁剪到剩 2 个数位后，nums &#x3D; [“02”,”73”,”51”,”14”] 。第 4 小的数字是 73 ，下标为 1 。<br>查询 4. 裁剪到剩 2 个数位后，最小数字是 2 ，下标为 0 。<br>   注意，裁剪后数字 “02” 值为 2 。</p></blockquote><p><strong>提示</strong>：<br>裁剪到剩下 <code>x</code> 个数位的意思是不断删除最左边的数位，直到剩下 <code>x</code> 个数位。<br><code>nums</code> 中的字符串可能会有前导 <code>0</code> 。<br>$1 \leq nums.length \leq 100$<br>$1 \leq nums[i].length \leq 100$<br><code>nums[i]</code> 只包含数字。<br>所有 <code>nums[i].length</code> 的长度 相同 。<br>$1 \leq queries.length \leq 100$<br>$queries[i].length &#x3D;&#x3D; 2$<br>$1 \leq k_{i} \leq nums.length$<br>$1 \leq trim_{i} \leq nums[0].length$</p><blockquote><p>这题和第四题换一下位置还行，这点数据量，写半天超时，找不到哪里出了死循环……儒哥更惨 👇👇👇</p></blockquote><p><img src="https://s2.loli.net/2022/07/17/fTxAbjd1YGFVuRB.png"></p><blockquote><p><code>TLE</code> 的代码 👇👇👇</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">smallestTrimmedNumbers</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], queries: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        n = <span class="hljs-built_in">len</span>(nums[<span class="hljs-number">0</span>])<br>        ret = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(queries)):<br>            nums_ = []<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>                temp = <span class="hljs-number">0</span><br>                <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(queries[i][<span class="hljs-number">1</span>]):<br>                    temp += <span class="hljs-built_in">int</span>(<span class="hljs-built_in">ord</span>(nums[j][n-k-<span class="hljs-number">1</span>])-<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;0&#x27;</span>))*<span class="hljs-number">10</span>**k<br>                nums_.append([j, temp])<br>            nums_.sort(key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>])<br>            ret.append(nums_[queries[i][<span class="hljs-number">0</span>]-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>])<br>            <br>        <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><blockquote><p>赛后，看了眼大家的写法，才意识到<strong>字符串可以直接排序</strong>，不一定非要转换成整数，字符串转化成整数的过程嵌套在双层循环中，时间复杂度爆炸。<br>以下为 <code>AC</code> 的代码 👇👇👇</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">smallestTrimmedNumbers</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], queries: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        n = <span class="hljs-built_in">len</span>(nums[<span class="hljs-number">0</span>])<br>        ret = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(queries)):<br>            nums_ = []<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>                nums_.append([j, nums[j][-queries[i][<span class="hljs-number">1</span>]:]])  <span class="hljs-comment"># 上面的四行改成这一行</span><br>            nums_.sort(key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>])<br>            ret.append(nums_[queries[i][<span class="hljs-number">0</span>]-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>])<br>                <br>        <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><h2 id="T4-6122-使数组可以被整除的最少删除次数"><a href="#T4-6122-使数组可以被整除的最少删除次数" class="headerlink" title="T4 6122. 使数组可以被整除的最少删除次数"></a>T4 <a href="https://leetcode.cn/contest/weekly-contest-302/problems/minimum-deletions-to-make-array-divisible/">6122. 使数组可以被整除的最少删除次数</a></h2><p>给你两个正整数数组 <code>nums</code> 和 <code>numsDivide</code> 。你可以从 <code>nums</code> 中删除任意数目的元素。</p><p>请你返回使 <code>nums</code> 中 <strong>最小</strong> 元素可以整除 <code>numsDivide</code> 中所有元素的 <strong>最少</strong> 删除次数。如果无法得到这样的元素，返回 <code>-1</code> 。</p><p>如果 <code>y % x == 0</code> ，那么我们说整数 <code>x</code> 整除 <code>y</code> 。</p><blockquote><p><strong>输入</strong>：nums &#x3D; [2,3,2,4,3], numsDivide &#x3D; [9,6,9,3,15]<br><strong>输出</strong>：2<br><strong>解释</strong>：<br>[2,3,2,4,3] 中最小元素是 2 ，它无法整除 numsDivide 中所有元素。<br>我们从 nums 中删除 2 个大小为 2 的元素，得到 nums &#x3D; [3,4,3] 。<br>[3,4,3] 中最小元素为 3 ，它可以整除 numsDivide 中所有元素。<br>可以证明 2 是最少删除次数。</p></blockquote><p><strong>提示</strong>：<br>$1 \leq nums.length, numsDivide.length \leq 10^{5}$<br>$1 \leq nums[i], numsDivide[i] \leq 10^{9}$</p><blockquote><p><code>Hard</code>……<br>问题等价于求在有序的 <code>nums</code> 中，能够被 <code>numsDivide</code> 中所有元素的最大公约数整除的最小元素的位置索引</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">minOperations</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], numsDivide: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        factor = numsDivide[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(numsDivide)):<br>            factor = gcd(factor, numsDivide[i])<br>        nums.sort()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>            <span class="hljs-keyword">if</span> factor % nums[i] == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">return</span> i<br>        <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><hr><h1 id="2022-07-13-蔚来-2023-届提前批笔试"><a href="#2022-07-13-蔚来-2023-届提前批笔试" class="headerlink" title="2022-07-13 蔚来 2023 届提前批笔试"></a>2022-07-13 蔚来 2023 届提前批笔试</h1><h2 id="T2-最少的操作次数"><a href="#T2-最少的操作次数" class="headerlink" title="T2 最少的操作次数"></a>T2 最少的操作次数</h2><p>小红可以对一个数进行如下两种操作：将这个数乘以 $x$，或将这个数乘以 $y$。操作次数是没有限制的。小红想知道，自己最少经过多少次操作以后，可以把 $a$ 变成 $b$ ？</p><blockquote><p><strong>输入描述</strong>：<br>四个正整数 $x, y, a, b$，用空格隔开。<br>$2 \leq x, y \leq 100$<br>$1 \leq a, b \leq 10^{9}$<br><strong>输出描述</strong>：<br>如果小红无论如何都无法把 $a$ 变成 $b$，则输出 $-1$。否则输出小红操作的最少次数，可以证明，如果存在某种操作，那么最少次数一定是固定的。</p></blockquote><blockquote><p><strong>示例输入</strong>：<br>2 3 5 20<br><strong>示例输出</strong>：<br>2<br><strong>解释</strong>：<br>x &#x3D; 2，y &#x3D; 3，进行两次乘 2 操作，可以把 5 变成 20。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python"><br></code></pre></td></tr></table></figure><h2 id="T3-牛牛的开心旅行"><a href="#T3-牛牛的开心旅行" class="headerlink" title="T3 牛牛的开心旅行"></a>T3 牛牛的开心旅行</h2><p>牛牛对 $n$ 个城市的旅游情况进行了规划。<br>已知每个城市有两种属性 $x_{i}$ 和 $y_{i}$，其中 $x_{i}$ 表示去第 $i$ 号城市的花费，$y_{i}$ 表示在第 $i$ 号城市游玩后会得到的开心值。<br>现在牛牛希望从中挑选一些城市去游玩，但挑选出的城市必须满足任意两个城市之间花费的绝对值小于 $k$。<br>他想请你帮他计算出在满足上述条件下能得到的最大开心值是多少。 </p><blockquote><p><strong>输入描述</strong>：<br>第一行输入两个正整数 $n$和 $k$。<br>接下来 $n$ 行，每行输入两个正整数 $x_{i}$ 和 $y_{i}$，分别表示每个城市的两种属性。<br>$1 \leq n \leq 10^{5}$<br>$1 \leq k \leq 10^{9}$<br>$1 \leq x_{i}, y_{i} \leq 10^{9}$<br><strong>输出描述</strong>：<br>输出一个整数表示答案。</p></blockquote><blockquote><p><strong>示例输入</strong>：<br>5 3<br>1 3<br>2 1<br>5 2<br>3 1<br>4 3<br><strong>示例输出</strong>：<br>6<br><strong>解释</strong>：<br>牛牛可以选择去 3 号， 4 号和 5 号城市进行游玩，并收获最大的开心值。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python"><br></code></pre></td></tr></table></figure><hr><h1 id="2022-07-10-第-301-场周赛"><a href="#2022-07-10-第-301-场周赛" class="headerlink" title="2022-07-10 第 301 场周赛"></a>2022-07-10 第 301 场周赛</h1><p>前一天晚上的双周赛太太太难了，给我整自闭了，问了一下儒哥，也觉得最近周赛难度上升了，跟公司笔试难度不匹配…..<br>然后上午的周赛A了三道，虽然有点慢，但我不自闭了</p><h2 id="T1-6112-装满杯子需要的最短总时长"><a href="#T1-6112-装满杯子需要的最短总时长" class="headerlink" title="T1. 6112. 装满杯子需要的最短总时长"></a>T1. <a href="https://leetcode.cn/contest/weekly-contest-301/problems/minimum-amount-of-time-to-fill-cups/">6112. 装满杯子需要的最短总时长</a></h2><p>现有一台饮水机，可以制备冷水、温水和热水。每秒钟，可以装满 <code>2</code> 杯 不同 类型的水或者 <code>1</code> 杯任意类型的水。<br>给你一个下标从 <code>0</code> 开始、长度为 <code>3</code> 的整数数组 <code>amount</code> ，其中 <code>amount[0]</code>、<code>amount[1]</code> 和 <code>amount[2]</code> 分别表示需要装满冷水、温水和热水的杯子数量。返回装满所有杯子所需的 <code>最少</code> 秒数。</p><blockquote><p><strong>输入</strong>：amount &#x3D; [1,4,2]<br><strong>输出</strong>：4<br><strong>解释</strong>：下面给出一种方案：<br>第 1 秒：装满一杯冷水和一杯温水。<br>第 2 秒：装满一杯温水和一杯热水。<br>第 3 秒：装满一杯温水和一杯热水。<br>第 4 秒：装满一杯温水。<br>可以证明最少需要 4 秒才能装满所有杯子。</p></blockquote><blockquote><p><strong>解题思路</strong>：<br><strong>简易贪心</strong> — 每次装水都选择剩余最多的两种类型的杯子。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fillCups</span>(<span class="hljs-params">self, amount: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        time = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-built_in">sum</span>(amount) &gt; <span class="hljs-number">0</span>:<br>            amount.sort()<br>            <span class="hljs-keyword">if</span> amount[-<span class="hljs-number">1</span>] &gt; <span class="hljs-number">0</span>:<br>                amount[-<span class="hljs-number">1</span>] -= <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> amount[-<span class="hljs-number">2</span>] &gt; <span class="hljs-number">0</span>:<br>                amount[-<span class="hljs-number">2</span>] -= <span class="hljs-number">1</span><br>            time += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> time<br></code></pre></td></tr></table></figure><h2 id="T2-6113-无限集中的最小数字"><a href="#T2-6113-无限集中的最小数字" class="headerlink" title="T2. 6113. 无限集中的最小数字"></a>T2. <a href="https://leetcode.cn/contest/weekly-contest-301/problems/smallest-number-in-infinite-set/">6113. 无限集中的最小数字</a></h2><p>现有一个包含所有正整数的集合 <code>[1, 2, 3, 4, 5, ...]</code>。<br>实现 <code>SmallestInfiniteSet</code> 类：</p><ul><li><code>SmallestInfiniteSet()</code> 初始化 <code>SmallestInfiniteSet</code> 对象以包含 <code>所有</code> 正整数。</li><li><code>int popSmallest()</code> 移除 并返回该无限集中的最小整数。</li><li><code>void addBack(int num)</code> 如果正整数 <code>num</code> 不 存在于无限集中，则将一个 <code>num</code> 添加 到该无限集中。</li></ul><blockquote><p><strong>输入</strong><br>[“SmallestInfiniteSet”, “addBack”, “popSmallest”, “popSmallest”, “popSmallest”, “addBack”, “popSmallest”, “popSmallest”, “popSmallest”]<br>[[], [2], [], [], [], [1], [], [], []]<br><strong>输出</strong><br>[null, null, 1, 2, 3, null, 1, 4, 5]<br><strong>解释</strong><br>SmallestInfiniteSet smallestInfiniteSet &#x3D; new SmallestInfiniteSet();<br>smallestInfiniteSet.addBack(2);    &#x2F;&#x2F; 2 已经在集合中，所以不做任何变更。<br>smallestInfiniteSet.popSmallest(); &#x2F;&#x2F; 返回 1 ，因为 1 是最小的整数，并将其从集合中移除。<br>smallestInfiniteSet.popSmallest(); &#x2F;&#x2F; 返回 2 ，并将其从集合中移除。<br>smallestInfiniteSet.popSmallest(); &#x2F;&#x2F; 返回 3 ，并将其从集合中移除。<br>smallestInfiniteSet.addBack(1);    &#x2F;&#x2F; 将 1 添加到该集合中。<br>smallestInfiniteSet.popSmallest(); &#x2F;&#x2F; 返回 1 ，因为 1 在上一步中被添加到集合中，且 1 是最小的整数，并将其从集合中移除。<br>smallestInfiniteSet.popSmallest(); &#x2F;&#x2F; 返回 4 ，并将其从集合中移除。<br>smallestInfiniteSet.popSmallest(); &#x2F;&#x2F; 返回 5 ，并将其从集合中移除。</p></blockquote><blockquote><p><strong>解题思路</strong>：<br><strong>别问，问就是哈希表</strong> — 建立一个 <code>pop_list</code> 记录已被移除的元素。<br>（1） <code>popSmallest()</code> 操作：检验 <code>pop_list</code> 中元素 <code>pop_list[i]</code> 与其下标 <code>i</code> 是否满足 <code>pop_list[i] == i+1</code>，不满足则说明 <code>i+1</code> 还在无限集合中，返回 <code>i+1</code> 即可，否则返回最后一个元素的下一个数字 <code>pop_list[-1]+1</code> ，并将返回结果添加至 <code>pop_list</code> 中。<br>（2）<code>addBack(num)</code> 操作：直接从 <code>pop_list</code> 中移除 <code>num</code> 即可。</p></blockquote><p>时间复杂度： $O(M \times NlogN)$，$M$ 为调用各方法的次数，空间复杂度： $O(N)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SmallestInfiniteSet</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.pop_list = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">popSmallest</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">if</span> self.pop_list == []:<br>            ret = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            self.pop_list.sort()<br>            n = <span class="hljs-built_in">len</span>(self.pop_list)<br>            i = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">while</span>(i &lt; n):<br>                <span class="hljs-keyword">if</span> self.pop_list[i] != i+<span class="hljs-number">1</span>:<br>                    ret = i+<span class="hljs-number">1</span><br>                    <span class="hljs-keyword">break</span><br>                i += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> i == n:<br>                ret = self.pop_list[-<span class="hljs-number">1</span>] + <span class="hljs-number">1</span><br>        self.pop_list.append(ret)<br>        <span class="hljs-keyword">return</span> ret<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">addBack</span>(<span class="hljs-params">self, num: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">if</span> num <span class="hljs-keyword">in</span> self.pop_list:<br>            self.pop_list.remove(num)<br></code></pre></td></tr></table></figure><h2 id="T3-6114-移动片段得到字符串"><a href="#T3-6114-移动片段得到字符串" class="headerlink" title="T3. 6114. 移动片段得到字符串"></a>T3. <a href="https://leetcode.cn/contest/weekly-contest-301/problems/move-pieces-to-obtain-a-string/">6114. 移动片段得到字符串</a></h2><p>给你两个字符串 <code>start</code> 和 <code>target</code> ，长度均为 <code>n</code> 。每个字符串 <strong><code>仅</code></strong> 由字符 <code>&#39;L&#39;</code>、<code>&#39;R&#39;</code> 和 <code>&#39;_&#39;</code> 组成，其中：</p><ul><li>字符 <code>&#39;L&#39;</code> 和 <code>&#39;R&#39;</code> 表示片段，其中片段 <code>&#39;L&#39;</code> 只有在其左侧直接存在一个 <strong><code>空位</code></strong> 时才能向 <strong><code>左</code></strong> 移动，而片段 <code>&#39;R&#39;</code> 只有在其右侧直接存在一个 <strong><code>空位</code></strong> 时才能向 <strong><code>右</code></strong> 移动。</li><li>字符 <code>&#39;_&#39;</code> 表示可以被 任意 <code>&#39;L&#39;</code> 或 <code>&#39;R&#39;</code> 片段占据的空位。</li></ul><p>如果在移动字符串 <code>start</code> 中的片段任意次之后可以得到字符串 <code>target</code> ，返回 <code>true</code> ；否则，返回 <code>false</code> 。</p><blockquote><p><strong>输入</strong>：start &#x3D; “_L__R__R_”, target &#x3D; “L______RR”<br><strong>输出</strong>：true<br><strong>解释</strong>：可以从字符串 start 获得 target ，需要进行下面的移动：<br>-&gt; 将第一个片段向左移动一步，字符串现在变为 “L___R__R_” 。<br>-&gt; 将最后一个片段向右移动一步，字符串现在变为 “L___R___R” 。<br>-&gt; 将第二个片段向右移动散步，字符串现在变为 “L______RR” 。<br>可以从字符串 start 得到 target ，所以返回 true 。</p></blockquote><blockquote><p><strong>解题思路</strong>：<br><strong>还是哈希表</strong> — 分别记录 <code>start</code> 和 <code>target</code> 中 <code>&#39;L&#39;</code> 和 <code>&#39;R&#39;</code> 出现的顺序和位置索引，若顺序不一致则直接返回 <code>false</code>，否则进一步判断 <code>start</code> 中每一个 <code>&#39;L&#39;</code> 和 <code>&#39;R&#39;</code> 的位置的合法性。<br>遍历所有 <code>&#39;L&#39;</code> 的索引，对于第 <code>i</code> 个 <code>&#39;L&#39;</code>， 其在 <code>start</code> 中的位置索引不能 <strong><code>小于</code></strong> 在 <code>target</code> 中的索引，即左侧。<br>遍历所有 <code>&#39;R&#39;</code> 的索引，对于第 <code>i</code> 个 <code>&#39;R&#39;</code>， 其在 <code>start</code> 中的位置索引不能 <strong><code>大于</code></strong> 在 <code>target</code> 中的索引，即右侧。</p></blockquote><p>时间复杂度： $O(N)$，空间复杂度： $O(N)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">canChange</span>(<span class="hljs-params">self, start: <span class="hljs-built_in">str</span>, target: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        n = <span class="hljs-built_in">len</span>(start)<br>        s_L, s_R, t_L, t_R = []<br>        s, t = <span class="hljs-string">&#x27;&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            <span class="hljs-keyword">if</span> start[i] == <span class="hljs-string">&#x27;L&#x27;</span>:<br>                s_L.append(i)<br>                s += <span class="hljs-string">&#x27;L&#x27;</span><br>            <span class="hljs-keyword">if</span> start[i] == <span class="hljs-string">&#x27;R&#x27;</span>:<br>                s_R.append(i)<br>                s += <span class="hljs-string">&#x27;R&#x27;</span><br>            <span class="hljs-keyword">if</span> target[i] == <span class="hljs-string">&#x27;L&#x27;</span>:<br>                t_L.append(i)<br>                t += <span class="hljs-string">&#x27;L&#x27;</span><br>            <span class="hljs-keyword">if</span> target[i] == <span class="hljs-string">&#x27;R&#x27;</span>:<br>                t_R.append(i)<br>                t += <span class="hljs-string">&#x27;R&#x27;</span><br>        <br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(s_L) != <span class="hljs-built_in">len</span>(t_L) <span class="hljs-keyword">or</span> <span class="hljs-built_in">len</span>(s_R) != <span class="hljs-built_in">len</span>(t_R) <span class="hljs-keyword">or</span> s != t:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(s_L)):<br>            <span class="hljs-keyword">if</span> s_L[i] &lt; t_L[i]:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(s_R)):<br>            <span class="hljs-keyword">if</span> s_R[i] &gt; t_R[i]:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><h2 id="T4-6115-统计理想数组的数目"><a href="#T4-6115-统计理想数组的数目" class="headerlink" title="T4. 6115. 统计理想数组的数目"></a>T4. <a href="https://leetcode.cn/contest/weekly-contest-301/problems/count-the-number-of-ideal-arrays/">6115. 统计理想数组的数目</a></h2><p>给你两个整数 <code>n</code> 和 <code>maxValue</code> ，用于描述一个 <strong><code>理想数组</code></strong> 。<br>对于下标从 <code>0</code> 开始、长度为 <code>n</code> 的整数数组 <code>arr</code> ，如果满足以下条件，则认为该数组是一个 <strong><code>理想数组</code></strong> ：</p><ul><li>每个 <code>arr[i]</code> 都是从 <code>1</code> 到 <code>maxValue</code> 范围内的一个值，其中 <code>0 &lt;= i &lt; n</code> 。</li><li>每个 <code>arr[i]</code> 都可以被 <code>arr[i - 1]</code> 整除，其中 <code>0 &lt; i &lt; n</code> 。</li></ul><p>返回长度为 <code>n</code> 的 <strong><code>不同</code></strong> 理想数组的数目。由于答案可能很大，返回对 <code>10^9 + 7</code> 取余的结果。</p><blockquote><p><strong>输入</strong>：n &#x3D; 2, maxValue &#x3D; 5<br><strong>输出</strong>：10<br><strong>解释</strong>：存在以下理想数组：<br>-&gt; 以 1 开头的数组（5 个）：[1,1]、[1,2]、[1,3]、[1,4]、[1,5]<br>-&gt; 以 2 开头的数组（2 个）：[2,2]、[2,4]<br>-&gt; 以 3 开头的数组（1 个）：[3,3]<br>-&gt; 以 4 开头的数组（1 个）：[4,4]<br>-&gt; 以 5 开头的数组（1 个）：[5,5]<br>共计 5 + 2 + 1 + 1 + 1 &#x3D; 10 个不同理想数组。</p></blockquote><blockquote><p><strong>解题思路</strong>：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python"><br></code></pre></td></tr></table></figure><hr><h1 id="2022-07-09-第-82-场双周赛"><a href="#2022-07-09-第-82-场双周赛" class="headerlink" title="2022-07-09 第 82 场双周赛"></a>2022-07-09 第 82 场双周赛</h1><p>晚上十点半开始的周赛，晚上十一点才想起来自己报名了，第一次参加周赛就给我来了个下马威……</p><h2 id="T1-6116-计算布尔二叉树的值"><a href="#T1-6116-计算布尔二叉树的值" class="headerlink" title="T1. 6116. 计算布尔二叉树的值"></a>T1. <a href="https://leetcode.cn/contest/biweekly-contest-82/problems/evaluate-boolean-binary-tree/">6116. 计算布尔二叉树的值</a></h2><p>给你一棵 <strong><code>完整二叉树</code></strong> 的根，这棵树有以下特征：<br><strong><code>叶子节点</code></strong> 要么值为 <code>0</code> 要么值为 <code>1</code> ，其中 <code>0</code> 表示 <code>False</code> ，<code>1</code> 表示 <code>True</code> 。<br><strong><code>非叶子节点</code></strong> 要么值为 <code>2</code> 要么值为 <code>3</code> ，其中 <code>2</code> 表示逻辑或 <code>OR</code> ，<code>3</code> 表示逻辑与 <code>AND</code> 。<br><strong><code>计算</code></strong> 一个节点的值方式如下：<br>如果节点是个叶子节点，那么节点的 <strong><code>值</code></strong> 为它本身，即 <code>True</code> 或者 <code>False</code> 。<br>否则，**<code>计算</code>** 两个孩子的节点值，然后将该节点的运算符对两个孩子值进行 <strong><code>运算</code></strong> 。<br>返回根节点 <code>root</code> 的布尔运算值。<br><strong><code>完整二叉树</code></strong> 是每个节点有 <code>0</code> 个或者 <code>2</code> 个孩子的二叉树。<br><strong><code>叶子节点</code></strong> 是没有孩子的节点。</p><p><img src="https://s2.loli.net/2022/07/10/cuzUoGgBhpmRPMA.png"></p><blockquote><p><strong>输入</strong>：root &#x3D; [2,1,3,null,null,0,1]<br><strong>输出</strong>：true<br><strong>解释</strong>：上图展示了计算过程。<br>AND 与运算节点的值为 False AND True &#x3D; False 。<br>OR 运算节点的值为 True OR False &#x3D; True 。<br>根节点的值为 True ，所以我们返回 true 。</p></blockquote><blockquote><p><strong>解题思路</strong>：<br><strong>递归</strong> — 由于是完整二叉树，只包含叶子节点和“伪根节点”，叶子节点直接返回，“伪根节点”返回运算结果。</p></blockquote><p>时间复杂度： $O(N)$，空间复杂度： $O(1)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluateTree</span>(<span class="hljs-params">self, root: <span class="hljs-type">Optional</span>[TreeNode]</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-keyword">if</span> root.val == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">elif</span> root.val == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">elif</span> root.val == <span class="hljs-number">2</span>:<br>            <span class="hljs-keyword">return</span> self.evaluateTree(root.left) <span class="hljs-keyword">or</span> self.evaluateTree(root.right)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> self.evaluateTree(root.left) <span class="hljs-keyword">and</span> self.evaluateTree(root.right)<br></code></pre></td></tr></table></figure><h2 id="T2-6117-坐上公交的最晚时间"><a href="#T2-6117-坐上公交的最晚时间" class="headerlink" title="T2. 6117. 坐上公交的最晚时间"></a>T2. <a href="https://leetcode.cn/contest/biweekly-contest-82/problems/the-latest-time-to-catch-a-bus/">6117. 坐上公交的最晚时间</a></h2><p>给你一个下标从 <code>0</code> 开始长度为 <code>n</code> 的整数数组 <code>buses</code> ，其中 <code>buses[i]</code> 表示第 <code>i</code> 辆公交车的出发时间。同时给你一个下标从 <code>0</code> 开始长度为 <code>m</code> 的整数数组 <code>passengers</code> ，其中 <code>passengers[j]</code> 表示第 <code>j</code> 位乘客的到达时间。所有公交车出发的时间互不相同，所有乘客到达的时间也互不相同。<br>给你一个整数 <code>capacity</code> ，表示每辆公交车 <strong><code>最多</code></strong> 能容纳的乘客数目。<br>每位乘客都会搭乘下一辆有座位的公交车。如果你在 <code>y</code> 时刻到达，公交在 <code>x</code> 时刻出发，满足 <code>y &lt;= x</code>  且公交没有满，那么你可以搭乘这一辆公交。**<code>最早</code>** 到达的乘客优先上车。<br>返回你可以搭乘公交车的最晚到达公交站时间。你 <strong><code>不能</code></strong> 跟别的乘客同时刻到达。<br><strong>注意</strong>：数组 <code>buses</code> 和 <code>passengers</code> 不一定是有序的。</p><blockquote><p><strong>输入</strong>：buses &#x3D; [10,20], passengers &#x3D; [2,17,18,19], capacity &#x3D; 2<br><strong>输出</strong>：16<br><strong>解释</strong>：<br>第 1 辆公交车载着第 1 位乘客。<br>第 2 辆公交车载着你和第 2 位乘客。<br>注意你不能跟其他乘客同一时间到达，所以你必须在第二位乘客之前到达。</p></blockquote><blockquote><p><strong>解题思路</strong>：<br><strong>模拟</strong>：周赛的时候只想着模拟，满脑子都是模拟，最后感觉要过了，提交了四五次都有问题……自闭从此开始</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br></code></pre></td></tr></table></figure><h2 id="T3-6118-最小差值平方和"><a href="#T3-6118-最小差值平方和" class="headerlink" title="T3. 6118. 最小差值平方和"></a>T3. <a href="https://leetcode.cn/contest/biweekly-contest-82/problems/minimum-sum-of-squared-difference/">6118. 最小差值平方和</a></h2><p>给你两个下标从 <code>0</code> 开始的整数数组 <code>nums1</code> 和 <code>nums2</code> ，长度为 <code>n</code> 。<br>数组 <code>nums1</code> 和 <code>nums2</code> 的 <strong><code>差值平方和</code></strong> 定义为所有满足 <code>0 &lt;= i &lt; n</code> 的 <code>(nums1[i] - nums2[i])^2</code> 之和。<br>同时给你两个正整数 <code>k1</code> 和 <code>k2</code> 。你可以将 <code>nums1</code> 中的任意元素 <code>+1</code> 或者 <code>-1</code> 至多 <code>k1</code> 次。类似的，你可以将 <code>nums2</code> 中的任意元素 <code>+1</code> 或者 <code>-1</code> 至多 <code>k2</code> 次。<br>请你返回修改数组 <code>nums1</code> 至多 <code>k1</code> 次且修改数组 <code>nums2</code> 至多 <code>k2</code> 次后的最小 <strong><code>差值平方和</code></strong> 。<br><strong>注意</strong>：你可以将数组中的元素变成 <strong><code>负</code></strong> 整数。</p><blockquote><p><strong>输入</strong>：nums1 &#x3D; [1,4,10,12], nums2 &#x3D; [5,8,6,9], k1 &#x3D; 1, k2 &#x3D; 1<br><strong>输出</strong>：43<br><strong>解释</strong>：一种得到最小差值平方和的方式为：<br>-&gt; 将 nums1[0] 增加一次。<br>-&gt; 将 nums2[2] 增加一次。<br>最小差值平方和为：<br>(2 - 5)^2 + (4 - 8)^2 + (10 - 7)^2 + (12 - 9)^2 &#x3D; 43 。<br>注意，也有其他方式可以得到最小差值平方和，但没有得到比 43 更小答案的方案。</p></blockquote><blockquote><p><strong>解题思路</strong>：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br></code></pre></td></tr></table></figure><h2 id="T4-6119-元素值大于变化阈值的子数组"><a href="#T4-6119-元素值大于变化阈值的子数组" class="headerlink" title="T4. 6119. 元素值大于变化阈值的子数组"></a>T4. <a href="https://leetcode.cn/contest/biweekly-contest-82/problems/subarray-with-elements-greater-than-varying-threshold/">6119. 元素值大于变化阈值的子数组</a></h2><p>给你一个整数数组 <code>nums</code> 和一个整数 <code>threshold</code> 。<br>找到长度为 <code>k</code> 的 <code>nums</code> 子数组，满足数组中 <strong><code>每个</code></strong> 元素都 <strong><code>大于</code></strong> <code>threshold / k</code> 。<br>请你返回满足要求的 <strong><code>任意</code></strong> 子数组的 <strong><code>大小</code></strong> 。如果没有这样的子数组，返回 <code>-1</code> 。<br><strong><code>子数组</code></strong> 是数组中一段连续非空的元素序列。</p><blockquote><p><strong>输入</strong>：nums &#x3D; [6,5,6,5,8], threshold &#x3D; 7<br><strong>输出</strong>：1<br><strong>解释</strong>：子数组 [8] 大小为 1 ，且 8 &gt; 7 &#x2F; 1 &#x3D; 7 。所以返回 1 。<br>注意子数组 [6,5] 大小为 2 ，每个元素都大于 7 &#x2F; 2 &#x3D; 3.5 。<br>类似的，子数组 [6,5,6] ，[6,5,6,5] ，[6,5,6,5,8] 都是符合条件的子数组。<br>所以返回 2, 3, 4 和 5 都可以。</p></blockquote><blockquote><p><strong>解题思路</strong>：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br></code></pre></td></tr></table></figure><hr><p><em>以下为常用方法，方便速查使用</em></p><h1 id="Python篇"><a href="#Python篇" class="headerlink" title="Python篇"></a>Python篇</h1><h2 id="常用数据结构及其操作"><a href="#常用数据结构及其操作" class="headerlink" title="常用数据结构及其操作"></a>常用数据结构及其操作</h2><h3 id="列表（List）"><a href="#列表（List）" class="headerlink" title="列表（List）"></a>列表（List）</h3><p>$\looparrowright$ 方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-built_in">list</span>.append(obj) - 在列表末尾添加新的对象<br><span class="hljs-built_in">list</span>.count(obj) - 统计某个元素在列表中出现的次数<br><span class="hljs-built_in">list</span>.extend(seq) - 在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）<br><span class="hljs-built_in">list</span>.index(obj) - 从列表中找出某个值第一个匹配项的索引位置<br><span class="hljs-built_in">list</span>.insert(index, obj) - 将对象插入列表index的位置<br><span class="hljs-built_in">list</span>.pop([index=-<span class="hljs-number">1</span>]) - 移除列表中的一个元素（默认最后一个元素），并且返回该元素的值<br><span class="hljs-built_in">list</span>.remove(obj) - 移除列表中某个值的第一个匹配项<br><span class="hljs-built_in">list</span>.reverse() - 反向列表中元素<br><span class="hljs-built_in">list</span>.sort(cmp=<span class="hljs-literal">None</span>, key=<span class="hljs-literal">None</span>, reverse=<span class="hljs-literal">False</span>) - 对原列表进行排序<br></code></pre></td></tr></table></figure><h3 id="元组（Tuple）"><a href="#元组（Tuple）" class="headerlink" title="元组（Tuple）"></a>元组（Tuple）</h3><p>$\looparrowright$ 操作大致与列表一致，区别在于<strong>元组的元素不能修改或删除</strong>，只能删除整个元组。</p><h3 id="字典（Dictionary）"><a href="#字典（Dictionary）" class="headerlink" title="字典（Dictionary）"></a>字典（Dictionary）</h3><p>$\looparrowright$ 可变容器模型，可存储任意类型对象。<br>$\looparrowright$ 方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-built_in">dict</span>.clear()<br> - 删除字典内所有元素<br><span class="hljs-built_in">dict</span>.copy()<br> - 返回一个字典的浅复制<br><span class="hljs-built_in">dict</span>.fromkeys(seq[, val])<br> - 创建一个新字典，以序列 seq 中元素做字典的键，val 为字典所有键对应的初始值<br><span class="hljs-built_in">dict</span>.get(key, default=<span class="hljs-literal">None</span>)<br> - 返回指定键的值，如果值不在字典中返回default值<br><span class="hljs-built_in">dict</span>.has_key(key)<br> - 如果键在字典<span class="hljs-built_in">dict</span>里返回true，否则返回false<br><span class="hljs-built_in">dict</span>.items()<br> - 以列表返回可遍历的(键, 值)元组数组<br><span class="hljs-built_in">dict</span>.keys()<br> - 以列表返回一个字典所有的键<br><span class="hljs-built_in">dict</span>.setdefault(key, default=<span class="hljs-literal">None</span>)<br> - 和get()类似, 但如果键不存在于字典中，将会添加键并将值设为default<br><span class="hljs-built_in">dict</span>.update(dict2)<br> - 把字典dict2的键/值对更新到<span class="hljs-built_in">dict</span>里<br><span class="hljs-built_in">dict</span>.values()<br> - 以列表返回字典中的所有值<br><span class="hljs-built_in">dict</span>.pop(key[,default])<br> - 删除字典给定键 key 所对应的值，返回值为被删除的值。key值必须给出。 否则，返回default值<br><span class="hljs-built_in">dict</span>.popitem()<br> - 返回并删除字典中的最后一对键和值<br></code></pre></td></tr></table></figure><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>$\looparrowright$ 方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><code class="hljs Python">string.capitalize()<br> - 把字符串的第一个字符大写<br>string.center(width)<br> - 返回一个原字符串居中,并使用空格填充至长度 width 的新字符串<br>string.count(<span class="hljs-built_in">str</span>, beg=<span class="hljs-number">0</span>, end=<span class="hljs-built_in">len</span>(string))<br> - 返回<span class="hljs-built_in">str</span>在string里面出现的次数，如果beg或者end指定则返回指定范围内<span class="hljs-built_in">str</span>出现的次数<br>string.decode(encoding=<span class="hljs-string">&#x27;UTF-8&#x27;</span>, errors=<span class="hljs-string">&#x27;strict&#x27;</span>)<br> - 以encoding指定的编码格式解码string，如果出错默认报一个ValueError的异常，除非errors指定<span class="hljs-string">&#x27;ignore&#x27;</span>或者<span class="hljs-string">&#x27;replace&#x27;</span><br>string.encode(encoding=<span class="hljs-string">&#x27;UTF-8&#x27;</span>, errors=<span class="hljs-string">&#x27;strict&#x27;</span>)<br> - 以encoding指定的编码格式编码string，如果出错默认报一个ValueError 的异常，除非errors指定的是<span class="hljs-string">&#x27;ignore&#x27;</span>或者<span class="hljs-string">&#x27;replace&#x27;</span><br>string.endswith(obj, beg=<span class="hljs-number">0</span>, end=<span class="hljs-built_in">len</span>(string))<br> - 检查字符串是否以obj结束，如果beg或者end指定则检查指定的范围内是否以obj 结束，如果是，返回<span class="hljs-literal">True</span>，否则返回<span class="hljs-literal">False</span><br>string.expandtabs(tabsize=<span class="hljs-number">8</span>)<br> - 把字符串string中的tab符号转为空格，tab符号默认的空格数是<span class="hljs-number">8</span><br>string.find(<span class="hljs-built_in">str</span>, beg=<span class="hljs-number">0</span>, end=<span class="hljs-built_in">len</span>(string))<br> - 检测<span class="hljs-built_in">str</span>是否包含在string中，如果beg和end指定范围，则检查是否包含在指定范围内，如果是返回开始的索引值，否则返回-<span class="hljs-number">1</span><br>string.<span class="hljs-built_in">format</span>()<br> - 格式化字符串<br>string.index(<span class="hljs-built_in">str</span>, beg=<span class="hljs-number">0</span>, end=<span class="hljs-built_in">len</span>(string))<br> - 跟find()方法一样，只不过如果<span class="hljs-built_in">str</span>不在string中会报一个异常.<br>string.isalnum()<br> - 如果string至少有一个字符并且所有字符都是字母或数字则返回<span class="hljs-literal">True</span>，否则返回<span class="hljs-literal">False</span><br>string.isalpha()<br> - 如果 string 至少有一个字符并且所有字符都是字母则返回 <span class="hljs-literal">True</span>，否则返回 <span class="hljs-literal">False</span><br>string.isdecimal()<br> - 如果 string 只包含十进制数字则返回 <span class="hljs-literal">True</span> 否则返回 <span class="hljs-literal">False</span>.<br>string.isdigit()<br> - 如果 string 只包含数字则返回 <span class="hljs-literal">True</span> 否则返回 <span class="hljs-literal">False</span>.<br>string.islower()<br> - 如果 string 中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是小写，则返回 <span class="hljs-literal">True</span>，否则返回 <span class="hljs-literal">False</span><br>string.isnumeric()<br> - 如果 string 中只包含数字字符，则返回 <span class="hljs-literal">True</span>，否则返回 <span class="hljs-literal">False</span><br>string.isspace()<br> - 如果 string 中只包含空格，则返回 <span class="hljs-literal">True</span>，否则返回 <span class="hljs-literal">False</span>.<br>string.istitle()<br> - 如果 string 是标题化的(见 title())则返回 <span class="hljs-literal">True</span>，否则返回 <span class="hljs-literal">False</span><br>string.isupper() <br> - 如果 string 中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是大写，则返回 <span class="hljs-literal">True</span>，否则返回 <span class="hljs-literal">False</span><br>string.join(seq)<br> - 以 string 作为分隔符，将 seq 中所有的元素(的字符串表示)合并为一个新的字符串<br>string.ljust(width)<br> - 返回一个原字符串左对齐,并使用空格填充至长度 width 的新字符串<br>string.lower()<br> - 转换 string 中所有大写字符为小写.<br>string.lstrip()<br> - 截掉 string 左边的空格<br>string.maketrans(intab, outtab])<br> - 用于创建字符映射的转换表，对于接受两个参数的最简单的调用方式，第一个参数是字符串，表示需要转换的字符，第二个参数也是字符串表示转换的目标。<br><span class="hljs-built_in">max</span>(<span class="hljs-built_in">str</span>)<br> - 返回字符串 <span class="hljs-built_in">str</span> 中最大的字母。<br><span class="hljs-built_in">min</span>(<span class="hljs-built_in">str</span>)<br> - 返回字符串 <span class="hljs-built_in">str</span> 中最小的字母。<br>string.partition(<span class="hljs-built_in">str</span>)<br> - 有点像 find()和 split()的结合体,从 <span class="hljs-built_in">str</span> 出现的第一个位置起,把 字 符 串 string 分 成 一 个 <span class="hljs-number">3</span> 元 素 的 元 组 (string_pre_str,<span class="hljs-built_in">str</span>,string_post_str),如果 string 中不包含<span class="hljs-built_in">str</span> 则 string_pre_str == string.<br>string.replace(str1, str2,  num=string.count(str1))<br> - 把 string 中的 str1 替换成 str2,如果 num 指定，则替换不超过 num 次.<br>string.rfind(<span class="hljs-built_in">str</span>, beg=<span class="hljs-number">0</span>,end=<span class="hljs-built_in">len</span>(string))<br> - 类似于 find() 函数，返回字符串最后一次出现的位置，如果没有匹配项则返回 -<span class="hljs-number">1</span>。<br>string.rindex( <span class="hljs-built_in">str</span>, beg=<span class="hljs-number">0</span>,end=<span class="hljs-built_in">len</span>(string))<br> - 类似于 index()，不过是返回最后一个匹配到的子字符串的索引号。<br>string.rjust(width)<br> - 返回一个原字符串右对齐,并使用空格填充至长度 width 的新字符串<br>string.rpartition(<span class="hljs-built_in">str</span>)<br> - 类似于 partition()函数,不过是从右边开始查找<br>string.rstrip()<br> - 删除 string 字符串末尾的空格<br>string.split(<span class="hljs-built_in">str</span>=<span class="hljs-string">&quot;&quot;</span>, num=string.count(<span class="hljs-built_in">str</span>))<br> - 以 <span class="hljs-built_in">str</span> 为分隔符切片 string，如果 num 有指定值，则仅分隔 num+<span class="hljs-number">1</span> 个子字符串<br>string.splitlines([keepends])<br> - 按照行(<span class="hljs-string">&#x27;\r&#x27;</span>, <span class="hljs-string">&#x27;\r\n&#x27;</span>, \n<span class="hljs-string">&#x27;)分隔，返回一个包含各行作为元素的列表，如果参数 keepends 为 False，不包含换行符，如果为 True，则保留换行符。</span><br><span class="hljs-string">string.startswith(obj, beg=0,end=len(string))</span><br><span class="hljs-string"> - 检查字符串是否是以 obj 开头，是则返回 True，否则返回 False。如果beg 和 end 指定值，则在指定范围内检查.</span><br><span class="hljs-string">string.strip([obj])</span><br><span class="hljs-string"> - 在 string 上执行 lstrip()和 rstrip()</span><br><span class="hljs-string">string.swapcase()</span><br><span class="hljs-string"> - 翻转 string 中的大小写</span><br><span class="hljs-string">string.title()</span><br><span class="hljs-string"> - 返回&quot;标题化&quot;的 string,就是说所有单词都是以大写开始，其余字母均为小写(见 istitle())</span><br><span class="hljs-string">string.translate(str, del=&quot;&quot;)</span><br><span class="hljs-string"> - 根据 str 给出的表(包含 256 个字符)转换 string 的字符，要过滤掉的字符放到 del 参数中</span><br><span class="hljs-string">string.upper()</span><br><span class="hljs-string"> - 转换 string 中的小写字母为大写</span><br><span class="hljs-string">string.zfill(width)</span><br><span class="hljs-string"> - 返回长度为 width 的字符串，原字符串 string 右对齐，前面填充0</span><br></code></pre></td></tr></table></figure><h2 id="常用内置函数（按函数名首字母排序）"><a href="#常用内置函数（按函数名首字母排序）" class="headerlink" title="常用内置函数（按函数名首字母排序）"></a>常用内置函数（按函数名首字母排序）</h2><ul><li><strong>all</strong><br>$\looparrowright$ 判断给定对象中的所有元素是否都为True，如果是返回True，否则返回False（元素为0&#x2F;NULL&#x2F;False），使用示例：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt;&gt;&gt; all([<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>]) = True<br><br>&gt;&gt;&gt; all([<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>]) = False<br><br>&gt;&gt;&gt; all((<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)) = False<br></code></pre></td></tr></table></figure><ul><li><strong>any</strong><br>$\looparrowright$ 判断给定对象中的所有元素是否都为False，如果是返回False，否则返回True，使用示例：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt;&gt;&gt; any([<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>]) = True<br><br>&gt;&gt;&gt; any([<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>]) = True<br><br>&gt;&gt;&gt; any((<span class="hljs-number">0</span>, &#x27;&#x27;, False)) = False<br></code></pre></td></tr></table></figure><ul><li><strong>cmp</strong><br>$\looparrowright$ 用于比较2个对象x，y，如果x &lt; y，返回-1, 如果x &#x3D;&#x3D; y返回0, 如果x &gt; y返回1。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt;&gt;&gt; cmp(1, 2) = -1<br></code></pre></td></tr></table></figure><ul><li><strong>enumerate</strong><br>$\looparrowright$ 将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在for循环当中，使用示例：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">设seasons = [<span class="hljs-string">&#x27;Spring&#x27;</span>, <span class="hljs-string">&#x27;Summer&#x27;</span>, <span class="hljs-string">&#x27;Fall&#x27;</span>, <span class="hljs-string">&#x27;Winter&#x27;</span>]<br><br>&gt;&gt;&gt; list(enumerate(seasons)) = [(0, <span class="hljs-string">&#x27;Spring&#x27;</span>), (1, <span class="hljs-string">&#x27;Summer&#x27;</span>), (2, <span class="hljs-string">&#x27;Fall&#x27;</span>), (3, <span class="hljs-string">&#x27;Winter&#x27;</span>)]<br><br>&gt;&gt;&gt; list(enumerate(seasons, start=1)) = [(1, <span class="hljs-string">&#x27;Spring&#x27;</span>), (2, <span class="hljs-string">&#x27;Summer&#x27;</span>), (3, <span class="hljs-string">&#x27;Fall&#x27;</span>), (4, <span class="hljs-string">&#x27;Winter&#x27;</span>)]      <span class="hljs-comment"># 下标从 1 开始</span><br></code></pre></td></tr></table></figure><ul><li><strong>hash</strong><br>$\looparrowright$ 获取对象（字符串或者数值等）的哈希值，使用示例：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt;&gt;&gt; <span class="hljs-built_in">hash</span>(<span class="hljs-string">&#x27;test&#x27;</span>) = 2314058222102390712                <span class="hljs-comment"># 字符串</span><br><br>&gt;&gt;&gt; <span class="hljs-built_in">hash</span>(1) = 1                                      <span class="hljs-comment"># 数字</span><br><br>&gt;&gt;&gt; <span class="hljs-built_in">hash</span>(str([1,2,3])) = 1335416675971793195         <span class="hljs-comment"># 集合</span><br><br>&gt;&gt;&gt; <span class="hljs-built_in">hash</span>(str(sorted(&#123;<span class="hljs-string">&#x27;1&#x27;</span>:1&#125;))) = 7666464346782421378 <span class="hljs-comment"># 字典</span><br></code></pre></td></tr></table></figure><ul><li><strong>map</strong><br>$\looparrowright$ 根据提供的函数对指定序列做映射。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt;&gt;&gt; def <span class="hljs-keyword">function</span>(x):<br><span class="hljs-built_in">return</span> x ** 2<br><br>&gt;&gt;&gt; map(square, [1,2,3,4,5]) = &lt;map object at 0x100d3d550&gt;              <span class="hljs-comment"># 返回迭代器</span><br><br>&gt;&gt;&gt; list(map(square, [1,2,3,4,5])) = [1, 4, 9, 16, 25]                  <span class="hljs-comment"># 使用 list() 转换为列表</span><br><br>&gt;&gt;&gt; list(map(lambda x: x ** 2, [1, 2, 3, 4, 5])) = [1, 4, 9, 16, 25]    <span class="hljs-comment"># 使用 lambda 匿名函数</span><br></code></pre></td></tr></table></figure><ul><li><strong>ord</strong><br>$\looparrowright$ chr()函数（对于8位的ASCII字符串）或unichr()函数（对于Unicode对象）的配对函数，它以一个字符（长度为1的字符串）作为参数，返回对应的ASCII数值，或者Unicode数值，如果所给的Unicode字符超出了你的Python定义范围，则会引发一个TypeError的异常。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt;&gt;&gt;ord(<span class="hljs-string">&#x27;a&#x27;</span>) = 97<br></code></pre></td></tr></table></figure><ul><li><strong>reverse</strong><br>$\looparrowright$ 反向列表元素，使用示例：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">设List = [<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>]<br>&gt;&gt;&gt; List.reverse()<br>&gt;&gt;&gt; List = [<span class="hljs-string">&#x27;d&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>]<br></code></pre></td></tr></table></figure><ul><li><strong>set</strong><br>$\looparrowright$ 创建输入对象的无序不重复元素集，set之间可以进行交集（&amp;）、差集（-）、并集（|）运算，使用示例：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt;&gt;&gt; <span class="hljs-built_in">set</span>(<span class="hljs-string">&#x27;abcddd&#x27;</span>) = [<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>]<br></code></pre></td></tr></table></figure><ul><li><strong>sorted</strong><br>$\looparrowright$ 对给定对象进行排序。（sorted(iterable, cmp&#x3D;None, key&#x3D;None, reverse&#x3D;False)）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">设a = [5,7,6,3,4,1,2]<br>&gt;&gt;&gt; sorted(a) = [5, 7, 6, 3, 4, 1, 2]<br><br>设L=[(<span class="hljs-string">&#x27;b&#x27;</span>,2),(<span class="hljs-string">&#x27;a&#x27;</span>,1),(<span class="hljs-string">&#x27;c&#x27;</span>,3),(<span class="hljs-string">&#x27;d&#x27;</span>,4)]<br>&gt;&gt;&gt; sorted(L, cmp=lambda x,y:cmp(x[1],y[1])) = [(<span class="hljs-string">&#x27;a&#x27;</span>, 1), (<span class="hljs-string">&#x27;b&#x27;</span>, 2), (<span class="hljs-string">&#x27;c&#x27;</span>, 3), (<span class="hljs-string">&#x27;d&#x27;</span>, 4)]   <span class="hljs-comment"># 利用cmp函数</span><br>&gt;&gt;&gt; sorted(L, key=lambda x:x[1]) = [(<span class="hljs-string">&#x27;a&#x27;</span>, 1), (<span class="hljs-string">&#x27;b&#x27;</span>, 2), (<span class="hljs-string">&#x27;c&#x27;</span>, 3), (<span class="hljs-string">&#x27;d&#x27;</span>, 4)]               <span class="hljs-comment"># 利用key</span><br><br>设students = [(<span class="hljs-string">&#x27;john&#x27;</span>, <span class="hljs-string">&#x27;A&#x27;</span>, 15), (<span class="hljs-string">&#x27;jane&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, 12), (<span class="hljs-string">&#x27;dave&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, 10)]<br>&gt;&gt;&gt; sorted(students, key=lambda s: s[2]) = [(<span class="hljs-string">&#x27;dave&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, 10), (<span class="hljs-string">&#x27;jane&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, 12), (<span class="hljs-string">&#x27;john&#x27;</span>, <span class="hljs-string">&#x27;A&#x27;</span>, 15)] <span class="hljs-comment"># 按年龄排序</span><br>&gt;&gt;&gt; sorted(students, key=lambda s: s[2], reverse=True) = [(<span class="hljs-string">&#x27;john&#x27;</span>, <span class="hljs-string">&#x27;A&#x27;</span>, 15), (<span class="hljs-string">&#x27;jane&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, 12), (<span class="hljs-string">&#x27;dave&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, 10)] <span class="hljs-comment"># 降序</span><br></code></pre></td></tr></table></figure><ul><li><strong>zip</strong><br>$\looparrowright$ zip([a, b, c, …])，将可迭代对象a，b，c，…打包成元组，返回元组组成的列表，使用示例：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">设a = [1,2,3], b = [4,5,6], c = [4,5,6,7,8]<br><br>&gt;&gt;&gt; zip(a,b) = [(1, 4), (2, 5), (3, 6)]<br><br>&gt;&gt;&gt; zip(a,c) = [(1, 4), (2, 5), (3, 6)]<br></code></pre></td></tr></table></figure><h1 id="C-x2F-C-篇"><a href="#C-x2F-C-篇" class="headerlink" title="C&#x2F;C++篇"></a><strong>C&#x2F;C++篇</strong></h1><h2 id="常用数据结构及其操作-1"><a href="#常用数据结构及其操作-1" class="headerlink" title="常用数据结构及其操作"></a>常用数据结构及其操作</h2><h3 id="向量（Vector）"><a href="#向量（Vector）" class="headerlink" title="向量（Vector）"></a>向量（Vector）</h3><p>$\looparrowright$ 动态大小数组的顺序容器，vector能够存放任意类型的动态数组。可以对序列中的任意元素进行快速直接访问，并利用指针进行操作。<br>$\Rightarrow$ 常用操作如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">（针对整型数据 vector&lt;int&gt; a）<br><br>（1）插入元素<br>a.push_back() —— 在数组的最后插入元素<br><br>（2）删除数据<br>a.pop_back() —— 删除数组的最后一个数据<br>a.erase —— 删除指针指向的数据项<br>a.clear —— 清空当前的vector<br><br>（3）查找数据<br>a.at(x) —— 获取位置x的元素<br><br>（4）获取向量属性<br>a.size() —— 当前向量的长度<br>a.max_size 得到vector最大长度<br>a.capacity —— 当前vector分配的大小<br>a.begin —— 得到数组头的指针<br>a.end —— 得到数组的最后一个单元+1的指针<br>a.front —— 得到数组头的引用<br>a.back —— 得到数组的最后一个单元的引用<br><br>（5）其它<br>a.rbegin —— 将vector反转后的开始指针返回(其实就是原来的end-1)<br>a.rend —— 将vector反转构的结束指针返回(其实就是原来的begin-1)<br>a.empty —— 判断vector是否为空<br>a.swap —— 与另一个vector交换数据<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 数据结构 </tag>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Swin Transformer 论文阅读笔记</title>
      <link href="/2022/08/10/033-SwinTransformer/"/>
      <url>/2022/08/10/033-SwinTransformer/</url>
      
        <content type="html"><![CDATA[<p><strong>论文来源</strong>：<br>【ICCV 2021 最佳论文】 Swin Transformer: Hierarchical Vision Transformer Using <strong>S</strong>hifted <strong>Win</strong>dows [<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html">Paper</a>] [<a href="https://github.com/microsoft/Swin-Transformer">Code</a>]</p><hr><h2 id="研究动机和思路"><a href="#研究动机和思路" class="headerlink" title="研究动机和思路"></a>研究动机和思路</h2><blockquote><p>“We seek to expand the applicability of Transformer such that it can serve as a general-purpose backbone for computer vision, as it does for NLP and as CNNs do in vision.” <strong>我们试图扩展 Transformer 的适用性，使其可以作为计算机视觉任务的通用主干，就像它在 NLP 领域和 CNN 在视觉邻域中所起到的效果。</strong></p></blockquote><blockquote><p><strong>图像信息建模</strong>：如下图所示，<code>ViT</code> 在对图像进行自注意力时，始终在原图 <code>1/16</code> 大小的 <code>patch</code> 上进行，实现图像信息的全局建模。受限于此，**<code>ViT</code> 无法从局部层面提取图像特征，以及无法实现图像多尺度特征的表示**（在密集预测型任务中尤为重要，如图像分割和目标检测）。</p></blockquote><blockquote><p><strong>时间复杂度</strong>：由于标准 <code>Transformer</code> 架构的自注意力计算过程是在 <code>token</code> 和 <code>token</code> 之间进行，因此复杂度极大程度上取决于 <code>token</code> 的数量。<br>“The global computation leads to quadratic complexity with respect to the number of tokens.” <strong>全局计算复杂度是关于 token 数量的二次复杂度。</strong></p></blockquote><p><img src="https://s2.loli.net/2022/08/08/Kn2H1Jwq8ucS4lF.png" alt="通过合并图像 patch 得到的多尺度特征图"></p><h3 id="如何兼顾局部和全局"><a href="#如何兼顾局部和全局" class="headerlink" title="如何兼顾局部和全局"></a>如何兼顾局部和全局</h3><blockquote><p><strong>Swin Transformer 的实现方式</strong>：<br>（1）<strong>预处理：</strong>将输入图像取成 <code>4×4 (pixel)</code> 的小 <code>patch</code>；<br>（2）<strong>Layer L：</strong>使用 <code>7×7 (patch)</code> 的 <code>window</code> 将 <code>patch</code> 块圈起来，在该 <code>window</code> 内对 <code>7×7=49</code> 个 <code>patch</code> 进行自注意力，实现<strong>图像局部特征的建模</strong>；<br>（3）<strong>Layer L+1：</strong>通过滑动 <code>window</code> 使得原本不在一个 <code>window</code> 内的 <code>patch</code> 处于一个 <code>window</code> 内，通过对其进行自注意力实现 <code>cross-window connections</code>。<br>（4）通过步长为 <code>s</code> 的 <code>patch merging</code> 将临近的 小 patch 合并成 <code>patch</code>， 使得整图分辨率下降 <code>1/s</code>，实现<strong>多尺度图像特征的提取</strong>；<br>（5）当图像尺寸减少至一定程度时，一个 <code>window</code> 能够对整图进行处理，实现<strong>图像全局特征的建模</strong>。</p></blockquote><table>    <tr>        <td><center><img src="https://s2.loli.net/2022/08/09/bBG6UK7ZeVEyw5M.png" width=350>shifted window approach</center></td>        <td><center><img src="https://s2.loli.net/2022/08/10/NBJtYxvLPe6bwIF.png" width=350>patch merging（序号仅用于理解）</center></td>    </tr></table><h2 id="Swin-Transformer"><a href="#Swin-Transformer" class="headerlink" title="Swin Transformer"></a>Swin Transformer</h2><h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><blockquote><p>以下结合网络架构图和代码推导一下<strong>（阅读文字时可将代码块折叠）</strong> 👇👇👇：</p></blockquote><p><strong>注</strong>：区别于 <code>ViT</code> 的一点在于，<code>Swin Transformer</code> 在进行分类任务时没用引入 <code>class token</code>，而是在最后使用 <code>global average pooling (GAP)</code> 得到类别预测的结果，目的在于使得 <code>Swin Transformer</code> 能够很好地兼容到视觉的其他任务中，如图像分割和目标检测。</p><p><img src="https://s2.loli.net/2022/08/10/b2JYwpi4okEzdsQ.png"></p><h3 id="Patch-Embedding"><a href="#Patch-Embedding" class="headerlink" title="Patch Embedding"></a>Patch Embedding</h3><blockquote><p><strong>Patch Partition:<strong>不妨设输入图像尺寸为 ${H}\times{W}\times{C_{in}}$，<code>Swin Transformer</code> 将 <code>patch size</code> 设置为 ${4}\times{4}$，则一个 <code>token</code> 的大小为 ${4}\times{4}\times{C_{in}}$，<code>token</code> 序列的长度为 $\frac{H}{4}\times\frac{W}{4}$；因此，整幅图像被转化成了维度为 $(\frac{H}{4}\times\frac{W}{4})\times({4}\times{4}\times{C_{in}})$ 的 <code>token</code> 序列，以 <code>224×224×3</code> 的输入图像为例，其产生的 <code>token</code> 序列的长度为 <code>(56×56)×(16×16×3)</code>；</strong>（以上过程通过 <code>4×4</code> 卷积层实现）</strong></p></blockquote><blockquote><p><strong>Linear Embedding:<strong>通过 <code>Patch Partition</code> 得到的 <code>token</code> 序列的长度对于 <code>Transformer</code> 模型而言是巨大的，因此需要减少其长度至设定的超参数 $C$；</strong>（以上过程通过 <code>Linear</code> 层实现）</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># Patch Partition + Linear Embedding</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PatchEmbed</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, img_size=<span class="hljs-number">224</span>, patch_size=<span class="hljs-number">4</span>, in_chans=<span class="hljs-number">3</span>, embed_dim=<span class="hljs-number">96</span>, norm_layer=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        img_size = to_2tuple(img_size)<br>        patch_size = to_2tuple(patch_size)<br>        patches_resolution = [img_size[<span class="hljs-number">0</span>] // patch_size[<span class="hljs-number">0</span>], img_size[<span class="hljs-number">1</span>] // patch_size[<span class="hljs-number">1</span>]]<br>        self.img_size = img_size<br>        self.patch_size = patch_size<br>        self.patches_resolution = patches_resolution<br>        self.num_patches = patches_resolution[<span class="hljs-number">0</span>] * patches_resolution[<span class="hljs-number">1</span>]<br><br>        self.in_chans = in_chans<br>        self.embed_dim = embed_dim<br><br>        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)<br>        <span class="hljs-keyword">if</span> norm_layer <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            self.norm = norm_layer(embed_dim)<br>        <span class="hljs-keyword">else</span>:<br>            self.norm = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        B, C, H, W = x.shape<br>        <span class="hljs-keyword">assert</span> H == self.img_size[<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> W == self.img_size[<span class="hljs-number">1</span>], \<br>            <span class="hljs-string">f&quot;Input image size (<span class="hljs-subst">&#123;H&#125;</span>*<span class="hljs-subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="hljs-subst">&#123;self.img_size[<span class="hljs-number">0</span>]&#125;</span>*<span class="hljs-subst">&#123;self.img_size[<span class="hljs-number">1</span>]&#125;</span>).&quot;</span><br>        x = self.proj(x).flatten(<span class="hljs-number">2</span>).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># B Ph*Pw C</span><br>        <span class="hljs-keyword">if</span> self.norm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            x = self.norm(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h3 id="Hierarchical-Stage"><a href="#Hierarchical-Stage" class="headerlink" title="Hierarchical Stage"></a>Hierarchical Stage</h3><blockquote><p><strong>以 <code>stage2</code> 为例（<code>stage3、4</code> 同理），推导一下网络：</strong><br>（1）<code>Layer Input</code>：输入特征图维度为 $\frac{H}{4}\times\frac{W}{4}\times{C}$；<br>（2）<code>Patch Merging</code>：经上图右侧所示过程，合并 patch 之后的特征图尺寸减少 <code>1/2</code> 倍，通道数增加 <code>4</code> 倍，即经 <code>patch merging</code> 之后的输出特征图维度为 $\frac{H}{8}\times\frac{W}{8}\times{4C}$；<br>（3）<code>Channel Reduction</code>：为了保持与卷积神经网络拥有相同的层级表示，进一步通过 <code>Linear</code> 层或 <code>1×1</code> 卷积层（二者作用一致，原文代码用的 <code>Linear</code> 层）将通道数降为 <code>2C</code>，使得最终输出特征图维度为 $\frac{H}{8}\times\frac{W}{8}\times{2C}$；<strong>（注：本过程为 <code>Patch Merging</code> 中的步骤）</strong><br>（3）Swin Transformer Block：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># Patch Merging</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PatchMerging</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_resolution, dim, norm_layer=nn.LayerNorm</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.input_resolution = input_resolution<br>        self.dim = dim<br>        self.reduction = nn.Linear(<span class="hljs-number">4</span> * dim, <span class="hljs-number">2</span> * dim, bias=<span class="hljs-literal">False</span>)<br>        self.norm = norm_layer(<span class="hljs-number">4</span> * dim)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        H, W = self.input_resolution<br>        B, L, C = x.shape<br>        <span class="hljs-keyword">assert</span> L == H * W, <span class="hljs-string">&quot;input feature has wrong size&quot;</span><br>        <span class="hljs-keyword">assert</span> H % <span class="hljs-number">2</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> W % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>, <span class="hljs-string">f&quot;x size (<span class="hljs-subst">&#123;H&#125;</span>*<span class="hljs-subst">&#123;W&#125;</span>) are not even.&quot;</span><br><br>        x = x.view(B, H, W, C)<br><br>        x0 = x[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>, :]  <span class="hljs-comment"># B H/2 W/2 C</span><br>        x1 = x[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>, :]  <span class="hljs-comment"># B H/2 W/2 C</span><br>        x2 = x[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>, :]  <span class="hljs-comment"># B H/2 W/2 C</span><br>        x3 = x[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>, :]  <span class="hljs-comment"># B H/2 W/2 C</span><br>        x = torch.cat([x0, x1, x2, x3], -<span class="hljs-number">1</span>)  <span class="hljs-comment"># B H/2 W/2 4*C</span><br>        x = x.view(B, -<span class="hljs-number">1</span>, <span class="hljs-number">4</span> * C)  <span class="hljs-comment"># B H/2*W/2 4*C</span><br><br>        x = self.norm(x)<br>        x = self.reduction(x)<br><br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h3 id="Swin-Transformer-Block"><a href="#Swin-Transformer-Block" class="headerlink" title="Swin Transformer Block"></a>Swin Transformer Block</h3><p><img src="https://s2.loli.net/2022/08/10/uxlBGh87RCIO3be.png"></p><h4 id="Shifted-Window-based-Multi-head-Self-Attention"><a href="#Shifted-Window-based-Multi-head-Self-Attention" class="headerlink" title="Shifted Window based Multi-head Self-Attention"></a>Shifted Window based Multi-head Self-Attention</h4><blockquote><p>前面提到，Swin Transformer 通过设置 window，对处于 window 内的 patch 做自注意力。以 stage1 为例，56×56 的 patch 数量，设置 7×7 的 window size，对整图运算则需要的 window 数量为 (56&#x2F;7)×(56&#x2F;7)&#x3D;8×8&#x3D;64。</p></blockquote><blockquote><p><strong>计算复杂度分析</strong><br>（1）标准 <code>Multi-head Self-attention</code>：<br>$$3HWC^{2}+(HW)^{2}C+(HW)^{2}C+HWC^{2}&#x3D;4HWC^{2}+2(HW)^{2}C, \tag{1}$$<br>（2）<code>Swin Transformer</code> 中的 <code>Self-attention</code>：<br>$$(\frac{H}{M}\times\frac{W}{M})\times(4MMC^{2}+2(MM)^{2}C)&#x3D;4HWC^{2}+2M^{2}HWC, \tag{2}$$<br>将 $(2)$ 式减 $(1)$ 式得 $(HW-M^{2})\times(2HWC)$，确实有 <strong>一定程度</strong> 的下降。</p></blockquote><p><img src="https://s2.loli.net/2022/08/10/mxWi9dtpV7UehPN.png"></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 图像分类 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像投影网络（Image Projection Network, IPN）系列论文阅读笔记</title>
      <link href="/2022/08/08/034-IPN_Series/"/>
      <url>/2022/08/08/034-IPN_Series/</url>
      
        <content type="html"><![CDATA[<p>主要是以下两篇论文：<br>IPN（<strong>TMI 2020</strong>）：Image Projection Network: 3D to 2D Image Segmentation in OCTA Images<br>IPN V2（<strong>arXiv 2020</strong>）：IPN-V2 and OCTA-500: Methodology and Dataset for Retinal Image Segmentation</p><!-- PAENet（**BIBM 2021**）：PAENet: A Progressive Attention-Enhanced Network for 3D to 2D Retinal Vessel Segmentation --><hr><h2 id="图像投影网络的设计来源"><a href="#图像投影网络的设计来源" class="headerlink" title="图像投影网络的设计来源"></a>图像投影网络的设计来源</h2><h3 id="眼科临床"><a href="#眼科临床" class="headerlink" title="眼科临床"></a>眼科临床</h3><blockquote><p>“Comparing to color fundus imaging technology, OCT can acquire more detailed information about retinal structures and thus becomes a leading modality in the clinic observation of retinopathy.” <strong>与彩色眼底成像技术相比，OCT可以获取更详细的视网膜结构信息，成为视网膜病变临床观察的主要方式。</strong></p></blockquote><p><img src="https://s2.loli.net/2022/08/08/9pKf7yDr5ogiFa3.png"></p><h3 id="诊疗指标"><a href="#诊疗指标" class="headerlink" title="诊疗指标"></a>诊疗指标</h3><blockquote><p>“Both OCT and OCTA can provide 3D data, but most retinal indicators, such as the vessel density and the FAZ area, are quantified on the projection maps rather than 3D space.” <strong>OCT 和 OCTA 都可以提供 3D 数据，但大多数视网膜指标，例如血管密度和 FAZ 面积，都是在投影图（上图 e）上量化的，而不是在 3D 空间上。</strong></p></blockquote><h3 id="数据标注问题"><a href="#数据标注问题" class="headerlink" title="数据标注问题"></a>数据标注问题</h3><blockquote><p>对于医生而言，在 OCT 或 OCTA 的 3D 数据上直接进行标注是困难的，相反在 2D 的投影图上标注则是相对简单和高效的。</p></blockquote><h3 id="已有深度学习方法的的局限性"><a href="#已有深度学习方法的的局限性" class="headerlink" title="已有深度学习方法的的局限性"></a>已有深度学习方法的的局限性</h3><p><img src="https://s2.loli.net/2022/08/08/DmW7CRPkyZ32vtU.png"></p><blockquote><p><strong>主流端到端方法</strong>：<br>（1）2D to category.<br>（2）2D to 2D semantic segmentation.<br>（3）3D to 3D semantic segmentation.<br>对于（3）而言，应用到 OCT 和 OCTA 图像上是几乎不可能的：<br>“Alternatively, they need 3D pixel-to-pixel labels, which are labor-intensive and difficult to be obtained.” <strong>或者，他们需要 3D 像素到像素的标签，这是劳动密集型且难以获得的。</strong></p></blockquote><h2 id="Image-Projection-Network-IPN"><a href="#Image-Projection-Network-IPN" class="headerlink" title="Image Projection Network (IPN)"></a>Image Projection Network (IPN)</h2><h3 id="2D-to-1D-IPN"><a href="#2D-to-1D-IPN" class="headerlink" title="2D-to-1D IPN"></a>2D-to-1D IPN</h3><blockquote><p><strong>（1）构造方式：</strong><br>“We use the framework of the classical VGG model for reference, remove all the full connection layers, and change the original pooling layer to the unidirectional pooling layer.” <strong>我们借鉴经典VGG模型的框架，去掉所有的全连接层，将原来的池化层改为单向池化层（下图右）。</strong><br><strong>（2）实现方式：</strong><br>将 <code>3×400×640</code> 的横截面图输入上述构造的 <code>IPN</code> 中，输出得到一维的尺寸为 <code>1×400×1</code> 的向量；通过将每个病例的 <code>400</code> 张横截面图像输入网络得到的输出向量拼接，即可得到 <code>1×400×400</code> 的预测图像。<br><strong>（3）局限性：</strong><br>最终的分割结果是通过拼接得到的，由于包含很多锯齿状的边缘，空间连续性较差。</p></blockquote><p><img src="https://s2.loli.net/2022/08/08/AElenYcpoRbU8gj.png"></p><h3 id="3D-to-2D-IPN"><a href="#3D-to-2D-IPN" class="headerlink" title="3D-to-2D IPN"></a>3D-to-2D IPN</h3><blockquote><p>“IPN can summarize the effective features in 3D data along the projection direction and output the segmentation results on a 2D plane, to realize the semantic segmentation from 3D to 2D.” <strong>IPN 可以沿投影方向总结 3D 数据中的有效特征并在 2D 平面上输出分割结果，实现从 3D 到 2D 的语义分割。</strong></p></blockquote><p><img src="https://s2.loli.net/2022/08/08/jJFpZ8Wo1TECk3P.png"></p><blockquote><p><strong>与 2D-to-1D IPN 的差异</strong>：使用 <strong>3D 卷积</strong> 而不是 2D 卷积，并且<strong>单向池化从 2D 扩展到 3D</strong>，但仍然只作用在投影方向。 通过这种变化，IPN 可以输入 3 维图像并输出 2 维截面图。以下介绍 3D-to-2D IPN 的关键组成部分。</p></blockquote><h4 id="投影学习模块"><a href="#投影学习模块" class="headerlink" title="投影学习模块"></a>投影学习模块</h4><blockquote><p><strong>Projection Learning Module, PLM</strong><br>（1）组成部分：三个 卷积核尺寸为 <code>(3×3×3)</code> 的 3D 卷积层和一个单向池化层。<br>（2）单个 <code>PLM</code> 的输入输出尺寸变化：$[H \times L \times W] \rightarrow [\frac{H}{k} \times L \times W]$，其中 <code>k</code> 为单向池化层的 <code>pooling size</code>。</p></blockquote><p><img src="https://s2.loli.net/2022/08/08/A1FIjEwidgaOc8L.png"></p><h4 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h4><table>    <tr>        <td>Layer</td><td>Channel Number</td><td>PLM Parameter</td><td>Output Size</td>   </tr>    <tr>          <td>Input</td><td>2</td><td>-</td><td>640×100×100</td>    </tr>    <tr>        <td>PLM1</td><td>32</td><td>5</td><td>128×100×100</td>    </tr>    <tr>        <td>PLM2</td><td>64</td><td>4</td><td>32×100×100</td>    </tr>    <tr>        <td>PLM3</td><td>128</td><td>4</td><td>8×100×100</td>    </tr>    <tr>        <td>PLM4</td><td>256</td><td>4</td><td>2×100×100</td>    </tr>    <tr>        <td>PLM5</td><td>512</td><td>2</td><td>1×100×100</td>    </tr>    <tr>        <td>Conv6</td><td>256</td><td>-</td><td>1×100×100</td>    </tr>    <tr>        <td>Conv7</td><td>128</td><td>-</td><td>1×100×100</td>    </tr>    <tr>        <td>Conv8</td><td>2</td><td>-</td><td>1×100×100</td>    </tr>    <tr>        <td>Softmax</td><td>2</td><td>-</td><td>1×100×100</td>    </tr></table><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><blockquote><p>以下实现时，为了适应后面 IPN V2 论文中的输入，做了一些改动：<br>（1）输入 Patch 大小为 640×100×100；<br>（2）减少一个 pooling size &#x3D; 4 的 PLM 层，以适应输出；<br>（3）减少最后的卷积层至 1 层；<br>（4）将 <code>Softmax</code> 替换成了 <code>Sigmoid</code>，训练过程中的损失函数使用 <code>BCELoss()</code>，而非原文中提到的 <code>CELoss()</code>，本质是一样的。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">IPN</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels=<span class="hljs-number">2</span>, channels=<span class="hljs-number">32</span>, n_classes=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>(IPN, self).__init__()<br>        self.in_channels = in_channels<br>        self.channels=channels<br>        self.n_classes = n_classes<br>        self.<span class="hljs-built_in">input</span> = InConv3d(in_channels, channels)<br>        self.PLM1 = PLM(<span class="hljs-number">5</span>, channels)<br>        self.PLM2 = PLM(<span class="hljs-number">4</span>, channels*<span class="hljs-number">2</span>)<br>        self.PLM3 = PLM(<span class="hljs-number">4</span>, channels*<span class="hljs-number">4</span>)<br>        self.PLM4 = PLM(<span class="hljs-number">2</span>, channels*<span class="hljs-number">8</span>) <br>        self.output = OutConv2d(channels*<span class="hljs-number">16</span>, n_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.<span class="hljs-built_in">input</span>(x)<br>        x = self.PLM1(x)<br>        x = self.PLM2(x)<br>        x = self.PLM3(x)<br>        feature = self.PLM4(x)<br>        feature = torch.squeeze(feature, <span class="hljs-number">2</span>)<br>        logits = self.output(feature)<br>        <span class="hljs-keyword">return</span> torch.sigmoid(logits), feature<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PLM</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, poolingsize, channels</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.plm = nn.Sequential(<br>            nn.MaxPool3d(kernel_size=[poolingsize, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),<br>            nn.Conv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv3d(channels, channels*<span class="hljs-number">2</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> self.plm(x)<br></code></pre></td></tr></table></figure><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><h4 id="RV-Segmentation"><a href="#RV-Segmentation" class="headerlink" title="RV Segmentation"></a>RV Segmentation</h4><blockquote><p><strong>输入数据：</strong>3D 的 OCT 和 OCTA Volume，原始数据大小为 640px × 400px × 400px；<br><strong>输入大小：</strong>640px × 100px × 100px 的采样数据；<br><strong>采样方式：</strong>由于投影图中血管分布均匀，因此采用 <strong>随机采样</strong>。</p></blockquote><h4 id="FAZ-Segmentation"><a href="#FAZ-Segmentation" class="headerlink" title="FAZ Segmentation"></a>FAZ Segmentation</h4><blockquote><p><strong>输入数据：</strong>除了 3D 的 OCT 和 OCTA Volume外，还考虑到 FAZ 位于图像的中心区域，添加了下图所示的距离图（distance map）作为第三通道；<br><strong>输入大小：</strong>640px × 100px × 100px 的采样数据；<br><strong>采样方式：</strong>由于投影图中 FAZ 仅占一小部分，导致 FAZ 的分割中存在正负样本的不平衡问题，为了增加正样本的比例，采用以投影中心为中心的 <strong>正态分布采样</strong>，以增加中心位置被选为训练数据的概率。</p></blockquote><p><img src="https://s2.loli.net/2022/08/08/pV5X7jR4guvokdy.png"></p><h4 id="参数配置"><a href="#参数配置" class="headerlink" title="参数配置"></a>参数配置</h4><table>    <tr>        <td>Optimizer</td><td>Adam Stochastic Optimization</td>    </tr>    <tr>        <td>GPU</td><td>1 NVIDIA GeForce GTX 1080Ti</td>    </tr>      <tr>        <td>Loss Function</td><td>Cross-entropy</td>    </tr>    <tr>        <td>Batch Size</td><td>3</td>    </tr>    <tr>        <td>Max Iteration Number</td><td>20000</td>    </tr>    <tr>        <td>Initial Learning Rate</td><td>10^(-4)</td>    </tr></table><h3 id="对比实验"><a href="#对比实验" class="headerlink" title="对比实验"></a>对比实验</h3><h4 id="3D-to-2D-IPN-vs-2D-to-1D-IPN"><a href="#3D-to-2D-IPN-vs-2D-to-1D-IPN" class="headerlink" title="3D-to-2D IPN vs. 2D-to-1D IPN"></a>3D-to-2D IPN vs. 2D-to-1D IPN</h4><blockquote><p>2D-to-1D IPN 在训练过程中缺少空间信息，导致预测结果中心存在明显的锯齿状。</p></blockquote><p><img src="https://s2.loli.net/2022/08/08/cJDWQtAq7dvi9lG.png"><br><img src="https://s2.loli.net/2022/08/08/4BIcpjNluJC7oQM.png" alt="FAZ分割结果使用 2D-to-1D IPN（左）和 3D-to-2D IPN（右）。红线代表真值图，黄线和绿线分别代表 2D-to-1D IPN 和 3D-to-2D IPN 的结果。"></p><h4 id="Multi-Channel-vs-Single-Channel"><a href="#Multi-Channel-vs-Single-Channel" class="headerlink" title="Multi-Channel vs. Single-Channel"></a>Multi-Channel vs. Single-Channel</h4><blockquote><p>（1）OCTA &gt; OCT；<br>（2）多模态数据 &gt; 单模态数据；<br>（3）<strong>Distance Map 的引入</strong> 增大了中心区域的权重，使得 FAZ 的分割结果上涨了 5 个百分点。（”Distance map plays an important role, which is related to the location specificity of FAZ. Without the distance map, the network mistakenly assumes that the areas with weak blood flow signals belong to FAZ, such as the weakening of local signals due to turbid refractive media and non-perfusion zone.” 距离图起着重要的作用，这与 FAZ 的位置特异性有关。 在没有距离图的情况下，网络错误地认为血流信号较弱的区域属于FAZ，例如由于屈光介质混浊和非灌注区导致局部信号减弱。）</p></blockquote><p><img src="https://s2.loli.net/2022/08/08/TnCejam5WVsdq4f.png"><br><img src="https://s2.loli.net/2022/08/08/efEOp29adl4HviM.png" alt="FAZ 分割结果的三个示例。 (a)-(c) 中的黄色区域代表真值图。 (d-f) 中的彩色线表示不同输入的 IPN 的结果：OCT（红线）、OCTA（绿线）、OCT+OCTA（蓝线）、OCT+OCTA+Distance map（黄线）"></p><h4 id="IPN-vs-Others"><a href="#IPN-vs-Others" class="headerlink" title="IPN vs. Others"></a>IPN vs. Others</h4><blockquote><p>（1）IPN 优于任何 2D 方法<br>（2）IPN 架构优于 U-Net 架构（max-pooling 和 skip-connection）</p></blockquote><p><img src="https://s2.loli.net/2022/08/08/ifJ4FwtGCPsylvc.png" alt="PRO + * 表示使用投影图进行分割的 2D 方法"></p><h2 id="IPN-V2"><a href="#IPN-V2" class="headerlink" title="IPN V2"></a>IPN V2</h2><h3 id="Plane-Perceptron"><a href="#Plane-Perceptron" class="headerlink" title="Plane Perceptron"></a>Plane Perceptron</h3><blockquote><p><strong>设计 V2 版本的出发点：</strong>IPN 的主要功能是将 3D 体积信息汇总成 2D 平面。 由于缺乏水平方向的下采样，它缺乏二维平面中的高级语义信息。<br><strong>解决方法：</strong>使用 U-Net 作为平面感知器（Plane Perceptron），并将其连接在 IPN 后面。 此外，将 IPN 输出的 2D 特征和第一个 PLM 输出的 3D 特征连接起来，以防止梯度消失并加快收敛进程。</p></blockquote><p><img src="https://s2.loli.net/2022/08/08/YMBeCLrU6fWsO74.png"></p><h3 id="Global-Retraining"><a href="#Global-Retraining" class="headerlink" title="Global Retraining"></a>Global Retraining</h3><blockquote><p>简单来说，就是<strong>把 IPN V2 的预测结果（Patch）拼接到一起，然后再用 U-Net 训练一遍</strong>。 并且，在图像拼接的时候采用 <strong>重叠</strong> 的方法以减少拼接过程带来的“棋盘效应”。</p></blockquote><h3 id="训练细节-1"><a href="#训练细节-1" class="headerlink" title="训练细节"></a>训练细节</h3><blockquote><p><strong>IPN V2 的训练细节较 IPN 而言有较大变化：</strong><br>（1）<strong>数据集扩充</strong>：从 316 例 6mm×6mm，扩充至 300 例 6mm×6mm 和 200 例 3mm×3mm 的 OCT 和 OCTA Volume；原始数据大小分别为 640px × 400px × 400px 和 640px × 304px × 304px；<br>（2）<strong>输入大小：</strong>分别为 <strong>160px</strong> × 100px × 100px 和 <strong>160px</strong> × 76px × 76px 的采样数据；<br>（3）<strong>参数调整：</strong>Maximum iteration number - 30000，Batch size - 1;<br>（4）<strong>Global Training 参数：</strong>Maximum iteration number - 5000，Batch size - 2。</p></blockquote><h3 id="对比实验-1"><a href="#对比实验-1" class="headerlink" title="对比实验"></a>对比实验</h3><h4 id="RV-Segmentation-1"><a href="#RV-Segmentation-1" class="headerlink" title="RV Segmentation"></a>RV Segmentation</h4><p><img src="https://s2.loli.net/2022/08/08/7PhX3A1HYQ6Oq52.png"></p><h4 id="FAZ-Segmentation-1"><a href="#FAZ-Segmentation-1" class="headerlink" title="FAZ Segmentation"></a>FAZ Segmentation</h4><p><img src="https://s2.loli.net/2022/08/08/dBsiqL3zKOmQgVT.png"></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> 图像分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>医学图像中的血管分割</title>
      <link href="/2022/07/05/021-VesselSegmentation/"/>
      <url>/2022/07/05/021-VesselSegmentation/</url>
      
        <content type="html"><![CDATA[<p>从去年11月的肺血管分割开始，到现在的眼底血管，一起整理一下！</p><hr><h1 id="肺部血管-肺部血管分割"><a href="#肺部血管-肺部血管分割" class="headerlink" title="肺部血管 [肺部血管分割]"></a><strong>肺部血管</strong> [<a href="https://paperswithcode.com/task/pulmorary-vessel-segmentation">肺部血管分割</a>]</h1><ul><li>这里paperwithcode网站的“肺”的英文都弄错了，肺-pulmonary</li></ul><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ul><li>VESSEL12 <a href="https://paperswithcode.com/dataset/vessel12">https://paperswithcode.com/dataset/vessel12</a></li><li>ISICDM2020挑战赛 <a href="https://svyj.github.io/2020/11/04/ISICDM2020/">https://svyj.github.io/2020/11/04/ISICDM2020/</a></li></ul><h2 id="SOTA模型"><a href="#SOTA模型" class="headerlink" title="SOTA模型"></a>SOTA模型</h2><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><h1 id="眼底OCT血管-眼底血管分割"><a href="#眼底OCT血管-眼底血管分割" class="headerlink" title="眼底OCT血管 [眼底血管分割]"></a><strong>眼底OCT血管</strong> [<a href="https://paperswithcode.com/task/retinal-vessel-segmentation">眼底血管分割</a>]</h1><h2 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h2><ul><li>DRIVE <a href="https://paperswithcode.com/dataset/drive">https://paperswithcode.com/dataset/drive</a></li><li>CHASE <a href="https://paperswithcode.com/dataset/chase-db1">https://paperswithcode.com/dataset/chase-db1</a></li><li>STARE <a href="https://paperswithcode.com/dataset/stare">https://paperswithcode.com/dataset/stare</a></li><li>HRF <a href="https://paperswithcode.com/dataset/hrf">https://paperswithcode.com/dataset/hrf</a> (这个数据集做的太少了，可以忽略不计)</li></ul><h2 id="SOTA模型-1"><a href="#SOTA模型-1" class="headerlink" title="SOTA模型"></a>SOTA模型</h2><ul><li><p>ResU-Net<br><img src="https://i.loli.net/2021/07/05/s9N3gaDQdAjorvn.png" alt="Res U-Net"></p></li><li><p>R2U-Net<br><img src="https://i.loli.net/2021/07/05/k2hqyaKDzO6bcrw.png" alt="R2U-Net"></p></li><li><p>DU-Net<br><img src="https://i.loli.net/2021/07/05/csoa37m9OvrCUL8.png" alt="DU-Net"></p></li><li><p>LadderNet<br><img src="https://i.loli.net/2021/07/05/rfcjeJlbQ17H6ID.png" alt="LadderNet"></p></li><li><p>IterNet<br><img src="https://i.loli.net/2021/07/05/IebxlPoaSWdN5iO.png" alt="IterNet"></p></li><li><p>BCDU-Net<br><img src="https://i.loli.net/2021/07/05/oOeSpf8nyWkB5YM.png" alt="BCDU-Net"></p></li><li><p>CENet<br><img src="https://i.loli.net/2021/07/05/vCbqJnw7QBjhXLK.png" alt="CENet"></p></li><li><p>ETNet<br><img src="https://i.loli.net/2021/07/05/ELcW31BCK5guOYw.png" alt="ETNet"></p></li><li><p>SAU-Net<br><img src="https://i.loli.net/2021/07/05/J24BuZtFx7SafWs.png" alt="SAU-Net"></p></li></ul><h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><h1 id="眼底OCTA血管"><a href="#眼底OCTA血管" class="headerlink" title="眼底OCTA血管"></a><strong>眼底OCTA血管</strong></h1><h2 id="数据集-2"><a href="#数据集-2" class="headerlink" title="数据集"></a>数据集</h2><h2 id="SOTA模型-2"><a href="#SOTA模型-2" class="headerlink" title="SOTA模型"></a>SOTA模型</h2><h2 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h2>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> 图像分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer系列的简单整理（挖坑）</title>
      <link href="/2022/07/05/022-Transformers/"/>
      <url>/2022/07/05/022-Transformers/</url>
      
        <content type="html"><![CDATA[<h1 id="什么是Transformer"><a href="#什么是Transformer" class="headerlink" title="什么是Transformer?"></a><strong>什么是Transformer?</strong></h1><h2 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h2><h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h2><h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h2><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a><strong>Transformer</strong></h2><p>Paper: <a href="https://arxiv.org/abs/1706.03762">Attention is not all you need: pure attention loses rank doubly exponentially with depth</a>[NeurIPS 2017] </p><h1 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a><strong>Vision Transformer</strong></h1><h2 id="ViT"><a href="#ViT" class="headerlink" title="ViT"></a><strong>ViT</strong></h2><p>Paper: <a href="">An Image Is Worth 16X16 Words: Transformers for Image Recognition at Scale</a>[ICLR 2021]<br><img src="https://i.loli.net/2021/07/20/zmd57MLaIEw4upN.png" alt="ViT"></p><h2 id="DETR"><a href="#DETR" class="headerlink" title="DETR"></a>DETR</h2><h2 id="CrossViT"><a href="#CrossViT" class="headerlink" title="CrossViT"></a>CrossViT</h2><p>Paper: <a href="">CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classificatio</a><br><img src="https://i.loli.net/2021/07/20/yKWVM2ei4RPkQlC.png" alt="CrossViT"></p><h2 id="MViT"><a href="#MViT" class="headerlink" title="MViT"></a>MViT</h2><p>Paper: <a href="">Multiscale Vision Transformer</a><br>Codes: <a href="https://github.com/facebookresearch/SlowFast/tree/master/projects/mvit">https://github.com/facebookresearch/SlowFast/tree/master/projects/mvit</a></p><h2 id="Swin-Transformer"><a href="#Swin-Transformer" class="headerlink" title="Swin-Transformer"></a>Swin-Transformer</h2><p>Paper: <a href="">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></p><h2 id="SETR"><a href="#SETR" class="headerlink" title="SETR"></a><strong>SETR</strong></h2><p>Paper: <a href="https://arxiv.org/abs/2105.05633">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</a>[CVPR 2021]<br>Codes: <a href="https://github.com/fudan-zvg/SETR">SETR</a><br><img src="https://i.loli.net/2021/07/20/CAvJluR4LfNS18e.png" alt="SETR"></p><h2 id="Segmenter"><a href="#Segmenter" class="headerlink" title="Segmenter"></a>Segmenter</h2><p>Paper: <a href="https://arxiv.org/abs/2105.05633">Segmenter: Transformer for Semantic Segmentation</a><br>Codes: <a href="https://github.com/rstrudel/segmenter">Segmenter</a></p><h1 id="医学图像中的Transformer"><a href="#医学图像中的Transformer" class="headerlink" title="医学图像中的Transformer"></a><strong>医学图像中的Transformer</strong></h1><h2 id="TransU-Net"><a href="#TransU-Net" class="headerlink" title="TransU-Net"></a>TransU-Net</h2><p>Paper: <a href="">TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</a><br><img src="https://i.loli.net/2021/07/20/mKI2VGrY7EHATUq.png" alt="TransU-Net(R50+ViT)"></p><h2 id="U-Net-Transformer"><a href="#U-Net-Transformer" class="headerlink" title="U-Net Transformer"></a>U-Net Transformer</h2><p>Paper: <a href="">U-Net Transformer: Self and Cross Attention for Medical Image Segmentation</a><br><img src="https://i.loli.net/2021/07/20/RJyVszCqudIGotW.png" alt="U-Net Transformer"></p><h2 id="Medical-Transformer-MedT"><a href="#Medical-Transformer-MedT" class="headerlink" title="Medical Transformer(MedT)"></a><strong>Medical Transformer(MedT)</strong></h2><p>Paper: <a href="">Medical Transformer: Gated Axial-Attention for Medical Image Segmentation</a>[MICCAI 2021]<br><img src="https://i.loli.net/2021/07/20/pHGLezSUiCWFmBa.png" alt="Medical Transformer(LoGo)"></p><h2 id="TransBTS"><a href="#TransBTS" class="headerlink" title="TransBTS"></a>TransBTS</h2><h2 id="UNETR"><a href="#UNETR" class="headerlink" title="UNETR"></a>UNETR</h2><h2 id="TransFuse"><a href="#TransFuse" class="headerlink" title="TransFuse"></a>TransFuse</h2><p>Paper: <a href="">TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation</a><br><img src="https://i.loli.net/2021/07/20/XdqYRoj6vLWb7AT.png" alt="TransFuse"></p><h2 id="SegTran"><a href="#SegTran" class="headerlink" title="SegTran"></a><strong>SegTran</strong></h2><p>Paper: <a href="https://arxiv.org/abs/2105.09511">Medical Image Segmentation Using Squeeze-and-Expansion Transformers</a>[IJCAI 2021]<br>Codes: <a href="https://github.com/askerlee/segtran">segtran</a></p><h2 id="Trans2Seg"><a href="#Trans2Seg" class="headerlink" title="Trans2Seg"></a>Trans2Seg</h2><h2 id="Swin-Unet"><a href="#Swin-Unet" class="headerlink" title="Swin-Unet"></a>Swin-Unet</h2><p>Paper: <a href="https://arxiv.org/abs/2105.05537">Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation</a><br>Codes: <a href="https://github.com/HuCaoFighting/Swin-Unet">Swin-Unet</a><br><img src="https://i.loli.net/2021/07/20/UJPC3YxTQVnLhFB.png" alt="Swin-Unet"></p><h2 id="DS-TransUNet"><a href="#DS-TransUNet" class="headerlink" title="DS-TransUNet"></a>DS-TransUNet</h2><h2 id="UTNet"><a href="#UTNet" class="headerlink" title="UTNet"></a><strong>UTNet</strong></h2><p>[MICCAI 2021]</p><h2 id="PNS-Net"><a href="#PNS-Net" class="headerlink" title="PNS-Net"></a><strong>PNS-Net</strong></h2><p>[MICCAI 2021]</p><h1 id="代码实现示例"><a href="#代码实现示例" class="headerlink" title="代码实现示例"></a><strong>代码实现示例</strong></h1><ul><li>参考资料<br>[1] <a href="https://arxiv.org/abs/2012.12556">A Survey on Visual Transformer</a><br>[2] <a href="https://zhuanlan.zhihu.com/p/339181742">https://zhuanlan.zhihu.com/p/339181742</a><br>[3] <a href="https://arxiv.org/abs/2101.01169">Transformers in Vision: A Survey</a><br>[4] <a href="https://zhuanlan.zhihu.com/p/390784659">https://zhuanlan.zhihu.com/p/390784659</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> 图像分割 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch使用Visdom可视化（安装和使用教程）</title>
      <link href="/2022/07/05/024-Visdom/"/>
      <url>/2022/07/05/024-Visdom/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Visdom安装"><a href="#1-Visdom安装" class="headerlink" title="1 Visdom安装"></a>1 Visdom安装</h1><ul><li>在代码环境下执行以下命令即可安装。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install visdom<br></code></pre></td></tr></table></figure><h1 id="2-Visdom可视化训练过程"><a href="#2-Visdom可视化训练过程" class="headerlink" title="2 Visdom可视化训练过程"></a>2 Visdom可视化训练过程</h1><h2 id="1-Visdom可视化方法的实现"><a href="#1-Visdom可视化方法的实现" class="headerlink" title="(1) Visdom可视化方法的实现"></a>(1) Visdom可视化方法的实现</h2><ul><li>以下python类可自行实现，这里贴出本人使用的Visdom类和方法。<br>注：以下代码直接拷贝至visualizer.py并与模型训练文件放在同一路径下即可，在需要可视化的文件中调用Visualizer类。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> visdom<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> time<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Visualizer</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    封装了visdom的基本操作，但是你仍然可以通过`self.vis.function`</span><br><span class="hljs-string">    或者`self.function`调用原生的visdom接口</span><br><span class="hljs-string">    比如</span><br><span class="hljs-string">    self.text(&#x27;hello visdom&#x27;)</span><br><span class="hljs-string">    self.histogram(t.randn(1000))</span><br><span class="hljs-string">    self.line(t.arange(0, 10),t.arange(1, 11))</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, env=<span class="hljs-string">&quot;default&quot;</span>, **kwargs</span>):<br>        self.vis = visdom.Visdom(env=env, **kwargs)<br>        self.env = env<br>        <span class="hljs-comment"># 画的第几个数，相当于横坐标</span><br>        <span class="hljs-comment"># 比如(&quot;loss&quot;, 23) 即loss的第23个点</span><br>        self.index = &#123;&#125;<br>        self.log_text = <span class="hljs-string">&quot;&quot;</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reinit</span>(<span class="hljs-params">self, env=<span class="hljs-string">&quot;default&quot;</span>, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        修改visdom的配置</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.vis = visdom.Visdom(env=env, **kwargs)<br>        self.env = env<br>        <br>        <span class="hljs-keyword">return</span> self<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_many</span>(<span class="hljs-params">self, d</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        一次plot多个</span><br><span class="hljs-string">        @params d: dict (name, value) i.e. (&quot;loss&quot;, 0.11)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> d.iteritems():<br>            self.plot(k, v)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">img_many</span>(<span class="hljs-params">self, d</span>):<br>        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> d.iteritems():<br>            self.img(k, v)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">plot</span>(<span class="hljs-params">self, name, y, **kwargs</span>):<br>        <span class="hljs-comment"># self.plot(&quot;loss&quot;, 1.00)</span><br>        <br>        x = self.index.get(name, <span class="hljs-number">0</span>)<br>        self.vis.line(Y=np.array([y]), X=np.array([x]),<br>                      win=name,<br>                      opts=<span class="hljs-built_in">dict</span>(title=name),<br>                      update=<span class="hljs-literal">None</span> <span class="hljs-keyword">if</span> x == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;append&quot;</span>,<br>                      **kwargs<br>                      )<br>        self.index[name] = x + <span class="hljs-number">1</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">img</span>(<span class="hljs-params">self, name, img_, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        self.img(&quot;input_img&quot;, t.Tensor(64, 64))</span><br><span class="hljs-string">        self.img(&quot;input_imgs&quot;, t.Tensor(3, 64, 64))</span><br><span class="hljs-string">        self.img(&quot;input_imgs&quot;, t.Tensor(100, 1, 64, 64))</span><br><span class="hljs-string">        self.img(&quot;input_imgs&quot;, t.Tensor(100, 3, 64, 64), nrows=10)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.vis.images(img_,<br>                        win=name,<br>                        opts=<span class="hljs-built_in">dict</span>(title=name),<br>                        **kwargs<br>                        )<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">log</span>(<span class="hljs-params">self, info, win=<span class="hljs-string">&quot;log_text&quot;</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        self.log(&#123;&quot;loss&quot;: 1, &quot;lr&quot;: 0.0001&#125;)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.log_text += (<span class="hljs-string">&quot;[&#123;time&#125;] &#123;info&#125; &lt;br&gt;&quot;</span>.<span class="hljs-built_in">format</span>(<br>            time=time.strftime(<span class="hljs-string">&quot;%m%d_%H%M%S&quot;</span>), info=info))<br>        self.vis.text(self.log_text, win)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getattr__</span>(<span class="hljs-params">self, name</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        self.function 等价于self.vis.function</span><br><span class="hljs-string">        自定义的plot, image, log, plot_many等除外</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">getattr</span>(self.vis, name)<br></code></pre></td></tr></table></figure><h2 id="2-Visualizer类的调用以及其方法的使用"><a href="#2-Visualizer类的调用以及其方法的使用" class="headerlink" title="(2) Visualizer类的调用以及其方法的使用"></a>(2) Visualizer类的调用以及其方法的使用</h2><h3 id="Visualizer类的初始化"><a href="#Visualizer类的初始化" class="headerlink" title="Visualizer类的初始化"></a>Visualizer类的初始化</h3><ul><li>Visualizer类包含环境名和端口号两个参数。<br>环境名（env）：字符串，无默认值，为便于区分，建议设置为实验名称。如’exp1’。<br>端口号（port）：整数，默认为8097，避免与他人冲突，建议更改。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">viz = Visualizer(env, port)<br></code></pre></td></tr></table></figure><h3 id="Visualizer类中方法的使用"><a href="#Visualizer类中方法的使用" class="headerlink" title="Visualizer类中方法的使用"></a>Visualizer类中方法的使用</h3><ul><li>以下通过举例来说明。</li></ul><h4 id="Loss曲线可视化"><a href="#Loss曲线可视化" class="headerlink" title="Loss曲线可视化"></a>Loss曲线可视化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">viz.plot(<span class="hljs-string">&quot;train loss&quot;</span>, loss.item())<br></code></pre></td></tr></table></figure><h4 id="图像可视化"><a href="#图像可视化" class="headerlink" title="图像可视化"></a>图像可视化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Python">viz.img(name=<span class="hljs-string">&quot;images&quot;</span>, img_=img[<span class="hljs-number">0</span>, :, :, :])<br>viz.img(name=<span class="hljs-string">&quot;labels&quot;</span>, img_=gt[<span class="hljs-number">0</span>, :, :, :])<br>viz.img(name=<span class="hljs-string">&quot;prediction&quot;</span>, img_=pred[<span class="hljs-number">0</span>, :, :, :])<br></code></pre></td></tr></table></figure><h2 id="3-Visdom服务器启动"><a href="#3-Visdom服务器启动" class="headerlink" title="(3) Visdom服务器启动"></a>(3) Visdom服务器启动</h2><ul><li>新开一个终端，在代码环境下执行以下命令即可打开。<br>注：此终端必须保持打开状态，不可关闭。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">python -m visdom.server -port=<span class="hljs-number">9999</span><br></code></pre></td></tr></table></figure><h2 id="4-打开Visdom可视化网页"><a href="#4-打开Visdom可视化网页" class="headerlink" title="(4) 打开Visdom可视化网页"></a>(4) 打开Visdom可视化网页</h2><ul><li>浏览器中输入server_ip:9999打开网页，在environment中选择自己设置的环境名即可看到可视化结果。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 环境配置 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>（整理链接）常用网络的Pytorch实现</title>
      <link href="/2022/07/05/025-PytorchImplementation/"/>
      <url>/2022/07/05/025-PytorchImplementation/</url>
      
        <content type="html"><![CDATA[<p>整理出来方便自用，不用每次都去到处找开源代码。（以下链接中的代码可直接拷贝使用）</p><h2 id="CNNs"><a href="#CNNs" class="headerlink" title="CNNs"></a>CNNs</h2><h3 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h3><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p><a href="https://github.com/dansuh17/alexnet-pytorch/blob/d0c1b1c52296ffcbecfbf5b17e1d1685b4ca6744/model.py#L40">alexnet-pytorch&#x2F;model.py at d0c1b1c52296ffcbecfbf5b17e1d1685b4ca6744 · dansuh17&#x2F;alexnet-pytorch (github.com)</a></p><h3 id="VGG系列"><a href="#VGG系列" class="headerlink" title="VGG系列"></a>VGG系列</h3><p><a href="https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/vgg.py#L24">vision&#x2F;vgg.py at 6db1569c89094cf23f3bc41f79275c45e9fcb3f3 · pytorch&#x2F;vision (github.com)</a></p><h3 id="ResNet系列及其衍生系列"><a href="#ResNet系列及其衍生系列" class="headerlink" title="ResNet系列及其衍生系列"></a>ResNet系列及其衍生系列</h3><ul><li>ResNet：<a href="https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py#L124">vision&#x2F;resnet.py at 6db1569c89094cf23f3bc41f79275c45e9fcb3f3 · pytorch&#x2F;vision (github.com)</a></li><li>ResNeSt：<a href="https://github.com/zhanghang1989/ResNeSt/blob/5fe47e93bd7e098d15bc278d8ab4812b82b49414/resnest/torch/resnet.py#L129">ResNeSt&#x2F;resnet.py at 5fe47e93bd7e098d15bc278d8ab4812b82b49414 · zhanghang1989&#x2F;ResNeSt (github.com)</a></li><li>ResNeXt：<a href="https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py#L124">vision&#x2F;resnet.py at 6db1569c89094cf23f3bc41f79275c45e9fcb3f3 · pytorch&#x2F;vision (github.com)</a></li><li>SENet：<a href="https://github.com/moskomule/senet.pytorch/blob/master/senet/se_resnet.py">senet.pytorch&#x2F;se_resnet.py at master · moskomule&#x2F;senet.pytorch (github.com)</a></li><li>SKNet：<a href="https://github.com/pppLang/SKNet/blob/master/sknet.py">SKNet&#x2F;sknet.py at master · pppLang&#x2F;SKNet (github.com)</a></li><li>RepVGG：<a href="https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py">RepVGG&#x2F;repvgg.py at main · DingXiaoH&#x2F;RepVGG (github.com)</a></li></ul><h3 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h3><p><a href="https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/densenet.py#L126">vision&#x2F;densenet.py at 6db1569c89094cf23f3bc41f79275c45e9fcb3f3 · pytorch&#x2F;vision (github.com)</a></p><h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><p><a href="https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/googlenet.py#L62">vision&#x2F;googlenet.py at 6db1569c89094cf23f3bc41f79275c45e9fcb3f3 · pytorch&#x2F;vision (github.com)</a></p><h3 id="MobileNet系列"><a href="#MobileNet系列" class="headerlink" title="MobileNet系列"></a>MobileNet系列</h3><ul><li>V1：<a href="https://github.com/osmr/imgclsmob/blob/956b4ebab0bbf98de4e1548287df5197a3c7154e/pytorch/pytorchcv/models/mobilenet.py#L14">imgclsmob&#x2F;mobilenet.py at 956b4ebab0bbf98de4e1548287df5197a3c7154e · osmr&#x2F;imgclsmob (github.com)</a></li><li>V2：<a href="https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/mobilenet.py#L77">vision&#x2F;mobilenet.py at 6db1569c89094cf23f3bc41f79275c45e9fcb3f3 · pytorch&#x2F;vision (github.com)</a></li></ul><h3 id="ShuffleNet系列"><a href="#ShuffleNet系列" class="headerlink" title="ShuffleNet系列"></a>ShuffleNet系列</h3><ul><li>V1：<a href="https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/shufflenet.py#L123">imgclsmob&#x2F;shufflenet.py at c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7 · osmr&#x2F;imgclsmob (github.com)</a></li><li>V2：<a href="https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/shufflenetv2.py#L86">vision&#x2F;shufflenetv2.py at 6db1569c89094cf23f3bc41f79275c45e9fcb3f3 · pytorch&#x2F;vision (github.com)</a></li></ul><h3 id="EfficientNet系列"><a href="#EfficientNet系列" class="headerlink" title="EfficientNet系列"></a>EfficientNet系列</h3><ul><li>V1：<a href="https://github.com/lukemelas/EfficientNet-PyTorch/blob/2eb7a7d264344ddf15d0a06ee99b0dca524c6a07/efficientnet_pytorch/model.py#L143">EfficientNet-PyTorch&#x2F;model.py at 2eb7a7d264344ddf15d0a06ee99b0dca524c6a07 · lukemelas&#x2F;EfficientNet-PyTorch (github.com)</a></li><li>V2：<a href="https://github.com/google/automl/blob/master/efficientnetv2/effnetv2_model.py">automl&#x2F;effnetv2_model.py at master · google&#x2F;automl (github.com)</a></li></ul><h3 id="HRNet"><a href="#HRNet" class="headerlink" title="HRNet"></a>HRNet</h3><p><a href="https://github.com/HRNet/HRNet-Image-Classification/blob/8f158719e821836e21e6cba99a3241a12a13bc41/lib/models/cls_hrnet.py#L254">HRNet-Image-Classification&#x2F;cls_hrnet.py at 8f158719e821836e21e6cba99a3241a12a13bc41 · HRNet&#x2F;HRNet-Image-Classification (github.com)</a></p><h3 id="Deformable-Convolutional-Network"><a href="#Deformable-Convolutional-Network" class="headerlink" title="Deformable Convolutional Network"></a>Deformable Convolutional Network</h3><ul><li>DCNv1：待补充</li><li>DCNv2：待补充</li></ul><h3 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h3><p><a href="https://github.com/pytorch/vision/blob/bf843c664b8ba0ff49d2921237500c77d82f2d04/torchvision/models/segmentation/fcn.py#L9">vision&#x2F;fcn.py at bf843c664b8ba0ff49d2921237500c77d82f2d04 · pytorch&#x2F;vision (github.com)</a></p><h3 id="UNet及其衍生"><a href="#UNet及其衍生" class="headerlink" title="UNet及其衍生"></a>UNet及其衍生</h3><ul><li>U-Net：<a href="https://github.com/milesial/Pytorch-UNet/blob/67bf11b4db4c5f2891bd7e8e7f58bcde8ee2d2db/unet/unet_model.py#L8">Pytorch-UNet&#x2F;unet_model.py at 67bf11b4db4c5f2891bd7e8e7f58bcde8ee2d2db · milesial&#x2F;Pytorch-UNet (github.com)</a></li><li>U-Net++：<a href="https://github.com/MrGiovanni/UNetPlusPlus/blob/e145ba63862982bf1099cf2ec11d5466b434ae0b/pytorch/nnunet/network_architecture/generic_UNetPlusPlus.py#L167">UNetPlusPlus&#x2F;generic_UNetPlusPlus.py at e145ba63862982bf1099cf2ec11d5466b434ae0b · MrGiovanni&#x2F;UNetPlusPlus (github.com)</a></li><li>U-Net 3+：<a href="https://github.com/ZJUGiveLab/UNet-Version/blob/master/models/UNet_3Plus.py">UNet-Version&#x2F;UNet_3Plus.py at master · ZJUGiveLab&#x2F;UNet-Version (github.com)</a></li><li>Attention U-Net：<a href="https://github.com/LeeJunHyun/Image_Segmentation/blob/master/network.py">Image_Segmentation&#x2F;network.py at master · LeeJunHyun&#x2F;Image_Segmentation (github.com)</a></li><li>R2 U-Net：<a href="https://github.com/LeeJunHyun/Image_Segmentation/blob/master/network.py">Image_Segmentation&#x2F;network.py at master · LeeJunHyun&#x2F;Image_Segmentation (github.com)</a></li><li>U^2 Net：<a href="https://github.com/xuebinqin/U-2-Net/blob/master/model/u2net.py">U-2-Net&#x2F;u2net.py at master · xuebinqin&#x2F;U-2-Net (github.com)</a></li></ul><h3 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h3><p><a href="https://github.com/yassouali/pytorch-segmentation/blob/8b8e3ee20a3aa733cb19fc158ad5d7773ed6da7f/models/segnet.py#L9">pytorch-segmentation&#x2F;segnet.py at 8b8e3ee20a3aa733cb19fc158ad5d7773ed6da7f · yassouali&#x2F;pytorch-segmentation (github.com)</a></p><h3 id="PSPNet"><a href="#PSPNet" class="headerlink" title="PSPNet"></a>PSPNet</h3><p><a href="https://github.com/hszhao/semseg/blob/7192f922b99468969cfd4535e3e35a838994b115/model/pspnet.py#L29">semseg&#x2F;pspnet.py at 7192f922b99468969cfd4535e3e35a838994b115 · hszhao&#x2F;semseg (github.com)</a></p><h3 id="DeepLab系列"><a href="#DeepLab系列" class="headerlink" title="DeepLab系列"></a>DeepLab系列</h3><ul><li>V1：<a href="https://github.com/kazuto1011/deeplab-pytorch/blob/master/libs/models/deeplabv1.py">deeplab-pytorch&#x2F;deeplabv1.py at master · kazuto1011&#x2F;deeplab-pytorch (github.com)</a></li><li>V2：<a href="https://github.com/kazuto1011/deeplab-pytorch/blob/master/libs/models/deeplabv2.py">deeplab-pytorch&#x2F;deeplabv2.py at master · kazuto1011&#x2F;deeplab-pytorch (github.com)</a></li><li>V3：<a href="https://github.com/fregu856/deeplabv3/blob/master/model/deeplabv3.py">deeplabv3&#x2F;deeplabv3.py at master · fregu856&#x2F;deeplabv3 (github.com)</a></li><li>V3+：<a href="https://github.com/YudeWang/deeplabv3plus-pytorch/blob/master/lib/net/deeplabv3plus.py">deeplabv3plus-pytorch&#x2F;deeplabv3plus.py at master · YudeWang&#x2F;deeplabv3plus-pytorch (github.com)</a></li></ul><h2 id="Transformers"><a href="#Transformers" class="headerlink" title="Transformers"></a>Transformers</h2><h3 id="ViT"><a href="#ViT" class="headerlink" title="ViT"></a>ViT</h3><p><a href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py">vit-pytorch&#x2F;vit.py at main · lucidrains&#x2F;vit-pytorch (github.com)</a></p><h3 id="DeepViT"><a href="#DeepViT" class="headerlink" title="DeepViT"></a>DeepViT</h3><p><a href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/deepvit.py">vit-pytorch&#x2F;deepvit.py at main · lucidrains&#x2F;vit-pytorch (github.com)</a></p><h3 id="Cross-ViT"><a href="#Cross-ViT" class="headerlink" title="Cross ViT"></a>Cross ViT</h3><p><a href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/cross_vit.py">vit-pytorch&#x2F;cross_vit.py at main · lucidrains&#x2F;vit-pytorch (github.com)</a></p><h3 id="Swin-Transformer"><a href="#Swin-Transformer" class="headerlink" title="Swin Transformer"></a>Swin Transformer</h3><p><a href="https://github.com/berniwal/swin-transformer-pytorch/blob/master/swin_transformer_pytorch/swin_transformer.py">swin-transformer-pytorch&#x2F;swin_transformer.py at master · berniwal&#x2F;swin-transformer-pytorch (github.com)</a></p><h3 id="Token-to-Token-ViT"><a href="#Token-to-Token-ViT" class="headerlink" title="Token-to-Token ViT"></a>Token-to-Token ViT</h3><p><a href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/t2t.py">vit-pytorch&#x2F;t2t.py at main · lucidrains&#x2F;vit-pytorch (github.com)</a></p><h3 id="TransUNet"><a href="#TransUNet" class="headerlink" title="TransUNet"></a>TransUNet</h3><p><a href="https://github.com/Beckschen/TransUNet/blob/main/networks/vit_seg_modeling.py">TransUNet&#x2F;vit_seg_modeling.py at main · Beckschen&#x2F;TransUNet (github.com)</a></p><h3 id="Medical-Transformer"><a href="#Medical-Transformer" class="headerlink" title="Medical Transformer"></a>Medical Transformer</h3><p><a href="https://github.com/jeya-maria-jose/Medical-Transformer/blob/main/lib/models/model_codes.py">Medical-Transformer&#x2F;model_codes.py at main · jeya-maria-jose&#x2F;Medical-Transformer (github.com)</a></p><h3 id="TransFuse"><a href="#TransFuse" class="headerlink" title="TransFuse"></a>TransFuse</h3><p>待补充</p><h3 id="Swin-UNet"><a href="#Swin-UNet" class="headerlink" title="Swin-UNet"></a>Swin-UNet</h3><p>待补充</p><h3 id="nnFormer"><a href="#nnFormer" class="headerlink" title="nnFormer"></a>nnFormer</h3><p><a href="https://github.com/282857341/nnFormer/blob/main/nnformer/network_architecture/neural_network.py">nnFormer&#x2F;neural_network.py at main · 282857341&#x2F;nnFormer (github.com)</a></p><h3 id="UCTranNet"><a href="#UCTranNet" class="headerlink" title="UCTranNet"></a>UCTranNet</h3><p><a href="https://github.com/McGregorWwww/UCTransNet/blob/main/nets/UCTransNet.py">UCTransNet&#x2F;UCTransNet.py at main · McGregorWwww&#x2F;UCTransNet (github.com)</a></p><h3 id="CrossFormer"><a href="#CrossFormer" class="headerlink" title="CrossFormer"></a>CrossFormer</h3><p><a href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/crossformer.py">vit-pytorch&#x2F;crossformer.py at main · lucidrains&#x2F;vit-pytorch (github.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ISICDM_2020挑战赛——肺部组织分割</title>
      <link href="/2022/07/05/017-ISICDM2020/"/>
      <url>/2022/07/05/017-ISICDM2020/</url>
      
        <content type="html"><![CDATA[<p>记录一下沈阳的第四届图像计算与数字医学国际研讨会(The 4th International Symposium on Image Computing and Digital Medicine, ISICDM)挑战赛的竞赛过程。不得不吐槽一下整个过程项目三的主办方的各种延误和低级错误，比如标签给的JPG格式，槽点太多，吐槽不过来。<br>挑战赛官方网址：<a href="http://isicdm2020.imagecomputing.org/cn/Challenges.html">http://isicdm2020.imagecomputing.org/cn/Challenges.html</a></p><hr><h1 id="一、竞赛背景"><a href="#一、竞赛背景" class="headerlink" title="一、竞赛背景"></a>一、竞赛背景</h1><ul><li>我国在工业化发展过程中，由于高度的空气污染、高吸烟率和较差的生产保护条件，导致肺癌、慢阻肺等肺部疾病的发病率一直维持在较高水平，并呈加剧趋势。另外，SARS、 COVID-19等冠状病毒肺炎传染速度快，已被列为威胁全球公众健康的重大传染性疾病。肺部疾病已经成为危害人民健康、社会及经济可持续发展的严重公共卫生问题和社会问题。 当前医学影像已经成为肺部疾病临床检测和诊断、治疗方案规划和术后评估等各个环节的重要依据。近年来，国内外研究者在肺部影像智能辅助诊疗技术方面已取得显著研究进展，在肺部疾病的早期诊断、精准治疗等方面开始发挥重要作用。传统采用检测图像中病变特征区域的异常依赖方案，对于正常组织的关注度较低，不符合专业医师的工作思路，在临床应用中具有局限性。由于肺部结构复杂、相邻组织区分度低、结构边界不清晰、区域密度不均匀和形状不规则，如何实现<strong>肺部多层次组织结构的精准提取和定量评估</strong>，已成为肺部影像智能化、定量化、精准化临床应用需求的难题和共性问题。</li></ul><h1 id="二、数据说明"><a href="#二、数据说明" class="headerlink" title="二、数据说明"></a>二、数据说明</h1><ul><li><strong>本数据集包含训练集和测试集两部分。训练集包含10例胸部CT平扫影像和10例CTA增强影像（注射血管造影剂，灰度值增强），层厚小于 2.0mm，分辨率512*512，DICOM格式，包括肺实质、支气管、肺部血管等组织的标注结构。测试集将分别由5例胸部CT影像、5例CTA影像组成。</strong></li><li>本数据集由竞赛参加人填写申请表后，提供下载链接，本数据集仅限于做本次竞赛和科研使用，不得用于任何商业用途。</li></ul><h1 id="三、评价说明"><a href="#三、评价说明" class="headerlink" title="三、评价说明"></a>三、评价说明</h1><ul><li>评价采用测试集机器评价与医生主观评价相结合的方式，按照得分顺序进行排名。</li></ul><hr><h1 id="四、数据处理"><a href="#四、数据处理" class="headerlink" title="四、数据处理"></a>四、数据处理</h1><h2 id="One-Hot编码"><a href="#One-Hot编码" class="headerlink" title="One-Hot编码"></a>One-Hot编码</h2><ul><li>在图像分割中，lable也是图片，进行训练时，需要将lable image转换成one hot编码的形式。本文仅使用np.equal，np.all，np.stack，三个操作实现one hot编码。其中label为gt image, label_values为class RGB Values list。</li><li>np.equal实现把label image每个像素的RGB值与某个class的RGB值进行比对，变成RGB bool值。</li><li>np.all 把RGB bool值，变成一个bool值，即实现某个class 的label mask。使用for循环，生成所有class的label mask。</li><li>np.stack实现所有class的label mask的堆叠。最终depth size 为num_classes的数量。</li></ul><h2 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h2><ul><li>本次比赛的数据处理和训练的Pipeline如下图。<br><img src="https://i.loli.net/2021/07/05/Pc5JtLSfEuWZOYk.png" alt="Pipeline"></li></ul><h1 id="五、模型选择"><a href="#五、模型选择" class="headerlink" title="五、模型选择"></a>五、模型选择</h1><ul><li><p>本次比赛都选择生成对抗网络（GAN）作为Backbone，再在生成器部分做大量尝试，将UNet等分割网络作为生成器，再在生成器的输出处进行深度监督（像素级）。<br><img src="https://i.loli.net/2021/07/05/vMwKE34APSe8TUa.png" alt="肺实质分割网络"><br><img src="https://i.loli.net/2021/07/05/ezMwF5oB6QyXUlj.png" alt="肺气管分割网络"><br><img src="https://i.loli.net/2021/07/05/LPioqldhRFGxE7u.png" alt="肺血管分割网络"></p></li><li><p>生成器部分的尝试包括：<br>（1）U-Net<br>（2）Two Stage（模型框架源自于HMOENet）<br>（3）U-Net with Inception Module</p></li><li><p>模型细节设计<br>生成器和判别器输出都不需要Sigmoid，会在训练中处理<br>附Codes: <a href="https://github.com/SvyJ/svyj.github.io/blob/master/codes/ISICDM">https://github.com/SvyJ/svyj.github.io/blob/master/codes/ISICDM</a></p></li></ul><h1 id="六、训练细节"><a href="#六、训练细节" class="headerlink" title="六、训练细节"></a>六、训练细节</h1><ul><li>模型训练细节</li></ul><p>（1）固定判别器，训练生成器<br>（2）固定生成器，训练判别器</p><ul><li>损失函数细节</li></ul><p>（1）生成器G_Loss：由三部分组成，像素点损失BCELoss，DiceLoss和一致性损失ConsistencyLoss（来自判别器）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">G_Loss = <span class="hljs-number">0.5</span>\*(loss_ce + loss_dice) + consisitency_loss\*consistency_weight<br></code></pre></td></tr></table></figure><p>（2）判别器D_Loss：由两部分组成，real &amp; fake</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">GAN：D_loss = D_loss_real + D_loss_fake<br>WGAN：D_loss = torch.<span class="hljs-built_in">abs</span>(D_outputs_fake.mean() - D_outputs_real.mean())<br></code></pre></td></tr></table></figure><ul><li>参数控制细节</li></ul><p>（1）学习率：根据评价指标降低学习率，指标在patience个batch后不变化则降低factor比例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">scheduler_d = torch.optim.lr_scheduler.ReduceLROnPlateau(D_optimizer, mode=<span class="hljs-string">&#x27;max&#x27;</span>, factor=<span class="hljs-number">0.5</span>, patience=<span class="hljs-number">10</span>)<br>scheduler_g = torch.optim.lr_scheduler.ReduceLROnPlateau(G_optimizer, mode=<span class="hljs-string">&#x27;max&#x27;</span>, factor=<span class="hljs-number">0.5</span>, patience=<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p>（2）权重：控制权重在clamp_lower&#x3D;-0.01和clamp_upper&#x3D;0.01之间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> D.parameters():<br>p.data.clamp_(clamp_lower, clamp_upper)<br></code></pre></td></tr></table></figure><p>（3）梯度：防止梯度爆炸</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">nn.utils.clip_grad_value_(D.parameters(), <span class="hljs-number">0.1</span>)<br></code></pre></td></tr></table></figure><ul><li>附Codes（训练细节在train_WGAN_airway）: <a href="https://github.com/SvyJ/svyj.github.io/tree/master/codes/ISICDM2020">https://github.com/SvyJ/svyj.github.io/tree/master/codes/ISICDM2020</a></li></ul><h1 id="七、实验结果"><a href="#七、实验结果" class="headerlink" title="七、实验结果"></a>七、实验结果</h1><ul><li>涨经验去了，请忽略本菜鸡们的成绩（Team49）<br><img src="https://i.loli.net/2021/07/05/QHKd9AmW2tprTgE.png" alt="热身赛成绩排名"><br><img src="https://i.loli.net/2021/07/05/oAT5f7pGV418zMg.png" alt="排位赛成绩排名"><br><img src="https://i.loli.net/2021/07/05/TrusH84xWmtjPqw.png" alt="决赛成绩排名"></li></ul>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 图像分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何评估你的网络模型？</title>
      <link href="/2022/07/05/008-DL_Models_Metrics/"/>
      <url>/2022/07/05/008-DL_Models_Metrics/</url>
      
        <content type="html"><![CDATA[<h1 id="二分类网络的评价指标"><a href="#二分类网络的评价指标" class="headerlink" title="二分类网络的评价指标"></a><strong>二分类网络的评价指标</strong></h1><ul><li>Acc, F1, ROC, AUC</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Metrics</span>(<span class="hljs-params">output, labels</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    output: &lt;list&gt;</span><br><span class="hljs-string">    labels: &lt;list&gt;</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    test_correct = <span class="hljs-number">0</span><br>    test_total = <span class="hljs-number">0</span><br>    predicts = []<br>    prob = []<br>    tp, tn, fp, fn = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">for</span> outputs <span class="hljs-keyword">in</span> output:<br>        index, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>        predicts.append([outputs.data, predicted, labels])<br>        test_total += labels.size(<span class="hljs-number">0</span>)<br>        test_correct += (predicted == labels.data).<span class="hljs-built_in">sum</span>()<br><br>        <span class="hljs-comment"># calc F1-score</span><br>        <br>        f1_predicts = predicted.cpu().numpy().tolist()<br>        f1_label = labels.data.cpu().numpy().tolist()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(f1_label)):<br>            <span class="hljs-keyword">if</span> f1_predicts[i] == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> f1_label[i] == <span class="hljs-number">1</span>:<br>                tp += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> f1_predicts[i] == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> f1_label[i] == <span class="hljs-number">0</span>:<br>                fp += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> f1_predicts[i] == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> f1_label[i] == <span class="hljs-number">1</span>:<br>                fn += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> f1_predicts[i] == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> f1_label[i] == <span class="hljs-number">0</span>:<br>                tn += <span class="hljs-number">1</span><br><br>        <span class="hljs-comment"># calc softmax probability</span><br>        outputs = outputs.data.cpu().numpy()<br>        probability = np.exp(outputs)/np.mat(np.<span class="hljs-built_in">sum</span>(np.exp(outputs), axis=<span class="hljs-number">1</span>)).T<br>        prob = probability[:,<span class="hljs-number">1</span>].T.tolist()[<span class="hljs-number">0</span>]  <br>        <span class="hljs-keyword">for</span> p, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prob, labels.data.cpu().numpy().tolist()):<br>            prob.append([p, label])<br><br>    F1_Score = <span class="hljs-number">2</span>*tp/(<span class="hljs-number">2</span>*tp+fp+fn)<br>    test_acc = test_correct.item()/test_total <br><br>    <span class="hljs-keyword">return</span> prob, test_acc, F1_Score<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">drawROC</span>(<span class="hljs-params">prob</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    prob: &lt;list&gt;: [probability of positive, true label]</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br><br>    neg = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(prob)):<br>        <span class="hljs-keyword">if</span> prob[i][<span class="hljs-number">1</span>] == <span class="hljs-number">0</span>:<br>            neg += <span class="hljs-number">1</span><br>    pos = <span class="hljs-built_in">len</span>(prob) - neg<br><br>    x = []<br>    y = []<br>    sample_sort = <span class="hljs-built_in">sorted</span>(prob, key=<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">0</span>], reverse=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sample_sort)):<br>        tp = <span class="hljs-number">0</span><br>        fp = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i, <span class="hljs-built_in">len</span>(sample_sort)):<br>            <span class="hljs-keyword">if</span> sample_sort[j][<span class="hljs-number">1</span>] == <span class="hljs-number">1</span>:  <span class="hljs-comment"># true positive</span><br>                tp += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> sample_sort[j][<span class="hljs-number">1</span>] == <span class="hljs-number">0</span>:  <span class="hljs-comment"># false positive</span><br>                fp += <span class="hljs-number">1</span><br>        x.append(fp/neg)<br>        y.append(tp/pos)<br>    x.append(<span class="hljs-number">0</span>)<br>    y.append(<span class="hljs-number">0</span>)<br><br>    auc = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x)-<span class="hljs-number">1</span>):<br>        auc += ((x[i]-x[i+<span class="hljs-number">1</span>]) * (y[i]+y[i+<span class="hljs-number">1</span>]))<br><br>    <span class="hljs-comment"># ROC curve</span><br>    plt.clf()<br>    plt.title(<span class="hljs-string">&#x27;Small_Intestine_Classifier_ROC&#x27;</span>)<br>    plt.plot(x, y, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;AUC = %.3f ACC = %.3f&#x27;</span> % (auc*<span class="hljs-number">0.5</span>))<br>    plt.xlabel(<span class="hljs-string">&#x27;FPR&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;TPR&#x27;</span>)<br>    plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br>    plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;r--&#x27;</span>)<br>    plt.savefig(<span class="hljs-string">&quot;./temp/Small_Intestine_Classifier_ROC_AUC=%.3f_EfficientNet.jpg&quot;</span> % (auc*<span class="hljs-number">0.5</span>))<br><br>    <span class="hljs-keyword">return</span> auc*<span class="hljs-number">0.5</span><br></code></pre></td></tr></table></figure><h1 id="多分类网络的评价指标-以四分类为例"><a href="#多分类网络的评价指标-以四分类为例" class="headerlink" title="多分类网络的评价指标(以四分类为例)"></a><strong>多分类网络的评价指标(以四分类为例)</strong></h1><ul><li>Acc, macro_F1, micro_F1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Metrics</span>(<span class="hljs-params">output, labels</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    output: &lt;list&gt;</span><br><span class="hljs-string">    labels: &lt;list&gt;</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    test_correct = <span class="hljs-number">0</span><br>    test_total = <span class="hljs-number">0</span><br>    Confusion_Matrix = np.zeros((<span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br><br>    <span class="hljs-comment"># traverse output</span><br>    <span class="hljs-keyword">for</span> outputs <span class="hljs-keyword">in</span> output:<br>        index, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>        test_total += labels.size(<span class="hljs-number">0</span>)<br>        test_correct += (predicted == labels.data).<span class="hljs-built_in">sum</span>()<br><br>        <span class="hljs-comment"># calc Confusion Matrix</span><br>        f1_predicts = predicted.cpu().numpy().tolist()<br>        f1_label = labels.data.cpu().numpy().tolist()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(f1_label)):<br>            Confusion_Matrix[f1_predicts[i]][f1_label[i]] += <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># print(&quot;Confusion_Matrix: \n&quot;, Confusion_Matrix)    </span><br><br>    <span class="hljs-comment"># calc F1-Score</span><br>    TP, FP, TN, FN, P, R = [], [], [], [], [], []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i+<span class="hljs-number">1</span>, <span class="hljs-number">4</span>):<br>            tp = Confusion_Matrix[i][i]<br>            tn = Confusion_Matrix[j][j]<br>            fp = Confusion_Matrix[j][i]<br>            fn = Confusion_Matrix[i][j]<br>        TP.append(tp)<br>        FP.append(fp)<br>        TN.append(tn)<br>        FN.append(fn)<br>        P.append(tp/(tp+fp))<br>        R.append(tp/(tp+fn))<br>    macro_P = np.mean(P)<br>    macro_R = np.mean(R)<br>    _TP = np.mean(TP)<br>    _FP = np.mean(FP)<br>    _TN = np.mean(TN)<br>    _FN = np.mean(FN)<br>    micro_P = _TP/(_TP+_FP)<br>    micro_R = _TP/(_TP+_FN)<br><br>    macro_F1 = (<span class="hljs-number">2</span>*macro_P*macro_R)/(macro_P+macro_R)<br>    micro_F1 = (<span class="hljs-number">2</span>*micro_P*micro_R)/(micro_P+micro_R)<br>    test_acc = test_correct.item()/test_total<br><br>    <span class="hljs-keyword">return</span> test_acc, macro_F1, micro_F1<br></code></pre></td></tr></table></figure><h1 id="分割网络的评价指标"><a href="#分割网络的评价指标" class="headerlink" title="分割网络的评价指标"></a><strong>分割网络的评价指标</strong></h1><ul><li>Pixel_Acc, Dice, Jac, IoU, Spe, Sen, Pre</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Metrics</span>(<span class="hljs-params">output, mask</span>):<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">output: model outputs, size=[Batch_Size, Channels, Height, Width]</span><br><span class="hljs-string">mask:   Ground Truth</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>tp, fp, tn, fn = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br><br><span class="hljs-comment"># output binarization</span><br>output[output&gt;=<span class="hljs-number">0.5</span>] = <span class="hljs-number">1</span><br>    output[output&lt;<span class="hljs-number">0.5</span>]  = <span class="hljs-number">0</span><br>    output = output.cpu().detach().numpy().flatten().flatten()<br>    img_mask = img_mask.cpu().numpy().flatten()<br><br>    <span class="hljs-comment"># nums for metrics</span><br>    test_pixel_correct = <span class="hljs-built_in">len</span>(np.argwhere(output == img_mask))<br>    test_pixel_total   = output.size<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(output.size):<br>        <span class="hljs-keyword">if</span> output[i] == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">if</span> img_mask[i]==<span class="hljs-number">1</span>: tp += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>: fp += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> img_mask[i]==<span class="hljs-number">0</span>: tn += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>: fn += <span class="hljs-number">1</span><br>    overlap = np.<span class="hljs-built_in">sum</span>(output * img_mask)<br> <br>    <span class="hljs-comment"># Pixel Accuracy</span><br>    test_pixel_acc = test_pixel_correct/test_pixel_total<br>    <span class="hljs-comment"># Dice </span><br>    dice = np.clip(((<span class="hljs-number">2.</span> * overlap) / (np.<span class="hljs-built_in">sum</span>(img_mask) + np.<span class="hljs-built_in">sum</span>(output) + <span class="hljs-number">1</span>)), <span class="hljs-number">1e-4</span>, <span class="hljs-number">0.9999</span>)<br>    <span class="hljs-comment"># Jaccard Index</span><br>    jac  = overlap/(np.<span class="hljs-built_in">sum</span>(img_mask) + np.<span class="hljs-built_in">sum</span>(output)-overlap)<br>    <span class="hljs-comment"># Intersection over Union</span><br>    IoU = dice/(<span class="hljs-number">2</span>-dice)<br>    <span class="hljs-comment"># specificity</span><br>    Spe = tn/(tn+fp)<br>    <span class="hljs-comment"># sensitivity</span><br>    Sen = tp/(tp+fn)<br>    <span class="hljs-comment"># Precision</span><br>    Pre = tp/(tp+fp)<br><br>    <span class="hljs-keyword">return</span> test_pixel_acc, dice, jac, IoU, Spe, Sen, Pre<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> 评价指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典分割模型的Pytorch实现</title>
      <link href="/2022/07/05/007-DL_Models_Segmentation/"/>
      <url>/2022/07/05/007-DL_Models_Segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="FCN-32-x2F-16-x2F-8-x2F-1s-2014"><a href="#FCN-32-x2F-16-x2F-8-x2F-1s-2014" class="headerlink" title="FCN-32&#x2F;16&#x2F;8&#x2F;1s(2014)"></a><br>FCN-32&#x2F;16&#x2F;8&#x2F;1s(2014)</br></h2><h4 id="注-Backbone-VGG"><a href="#注-Backbone-VGG" class="headerlink" title="注.Backbone: VGG"></a>注.Backbone: VGG</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> torchvision.models.vgg <span class="hljs-keyword">import</span> VGG<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FCN32s</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, pretrained_net, n_class=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.n_class = n_class<br>        self.pretrained_net = pretrained_net<br>        self.relu    = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.deconv1 = nn.ConvTranspose2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn1     = nn.BatchNorm2d(<span class="hljs-number">512</span>)<br>        self.deconv2 = nn.ConvTranspose2d(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn2     = nn.BatchNorm2d(<span class="hljs-number">256</span>)<br>        self.deconv3 = nn.ConvTranspose2d(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn3     = nn.BatchNorm2d(<span class="hljs-number">128</span>)<br>        self.deconv4 = nn.ConvTranspose2d(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn4     = nn.BatchNorm2d(<span class="hljs-number">64</span>)<br>        self.deconv5 = nn.ConvTranspose2d(<span class="hljs-number">64</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn5     = nn.BatchNorm2d(<span class="hljs-number">32</span>)<br>        self.classifier = nn.Conv2d(<span class="hljs-number">32</span>, n_class, kernel_size=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        output = self.pretrained_net(x)<br>        x5 = output[<span class="hljs-string">&#x27;x5&#x27;</span>]<br><br>        score = self.bn1(self.relu(self.deconv1(x5)))     <br>        score = self.bn2(self.relu(self.deconv2(score)))  <br>        score = self.bn3(self.relu(self.deconv3(score)))  <br>        score = self.bn4(self.relu(self.deconv4(score)))  <br>        score = self.bn5(self.relu(self.deconv5(score)))  <br>        score = self.classifier(score)                    <br><br>        score = nn.Sigmoid()(score)<br>        <span class="hljs-keyword">return</span> score <br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FCN16s</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, pretrained_net, n_class=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.n_class = n_class<br>        self.pretrained_net = pretrained_net<br>        self.relu    = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.deconv1 = nn.ConvTranspose2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn1     = nn.BatchNorm2d(<span class="hljs-number">512</span>)<br>        self.deconv2 = nn.ConvTranspose2d(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn2     = nn.BatchNorm2d(<span class="hljs-number">256</span>)<br>        self.deconv3 = nn.ConvTranspose2d(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn3     = nn.BatchNorm2d(<span class="hljs-number">128</span>)<br>        self.deconv4 = nn.ConvTranspose2d(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn4     = nn.BatchNorm2d(<span class="hljs-number">64</span>)<br>        self.deconv5 = nn.ConvTranspose2d(<span class="hljs-number">64</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn5     = nn.BatchNorm2d(<span class="hljs-number">32</span>)<br>        self.classifier = nn.Conv2d(<span class="hljs-number">32</span>, n_class, kernel_size=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        output = self.pretrained_net(x)<br>        x5 = output[<span class="hljs-string">&#x27;x5&#x27;</span>]  <br>        x4 = output[<span class="hljs-string">&#x27;x4&#x27;</span>]  <br><br>        score = self.relu(self.deconv1(x5))               <br>        score = self.bn1(score + x4)                      <br>        score = self.bn2(self.relu(self.deconv2(score)))  <br>        score = self.bn3(self.relu(self.deconv3(score)))  <br>        score = self.bn4(self.relu(self.deconv4(score)))  <br>        score = self.bn5(self.relu(self.deconv5(score)))  <br>        score = self.classifier(score)                   <br>        <br>        score = nn.Sigmoid()(score)<br>        <span class="hljs-keyword">return</span> score  <br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FCN8s</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, pretrained_net, n_class=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.n_class = n_class<br>        self.pretrained_net = pretrained_net<br>        self.relu    = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.deconv1 = nn.ConvTranspose2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn1     = nn.BatchNorm2d(<span class="hljs-number">512</span>)<br>        self.deconv2 = nn.ConvTranspose2d(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn2     = nn.BatchNorm2d(<span class="hljs-number">256</span>)<br>        self.deconv3 = nn.ConvTranspose2d(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn3     = nn.BatchNorm2d(<span class="hljs-number">128</span>)<br>        self.deconv4 = nn.ConvTranspose2d(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn4     = nn.BatchNorm2d(<span class="hljs-number">64</span>)<br>        self.deconv5 = nn.ConvTranspose2d(<span class="hljs-number">64</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn5     = nn.BatchNorm2d(<span class="hljs-number">32</span>)<br>        self.classifier = nn.Conv2d(<span class="hljs-number">32</span>, n_class, kernel_size=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        output = self.pretrained_net(x)<br>        x5 = output[<span class="hljs-string">&#x27;x5&#x27;</span>]  <br>        x4 = output[<span class="hljs-string">&#x27;x4&#x27;</span>]  <br>        x3 = output[<span class="hljs-string">&#x27;x3&#x27;</span>]  <br><br>        score = self.relu(self.deconv1(x5))              <br>        score = self.bn1(score + x4)                      <br>        score = self.relu(self.deconv2(score))            <br>        score = self.bn2(score + x3)                      <br>        score = self.bn3(self.relu(self.deconv3(score)))  <br>        score = self.bn4(self.relu(self.deconv4(score)))  <br>        score = self.bn5(self.relu(self.deconv5(score)))  <br>        score = self.classifier(score)                    <br><br>        score = nn.Sigmoid()(score)<br>        <span class="hljs-keyword">return</span> score  <br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FCNs</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, pretrained_net, n_class=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.n_class = n_class<br>        self.pretrained_net = pretrained_net<br>        self.relu    = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.deconv1 = nn.ConvTranspose2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn1     = nn.BatchNorm2d(<span class="hljs-number">512</span>)<br>        self.deconv2 = nn.ConvTranspose2d(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn2     = nn.BatchNorm2d(<span class="hljs-number">256</span>)<br>        self.deconv3 = nn.ConvTranspose2d(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn3     = nn.BatchNorm2d(<span class="hljs-number">128</span>)<br>        self.deconv4 = nn.ConvTranspose2d(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn4     = nn.BatchNorm2d(<span class="hljs-number">64</span>)<br>        self.deconv5 = nn.ConvTranspose2d(<span class="hljs-number">64</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, output_padding=<span class="hljs-number">1</span>)<br>        self.bn5     = nn.BatchNorm2d(<span class="hljs-number">32</span>)<br>        self.classifier = nn.Conv2d(<span class="hljs-number">32</span>, n_class, kernel_size=<span class="hljs-number">1</span>) <br>        <span class="hljs-comment"># classifier is 1x1 conv, to reduce channels from 32 to n_class</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        output = self.pretrained_net(x)<br>        x5 = output[<span class="hljs-string">&#x27;x5&#x27;</span>]  <br>        x4 = output[<span class="hljs-string">&#x27;x4&#x27;</span>]  <br>        x3 = output[<span class="hljs-string">&#x27;x3&#x27;</span>]  <br>        x2 = output[<span class="hljs-string">&#x27;x2&#x27;</span>]  <br>        x1 = output[<span class="hljs-string">&#x27;x1&#x27;</span>]  <br><br>        score = self.bn1(self.relu(self.deconv1(x5)))     <br>        score = score + x4                                <br>        score = self.bn2(self.relu(self.deconv2(score)))  <br>        score = score + x3                                <br>        score = self.bn3(self.relu(self.deconv3(score)))  <br>        score = score + x2                                <br>        score = self.bn4(self.relu(self.deconv4(score)))  <br>        score = score + x1                                <br>        score = self.bn5(self.relu(self.deconv5(score)))  <br>        score = self.classifier(score)                    <br><br>        score = nn.Sigmoid()(score)<br>        <span class="hljs-keyword">return</span> score  <br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VGGNet</span>(<span class="hljs-title class_ inherited__">VGG</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, pretrained=<span class="hljs-literal">True</span>, model=<span class="hljs-string">&#x27;vgg16&#x27;</span>, requires_grad=<span class="hljs-literal">True</span>, remove_fc=<span class="hljs-literal">True</span>, show_params=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__(make_layers(cfg[model]))<br>        self.ranges = ranges[model]<br><br>        <span class="hljs-keyword">if</span> pretrained:<br>            <span class="hljs-built_in">exec</span>(<span class="hljs-string">&quot;self.load_state_dict(models.%s(pretrained=True).state_dict())&quot;</span> % model)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> requires_grad:<br>            <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> <span class="hljs-built_in">super</span>().parameters():<br>                param.requires_grad = <span class="hljs-literal">False</span><br><br>        <span class="hljs-comment"># delete redundant fully-connected layers</span><br>        <span class="hljs-keyword">if</span> remove_fc:  <br>            <span class="hljs-keyword">del</span> self.classifier<br><br>        <span class="hljs-keyword">if</span> show_params:<br>            <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> self.named_parameters():<br>                <span class="hljs-built_in">print</span>(name, param.size())<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        output = &#123;&#125;<br>        <span class="hljs-comment"># get the output of each maxpooling layer (5 maxpool in VGG net)</span><br>        <span class="hljs-keyword">for</span> idx, (begin, end) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.ranges):<br>        <span class="hljs-comment">#self.ranges = ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)) (vgg16 examples)</span><br>            <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(begin, end):<br>                x = self.features[layer](x)<br>            output[<span class="hljs-string">&quot;x%d&quot;</span>%(idx+<span class="hljs-number">1</span>)] = x<br><br>        <span class="hljs-keyword">return</span> output<br><br><br>ranges = &#123;<br>    <span class="hljs-string">&#x27;vgg11&#x27;</span>: ((<span class="hljs-number">0</span>, <span class="hljs-number">3</span>), (<span class="hljs-number">3</span>, <span class="hljs-number">6</span>),  (<span class="hljs-number">6</span>, <span class="hljs-number">11</span>),  (<span class="hljs-number">11</span>, <span class="hljs-number">16</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">21</span>)),<br>    <span class="hljs-string">&#x27;vgg13&#x27;</span>: ((<span class="hljs-number">0</span>, <span class="hljs-number">5</span>), (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>), (<span class="hljs-number">10</span>, <span class="hljs-number">15</span>), (<span class="hljs-number">15</span>, <span class="hljs-number">20</span>), (<span class="hljs-number">20</span>, <span class="hljs-number">25</span>)),<br>    <span class="hljs-string">&#x27;vgg16&#x27;</span>: ((<span class="hljs-number">0</span>, <span class="hljs-number">5</span>), (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>), (<span class="hljs-number">10</span>, <span class="hljs-number">17</span>), (<span class="hljs-number">17</span>, <span class="hljs-number">24</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">31</span>)),<br>    <span class="hljs-string">&#x27;vgg19&#x27;</span>: ((<span class="hljs-number">0</span>, <span class="hljs-number">5</span>), (<span class="hljs-number">5</span>, <span class="hljs-number">10</span>), (<span class="hljs-number">10</span>, <span class="hljs-number">19</span>), (<span class="hljs-number">19</span>, <span class="hljs-number">28</span>), (<span class="hljs-number">28</span>, <span class="hljs-number">37</span>))<br>&#125;<br><br><span class="hljs-comment"># Vgg-Net config </span><br>cfg = &#123;<br>    <span class="hljs-string">&#x27;vgg11&#x27;</span>: [<span class="hljs-number">64</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">128</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>],<br>    <span class="hljs-string">&#x27;vgg13&#x27;</span>: [<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>],<br>    <span class="hljs-string">&#x27;vgg16&#x27;</span>: [<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>],<br>    <span class="hljs-string">&#x27;vgg19&#x27;</span>: [<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>],<br>&#125;<br><br><span class="hljs-comment"># make layers using Vgg-Net config(cfg)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_layers</span>(<span class="hljs-params">cfg, batch_norm=<span class="hljs-literal">False</span></span>):<br>    layers = []<br>    in_channels = <span class="hljs-number">3</span><br>    <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> cfg:<br>        <span class="hljs-keyword">if</span> v == <span class="hljs-string">&#x27;M&#x27;</span>:<br>            layers += [nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)]<br>        <span class="hljs-keyword">else</span>:<br>            conv2d = nn.Conv2d(in_channels, v, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> batch_norm:<br>                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=<span class="hljs-literal">True</span>)]<br>            <span class="hljs-keyword">else</span>:<br>                layers += [conv2d, nn.ReLU(inplace=<span class="hljs-literal">True</span>)]<br>            in_channels = v<br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br></code></pre></td></tr></table></figure><h2 id="UNet-2015"><a href="#UNet-2015" class="headerlink" title="UNet(2015)"></a><br>UNet(2015)</br></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch<br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DoubleConv</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels</span>):<br>        <span class="hljs-built_in">super</span>(DoubleConv, self).__init__()<br>        self.conv = nn.Sequential(<br>                nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), <br>                nn.BatchNorm2d(out_channels),<br>                nn.ReLU(inplace = <span class="hljs-literal">True</span>),<br>                nn.Conv2d(out_channels, out_channels, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>                nn.BatchNorm2d(out_channels),<br>                nn.ReLU(inplace = <span class="hljs-literal">True</span>)  <br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> self.conv(x)<br> <br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">UNet</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>(UNet,self).__init__()<br>        <span class="hljs-comment"># Conv</span><br>        self.conv1 = DoubleConv(in_channels, <span class="hljs-number">64</span>)<br>        self.pool1 = nn.MaxPool2d(<span class="hljs-number">2</span>) <br><br>        self.conv2 = DoubleConv(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>)<br>        self.pool2 = nn.MaxPool2d(<span class="hljs-number">2</span>)<br><br>        self.conv3 = DoubleConv(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>)<br>        self.pool3 = nn.MaxPool2d(<span class="hljs-number">2</span>)<br><br>        self.conv4 = DoubleConv(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>)<br>        self.pool4 = nn.MaxPool2d(<span class="hljs-number">2</span>)<br><br>        self.conv5 = DoubleConv(<span class="hljs-number">512</span>, <span class="hljs-number">1024</span>)<br><br>        <span class="hljs-comment"># DeConv</span><br>        self.up_conv6 = nn.ConvTranspose2d(<span class="hljs-number">1024</span>, <span class="hljs-number">512</span>, <span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        self.conv6 = DoubleConv(<span class="hljs-number">1024</span>, <span class="hljs-number">512</span>)<br><br>        self.up_conv7 = nn.ConvTranspose2d(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        self.conv7 = DoubleConv(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>)<br><br>        self.up_conv8 = nn.ConvTranspose2d(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, <span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        self.conv8 = DoubleConv(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>)<br><br>        self.up_conv9 = nn.ConvTranspose2d(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>, <span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        self.conv9 = DoubleConv(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>)<br>        <br>        self.conv10 = nn.Conv2d(<span class="hljs-number">64</span>, out_channels, <span class="hljs-number">1</span>)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        conv_out_1 = self.conv1(x)<br>        pool_out_1 = self.pool1(conv_out_1)<br>        conv_out_2 = self.conv2(pool_out_1)<br>        pool_out_2 = self.pool2(conv_out_2)<br>        conv_out_3 = self.conv3(pool_out_2)<br>        pool_out_3 = self.pool3(conv_out_3)<br>        conv_out_4 = self.conv4(pool_out_3)<br>        pool_out_4 = self.pool4(conv_out_4)<br>        conv_out_5 = self.conv5(pool_out_4)<br>        up_conv_out_6 = self.up_conv6(conv_out_5)<br>        concate_6 = torch.cat([up_conv_out_6, conv_out_4], dim=<span class="hljs-number">1</span>) <br><br>        conv_out_6 = self.conv6(concate_6)<br>        up_conv_out_7 = self.up_conv7(conv_out_6)<br>        concate_7 = torch.cat([up_conv_out_7, conv_out_3], dim=<span class="hljs-number">1</span>)<br><br>        conv_out_7 = self.conv7(concate_7)<br>        up_conv_out_8 = self.up_conv8(conv_out_7)<br>        concate_8 = torch.cat([up_conv_out_8, conv_out_2], dim=<span class="hljs-number">1</span>)<br><br>        conv_out_8 = self.conv8(concate_8)<br>        up_conv_out_9 = self.up_conv9(conv_out_8)<br>        concate_9 = torch.cat([up_conv_out_9, conv_out_1], dim=<span class="hljs-number">1</span>)<br><br>        conv_out_9 = self.conv9(concate_9)<br>        conv_out_10 = self.conv10(conv_out_9)<br>        out = nn.Sigmoid()(conv_out_10) <br>        <span class="hljs-keyword">return</span> out<br><br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> 图像分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典分类模型的Pytorch实现</title>
      <link href="/2022/07/05/005-DL_Models_Classification/"/>
      <url>/2022/07/05/005-DL_Models_Classification/</url>
      
        <content type="html"><![CDATA[<p>以下所有模型输入为224*224的图片，输出层为2个神经元（即可实现二分类问题, 更改输出神经元个数以实现多分类）</p><h2 id="LeNet-1998"><a href="#LeNet-1998" class="headerlink" title="LeNet(1998)"></a><br>LeNet(1998)</br></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LeNet</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(LeNet,self).__init__()<br>        self.conv1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        )<br>        self.conv2 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>),                  <br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        )<br>        self.conv3 = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>, <span class="hljs-number">120</span>),<br>            nn.ReLU()<br>        )<br>        self.fc2 = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>),<br>            nn.ReLU()<br>        )<br>        self.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = self.conv1(x)<br>        x = self.conv2(x)<br> <br>        x = x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        x = self.conv3(x)<br>        x = self.fc2(x)<br>        x = self.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="AlexNet-2012"><a href="#AlexNet-2012" class="headerlink" title="AlexNet(2012)"></a><br>AlexNet(2012)</br></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">AlexNet</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(AlexNet, self).__init__()<br>        self.features = nn.Sequential(<br>            nn.Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">96</span>, kernel_size=<span class="hljs-number">11</span>, stride=<span class="hljs-number">4</span>, padding=<span class="hljs-number">3</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>            <br>            nn.Conv2d(in_channels=<span class="hljs-number">96</span>, out_channels=<span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>            <br>            nn.Conv2d(in_channels=<span class="hljs-number">256</span>, out_channels=<span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.Conv2d(in_channels=<span class="hljs-number">384</span>, out_channels=<span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.Conv2d(in_channels=<span class="hljs-number">384</span>, out_channels=<span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)<br>        )<br><br>        self.classifier = nn.Sequential(<br>            nn.Dropout(),<br>            nn.Linear(<span class="hljs-number">256</span>*<span class="hljs-number">6</span>*<span class="hljs-number">6</span>, <span class="hljs-number">4096</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.Dropout(),<br>            nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">1024</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">2</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>): <br>        <span class="hljs-comment"># print(x.size())</span><br>        x = self.features(x) <br>        <span class="hljs-comment"># print(x.size())</span><br>        x = x.view(x.size(<span class="hljs-number">0</span>), <span class="hljs-number">256</span>*<span class="hljs-number">6</span>*<span class="hljs-number">6</span>)<br>        x = self.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="VGG-2014"><a href="#VGG-2014" class="headerlink" title="VGG(2014)"></a><br>VGG(2014)</br></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VGG16</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(VGG16, self).__init__()<br>        self.features = nn.Sequential(<br>            nn.Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(in_channels=<span class="hljs-number">64</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br><br>            nn.Conv2d(in_channels=<span class="hljs-number">64</span>, out_channels=<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(in_channels=<span class="hljs-number">128</span>, out_channels=<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br><br>            nn.Conv2d(in_channels=<span class="hljs-number">128</span>, out_channels=<span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(in_channels=<span class="hljs-number">256</span>, out_channels=<span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(in_channels=<span class="hljs-number">256</span>, out_channels=<span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br><br>            nn.Conv2d(in_channels=<span class="hljs-number">256</span>, out_channels=<span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(in_channels=<span class="hljs-number">512</span>, out_channels=<span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(in_channels=<span class="hljs-number">512</span>, out_channels=<span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br><br>            nn.Conv2d(in_channels=<span class="hljs-number">512</span>, out_channels=<span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(in_channels=<span class="hljs-number">512</span>, out_channels=<span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(in_channels=<span class="hljs-number">512</span>, out_channels=<span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        )<br>        self.classifier = nn.Sequential(<br>            nn.Dropout(),<br>            nn.Linear(<span class="hljs-number">512</span>*<span class="hljs-number">3</span>*<span class="hljs-number">3</span>, <span class="hljs-number">4096</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.Dropout(),<br>            nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">2</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>): <br>        <span class="hljs-comment"># print(x.size())</span><br>        x = self.features(x) <br>        <span class="hljs-comment"># print(x.size())</span><br>        x = x.view(x.size(<span class="hljs-number">0</span>), <span class="hljs-number">512</span>*<span class="hljs-number">3</span>*<span class="hljs-number">3</span>)<br>        x = self.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="InceptionV1-2014"><a href="#InceptionV1-2014" class="headerlink" title="InceptionV1(2014)"></a><br>InceptionV1(2014)</br></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InceptionModule</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels_1, reduce_3, out_channels_3, reduce_5, out_channels_5, pool_proj</span>):<br>        <span class="hljs-built_in">super</span>(InceptionModule, self).__init__()<br>        self.branch_1 = nn.Sequential(<br>            nn.Conv2d(in_channels, out_channels_1, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels_1),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>        self.branch_2 = nn.Sequential(<br>            nn.Conv2d(in_channels, reduce_3, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(reduce_3),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(reduce_3, out_channels_3, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels_3),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>        self.branch_3 = nn.Sequential(<br>            nn.Conv2d(in_channels, reduce_5, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(reduce_5),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(reduce_5, out_channels_5, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(out_channels_5),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>        self.branch_4 = nn.Sequential(<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>            nn.Conv2d(in_channels, pool_proj, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(pool_proj),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># print(self.branch_1(x).shape, self.branch_2(x).shape, self.branch_3(x).shape, self.branch_4(x).shape)</span><br>        <span class="hljs-keyword">return</span> torch.cat([self.branch_1(x), self.branch_2(x), self.branch_3(x), self.branch_4(x)], dim=<span class="hljs-number">1</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InceptionV1</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">2</span></span>):<br>        <span class="hljs-built_in">super</span>(InceptionV1, self).__init__()<br>        self.stage1 = nn.Sequential(<br>            nn.Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),<br>            nn.Conv2d(in_channels=<span class="hljs-number">64</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>        self.stage2 = nn.Sequential(<br>            nn.Conv2d(in_channels=<span class="hljs-number">64</span>, out_channels=<span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">192</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>        )<br>        self.stage3 = nn.Sequential(<br>            <span class="hljs-comment"># InceptionModule(in_channels, out_channels_1, reduce_3, out_channels_3, reduce_5, out_channels_5, pool_proj)</span><br>            InceptionModule(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, <span class="hljs-number">96</span>, <span class="hljs-number">128</span>, <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>),<br>            InceptionModule(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">192</span>, <span class="hljs-number">32</span>, <span class="hljs-number">96</span>, <span class="hljs-number">64</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>        )<br>        self.stage4 = nn.Sequential(<br>            InceptionModule(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, <span class="hljs-number">96</span>, <span class="hljs-number">208</span>, <span class="hljs-number">16</span>, <span class="hljs-number">48</span>, <span class="hljs-number">64</span>),<br>            InceptionModule(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, <span class="hljs-number">112</span>, <span class="hljs-number">224</span>, <span class="hljs-number">24</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>),<br>            InceptionModule(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">24</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>),<br>            InceptionModule(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, <span class="hljs-number">144</span>, <span class="hljs-number">288</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>),<br>            InceptionModule(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>        )<br>        self.stage5 = nn.Sequential(<br>            InceptionModule(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>),<br>            InceptionModule(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">48</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>),<br>        )<br>        self.avg_pool = nn.AvgPool2d(kernel_size=<span class="hljs-number">7</span>)<br>        self.classifier = nn.Sequential(<br>            nn.Dropout(<span class="hljs-number">0.4</span>),<br>            nn.Linear(<span class="hljs-number">1024</span>, num_classes)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.stage1(x)<br>        x = self.stage2(x)<br>        x = self.stage3(x)<br>        x = self.stage4(x)<br>        x = self.stage5(x)<br>        x = self.avg_pool(x)<br>        x = x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        x = self.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="ResNet-18-x2F-34-x2F-50-x2F-101-x2F-152-2015"><a href="#ResNet-18-x2F-34-x2F-50-x2F-101-x2F-152-2015" class="headerlink" title="ResNet-18&#x2F;34&#x2F;50&#x2F;101&#x2F;152(2015)"></a><br>ResNet-18&#x2F;34&#x2F;50&#x2F;101&#x2F;152(2015)</br></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResidualBlock</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inchannel, outchannel, stride=<span class="hljs-number">1</span>, shortcut=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(ResidualBlock, self).__init__()<br><br>        <span class="hljs-comment"># left_2 for ResNet-18/34, left_3 for ResNet-50/101/152 </span><br>        self.left_2 = nn.Sequential(<br>            nn.Conv2d(inchannel, outchannel, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(outchannel),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(outchannel, outchannel, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(outchannel)<br>        )<br>        self.left_3 = nn.Sequential(<br>            nn.Conv2d(inchannel, outchannel, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(outchannel),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(outchannel, outchannel, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(outchannel),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(outchannel, outchannel, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(outchannel)<br>        )<br>        self.right = shortcut<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        out = self.left_2(x)<br>        <span class="hljs-built_in">print</span>(out.shape)<br>        resisdual = x <span class="hljs-keyword">if</span> self.right <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> self.right(x)<br>        out += resisdual<br>        <span class="hljs-keyword">return</span> F.relu(out)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResNet34</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">2</span></span>):<br>        <span class="hljs-built_in">super</span>(ResNet34, self).__init__()<br>        self.layer1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">7</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-comment"># ResNet-18: 2,2,2,2  ResNet-34:  3,4,6,3</span><br>        <span class="hljs-comment"># ResNet-50: 3,4,6,3  ResNet-101: 3,4,23,3  ResNet-152: 3,8,36,3</span><br>        self.layer2 = self._make_layer(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>)<br>        self.layer3 = self._make_layer(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>)<br>        self.layer4 = self._make_layer(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">6</span>, stride=<span class="hljs-number">2</span>)<br>        self.layer5 = self._make_layer(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)<br>        self.avg_pool = nn.AvgPool2d(kernel_size=<span class="hljs-number">7</span>)<br>        self.classifier = nn.Linear(<span class="hljs-number">512</span>, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layer</span>(<span class="hljs-params">self, inchannel, outchannel, block_num, stride=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-comment"># 1*1 Conv Projection</span><br>        shortcut = nn.Sequential(<br>            nn.Conv2d(inchannel, outchannel, <span class="hljs-number">1</span>, stride, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(outchannel)<br>        )<br><br>        layers = []<br>        layers.append(ResidualBlock(inchannel, outchannel, stride, shortcut))<br><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, block_num):<br>            layers.append(ResidualBlock(outchannel, outchannel))<br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.layer1(x)<br>        x = self.layer2(x)<br>        x = self.layer3(x)<br>        x = self.layer4(x)<br>        x = self.layer5(x)<br>        x = self.avg_pool(x)<br>        x = x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        x = self.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="MobileNetV1-2017"><a href="#MobileNetV1-2017" class="headerlink" title="MobileNetV1(2017)"></a><br>MobileNetV1(2017)</br></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MobileNetV1</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MobileNetV1, self).__init__()<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_bn</span>(<span class="hljs-params">inp, oup, stride</span>):<br>            <span class="hljs-keyword">return</span> nn.Sequential(<br>                nn.Conv2d(inp, oup, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(oup),<br>                nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>            )<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_dw</span>(<span class="hljs-params">inp, oup, stride</span>):<br>            <span class="hljs-keyword">return</span> nn.Sequential(<br>                nn.Conv2d(inp, inp, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, groups=inp, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(inp),<br>                nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>    <br>                nn.Conv2d(inp, oup, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(oup),<br>                nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            )<br>        self.model = nn.Sequential(<br>            conv_bn(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">2</span>),<br>            conv_dw(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">1</span>),<br>            conv_dw(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">2</span>),<br>            conv_dw(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">1</span>),<br>            conv_dw(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>),<br>            conv_dw(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>),<br>            conv_dw(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">2</span>),<br>            conv_dw(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>),<br>            conv_dw(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>),<br>            conv_dw(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>),<br>            conv_dw(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>),<br>            conv_dw(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>),<br>            conv_dw(<span class="hljs-number">512</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">2</span>),<br>            conv_dw(<span class="hljs-number">1024</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">1</span>),<br>            nn.AvgPool2d(<span class="hljs-number">7</span>),<br>        )<br>        self.fc = nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">2</span>)<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.model(x)<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1024</span>)<br>        x = self.fc(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="MobileNetV2-2018"><a href="#MobileNetV2-2018" class="headerlink" title="MobileNetV2(2018)"></a><br>MobileNetV2(2018)</br></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> math<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_bn</span>(<span class="hljs-params">inp, oup, stride</span>):<br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        nn.Conv2d(inp, oup, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>        nn.BatchNorm2d(oup),<br>        nn.ReLU6(inplace=<span class="hljs-literal">True</span>)<br>    )<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_1x1_bn</span>(<span class="hljs-params">inp, oup</span>):<br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        nn.Conv2d(inp, oup, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>        nn.BatchNorm2d(oup),<br>        nn.ReLU6(inplace=<span class="hljs-literal">True</span>)<br>    )<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_divisible</span>(<span class="hljs-params">x, divisible_by=<span class="hljs-number">8</span></span>):<br>    <span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">int</span>(np.ceil(x * <span class="hljs-number">1.</span> / divisible_by) * divisible_by)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InvertedResidual</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inp, oup, stride, expand_ratio</span>):<br>        <span class="hljs-built_in">super</span>(InvertedResidual, self).__init__()<br>        self.stride = stride<br>        <span class="hljs-keyword">assert</span> stride <span class="hljs-keyword">in</span> [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]<br><br>        hidden_dim = <span class="hljs-built_in">int</span>(inp * expand_ratio)<br>        self.use_res_connect = self.stride == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> inp == oup<br><br>        <span class="hljs-keyword">if</span> expand_ratio == <span class="hljs-number">1</span>:<br>            self.conv = nn.Sequential(<br>                <span class="hljs-comment"># dw</span><br>                nn.Conv2d(hidden_dim, hidden_dim, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, groups=hidden_dim, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(hidden_dim),<br>                nn.ReLU6(inplace=<span class="hljs-literal">True</span>),<br>                <span class="hljs-comment"># pw-linear</span><br>                nn.Conv2d(hidden_dim, oup, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(oup),<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            self.conv = nn.Sequential(<br>                <span class="hljs-comment"># pw</span><br>                nn.Conv2d(inp, hidden_dim, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(hidden_dim),<br>                nn.ReLU6(inplace=<span class="hljs-literal">True</span>),<br>                <span class="hljs-comment"># dw</span><br>                nn.Conv2d(hidden_dim, hidden_dim, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, groups=hidden_dim, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(hidden_dim),<br>                nn.ReLU6(inplace=<span class="hljs-literal">True</span>),<br>                <span class="hljs-comment"># pw-linear</span><br>                nn.Conv2d(hidden_dim, oup, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(oup),<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">if</span> self.use_res_connect:<br>            <span class="hljs-keyword">return</span> x + self.conv(x)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> self.conv(x)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MobileNetV2</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_class=<span class="hljs-number">2</span>, input_size=<span class="hljs-number">224</span>, width_mult=<span class="hljs-number">1.</span></span>):<br>        <span class="hljs-built_in">super</span>(MobileNetV2, self).__init__()<br>        block = InvertedResidual<br>        input_channel = <span class="hljs-number">32</span><br>        last_channel = <span class="hljs-number">1280</span><br>        interverted_residual_setting = [<br>            <span class="hljs-comment"># t, c, n, s</span><br>            [<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">24</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">64</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">96</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">160</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">320</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>        ]<br><br>        <span class="hljs-comment"># building first layer</span><br>        <span class="hljs-keyword">assert</span> input_size % <span class="hljs-number">32</span> == <span class="hljs-number">0</span><br>        <span class="hljs-comment"># input_channel = make_divisible(input_channel * width_mult)  # first channel is always 32!</span><br>        self.last_channel = make_divisible(last_channel * width_mult) <span class="hljs-keyword">if</span> width_mult &gt; <span class="hljs-number">1.0</span> <span class="hljs-keyword">else</span> last_channel<br>        self.features = [conv_bn(<span class="hljs-number">3</span>, input_channel, <span class="hljs-number">2</span>)]<br><br>        <span class="hljs-comment"># building inverted residual blocks</span><br>        <span class="hljs-keyword">for</span> t, c, n, s <span class="hljs-keyword">in</span> interverted_residual_setting:<br>            output_channel = make_divisible(c * width_mult) <span class="hljs-keyword">if</span> t &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> c<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>                <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>:<br>                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))<br>                <span class="hljs-keyword">else</span>:<br>                    self.features.append(block(input_channel, output_channel, <span class="hljs-number">1</span>, expand_ratio=t))<br>                input_channel = output_channel<br>                <br>        <span class="hljs-comment"># building last several layers</span><br>        self.features.append(conv_1x1_bn(input_channel, self.last_channel))<br><br>        <span class="hljs-comment"># make it nn.Sequential</span><br>        self.features = nn.Sequential(*self.features)<br><br>        <span class="hljs-comment"># building classifier</span><br>        self.classifier = nn.Linear(self.last_channel, n_class)<br>        self._initialize_weights()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.features(x)<br>        x = x.mean(<span class="hljs-number">3</span>).mean(<span class="hljs-number">2</span>)<br>        x = self.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_initialize_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>                n = m.kernel_size[<span class="hljs-number">0</span>] * m.kernel_size[<span class="hljs-number">1</span>] * m.out_channels<br>                m.weight.data.normal_(<span class="hljs-number">0</span>, math.sqrt(<span class="hljs-number">2.</span> / n))<br>                <span class="hljs-keyword">if</span> m.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    m.bias.data.zero_()<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.BatchNorm2d):<br>                m.weight.data.fill_(<span class="hljs-number">1</span>)<br>                m.bias.data.zero_()<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>                n = m.weight.size(<span class="hljs-number">1</span>)<br>                m.weight.data.normal_(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>)<br>                m.bias.data.zero_()<br></code></pre></td></tr></table></figure><h2 id="MobileNetV3-2019"><a href="#MobileNetV3-2019" class="headerlink" title="MobileNetV3(2019)"></a><br>MobileNetV3(2019)</br></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable<br><span class="hljs-keyword">import</span> math<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">HardSwish</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inplace=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>(HardSwish, self).__init__()<br>        self.relu6 = nn.ReLU6(inplace=inplace)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> x * (self.relu6(x+<span class="hljs-number">3</span>)) / <span class="hljs-number">6</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">HardSigmoid</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inplace=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>(HardSigmoid, self).__init__()<br>        self.relu6 = nn.ReLU6(inplace=inplace)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> (self.relu6(x+<span class="hljs-number">3</span>)) / <span class="hljs-number">6</span><br><br><br>ACT_FNS = &#123;<br>    <span class="hljs-string">&#x27;RE&#x27;</span>: nn.ReLU6(inplace=<span class="hljs-literal">True</span>),<br>    <span class="hljs-string">&#x27;HS&#x27;</span>: HardSwish(),<br>    <span class="hljs-string">&#x27;HG&#x27;</span>: HardSigmoid()<br>&#125;<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_divisible</span>(<span class="hljs-params">v, divisor, min_value=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-keyword">if</span> min_value <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        min_value = divisor<br>    new_v = <span class="hljs-built_in">max</span>(min_value, <span class="hljs-built_in">int</span>(v + divisor / <span class="hljs-number">2</span>) // divisor * divisor)<br>    <span class="hljs-keyword">if</span> new_v &lt; <span class="hljs-number">0.9</span> * v:<br>        new_v += divisor<br>    <span class="hljs-keyword">return</span> new_v<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_3x3_bn</span>(<span class="hljs-params">inp, oup, stride, nl=<span class="hljs-string">&#x27;RE&#x27;</span></span>):<br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        nn.Conv2d(inp, oup, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>        nn.BatchNorm2d(oup),<br>        ACT_FNS[nl]<br>    )<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_1x1</span>(<span class="hljs-params">inp, oup, nl=<span class="hljs-string">&#x27;RE&#x27;</span>, with_se=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-keyword">if</span> with_se:<br>        <span class="hljs-keyword">return</span> nn.Sequential(<br>            nn.Conv2d(inp, oup, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>            SqueezeAndExcite(oup, reduction=<span class="hljs-number">4</span>),<br>            ACT_FNS[nl]<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> nn.Sequential(<br>            nn.Conv2d(inp, oup, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>            ACT_FNS[nl]<br>        )<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_1x1_bn</span>(<span class="hljs-params">inp, oup, nl=<span class="hljs-string">&#x27;RE&#x27;</span>, with_se=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-keyword">if</span> with_se:<br>        <span class="hljs-keyword">return</span> nn.Sequential(<br>            nn.Conv2d(inp, oup, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>            SqueezeAndExcite(oup, reduction=<span class="hljs-number">4</span>),<br>            nn.BatchNorm2d(oup),<br>            ACT_FNS[nl]<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> nn.Sequential(<br>            nn.Conv2d(inp, oup, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(oup),<br>            ACT_FNS[nl]<br>        )<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SqueezeAndExcite</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_features, reduction=<span class="hljs-number">4</span></span>):<br>        <span class="hljs-built_in">super</span>(SqueezeAndExcite, self).__init__()<br>        <span class="hljs-keyword">if</span> n_features % reduction != <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;n_features must be divisible by reduction (default = 4)&#x27;</span>)<br>        self.linear1 = nn.Linear(n_features, n_features // reduction, bias=<span class="hljs-literal">True</span>)<br>        self.nonlin1 = ACT_FNS[<span class="hljs-string">&#x27;RE&#x27;</span>]<br>        self.linear2 = nn.Linear(n_features // reduction, n_features, bias=<span class="hljs-literal">True</span>)<br>        self.nonlin2 = ACT_FNS[<span class="hljs-string">&#x27;HG&#x27;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        y = F.avg_pool2d(x, kernel_size=x.size()[<span class="hljs-number">2</span>:<span class="hljs-number">4</span>])<br>        y = y.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        y = self.nonlin1(self.linear1(y))<br>        y = self.nonlin2(self.linear2(y))<br>        y = y.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        y = x * y<br>        <span class="hljs-keyword">return</span> y<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InvertedResidual</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inp, oup, kernel, stride, expand_size, nl=<span class="hljs-string">&#x27;RE&#x27;</span>, with_se=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>(InvertedResidual, self).__init__()<br>        <span class="hljs-keyword">assert</span> stride <span class="hljs-keyword">in</span> [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]<br><br>        hidden_dim = expand_size<br><br>        self.identity = stride == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> inp == oup<br><br>        self.pw = nn.Sequential(<br>            nn.Conv2d(inp, hidden_dim, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(hidden_dim),<br>            ACT_FNS[nl],<br>        )<br><br>        self.dw = nn.Sequential(<br>            nn.Conv2d(hidden_dim, hidden_dim, kernel, stride, kernel//<span class="hljs-number">2</span>, groups=hidden_dim, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(hidden_dim),<br>            ACT_FNS[nl],<br>        )<br><br>        self.se = nn.Sequential(<br>            SqueezeAndExcite(hidden_dim, reduction=<span class="hljs-number">4</span>)<br>        )<br><br>        self.pw_linear = nn.Sequential(<br>            nn.Conv2d(hidden_dim, oup, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(oup),<br>        )<br><br>        <span class="hljs-keyword">if</span> with_se: <span class="hljs-comment"># with squeeze and excite </span><br>            <span class="hljs-keyword">if</span> expand_size == oup: <span class="hljs-comment"># exp_ratio = 1</span><br>                self.conv = nn.Sequential(<br>                    self.dw,<br>                    self.se,<br>                    self.pw_linear,<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                self.conv = nn.Sequential(<br>                    self.pw,<br>                    self.dw,<br>                    self.se,<br>                    self.pw_linear,<br>                )<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> expand_size == oup:<br>                self.conv = nn.Sequential(<br>                    self.dw,<br>                    self.pw_linear,<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                self.conv = nn.Sequential(<br>                    self.pw,<br>                    self.dw,<br>                    self.pw_linear,<br>                )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">if</span> self.identity:<br>            <span class="hljs-keyword">return</span> x + self.conv(x)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> self.conv(x)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MobileNetV3</span>(nn.Module):<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> [kernel, expansion, output, SE, NL, s]</span><br>    cfg = [(<span class="hljs-number">3</span>,  <span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-literal">True</span>,  <span class="hljs-string">&#x27;RE&#x27;</span>, <span class="hljs-number">2</span>),<br>           (<span class="hljs-number">3</span>,  <span class="hljs-number">72</span>, <span class="hljs-number">24</span>, <span class="hljs-literal">False</span>, <span class="hljs-string">&#x27;RE&#x27;</span>, <span class="hljs-number">2</span>),  <br>           (<span class="hljs-number">3</span>,  <span class="hljs-number">88</span>, <span class="hljs-number">24</span>, <span class="hljs-literal">False</span>, <span class="hljs-string">&#x27;RE&#x27;</span>, <span class="hljs-number">1</span>),<br>           (<span class="hljs-number">5</span>,  <span class="hljs-number">96</span>, <span class="hljs-number">40</span>, <span class="hljs-literal">True</span>,  <span class="hljs-string">&#x27;HS&#x27;</span>, <span class="hljs-number">2</span>),<br>           (<span class="hljs-number">5</span>, <span class="hljs-number">240</span>, <span class="hljs-number">40</span>, <span class="hljs-literal">True</span>,  <span class="hljs-string">&#x27;HS&#x27;</span>, <span class="hljs-number">1</span>),<br>           (<span class="hljs-number">5</span>, <span class="hljs-number">240</span>, <span class="hljs-number">40</span>, <span class="hljs-literal">True</span>,  <span class="hljs-string">&#x27;HS&#x27;</span>, <span class="hljs-number">1</span>),<br>           (<span class="hljs-number">5</span>, <span class="hljs-number">120</span>, <span class="hljs-number">48</span>, <span class="hljs-literal">True</span>,  <span class="hljs-string">&#x27;HS&#x27;</span>, <span class="hljs-number">1</span>),<br>           (<span class="hljs-number">5</span>, <span class="hljs-number">144</span>, <span class="hljs-number">48</span>, <span class="hljs-literal">True</span>,  <span class="hljs-string">&#x27;HS&#x27;</span>, <span class="hljs-number">1</span>),<br>           (<span class="hljs-number">5</span>, <span class="hljs-number">288</span>, <span class="hljs-number">96</span>, <span class="hljs-literal">True</span>,  <span class="hljs-string">&#x27;HS&#x27;</span>, <span class="hljs-number">2</span>),<br>           (<span class="hljs-number">5</span>, <span class="hljs-number">576</span>, <span class="hljs-number">96</span>, <span class="hljs-literal">True</span>,  <span class="hljs-string">&#x27;HS&#x27;</span>, <span class="hljs-number">1</span>),<br>           (<span class="hljs-number">5</span>, <span class="hljs-number">576</span>, <span class="hljs-number">96</span>, <span class="hljs-literal">True</span>,  <span class="hljs-string">&#x27;HS&#x27;</span>, <span class="hljs-number">1</span>)]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">2</span>, input_size=<span class="hljs-number">224</span>, width_mult=<span class="hljs-number">1.</span></span>):<br>        <span class="hljs-built_in">super</span>(MobileNetV3, self).__init__()<br>        <span class="hljs-comment"># building first layer</span><br>        <span class="hljs-keyword">assert</span> input_size % <span class="hljs-number">32</span> == <span class="hljs-number">0</span><br>        input_channel = _make_divisible(<span class="hljs-number">16</span> * width_mult, <span class="hljs-number">8</span>)<br>        self.conv0 = conv_3x3_bn(<span class="hljs-number">3</span>, input_channel, <span class="hljs-number">2</span>, nl=<span class="hljs-string">&#x27;HS&#x27;</span>)<br>        layers = []<br>        <span class="hljs-comment"># building inverted residual blocks</span><br>        block = InvertedResidual<br>        <span class="hljs-comment"># for t, c, n, s in self.cfgs:</span><br>        <span class="hljs-keyword">for</span> kernel, expansion, output_channel, se, nl, stride <span class="hljs-keyword">in</span> self.cfg:<br>            layers.append(block(input_channel, output_channel, kernel, stride, expansion, nl=nl, with_se=se))<br>            input_channel = output_channel<br>        self.features = nn.Sequential(*layers)<br><br>        <span class="hljs-comment"># building last several layers</span><br>        self.conv1 = conv_1x1_bn(input_channel, expansion, nl=<span class="hljs-string">&#x27;HS&#x27;</span>, with_se=<span class="hljs-literal">False</span>)<br>        input_channel = expansion<br><br>        self.avgpool = nn.AvgPool2d(input_size // <span class="hljs-number">32</span>, stride=<span class="hljs-number">1</span>)<br>        output_channel = _make_divisible(<span class="hljs-number">1280</span> * width_mult, <span class="hljs-number">8</span>) <span class="hljs-keyword">if</span> width_mult &gt; <span class="hljs-number">1.0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1280</span><br>        self.conv2 = conv_1x1(input_channel, output_channel, nl=<span class="hljs-string">&#x27;HS&#x27;</span>, with_se=<span class="hljs-literal">False</span>)<br>        self.classifier = nn.Linear(output_channel, num_classes)<br><br>        self._initialize_weights()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv0(x)<br>        x = self.features(x)<br>        x = self.conv1(x)<br>        x = self.avgpool(x)<br>        x = self.conv2(x)<br>        x = x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        x = self.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_initialize_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>                n = m.kernel_size[<span class="hljs-number">0</span>] * m.kernel_size[<span class="hljs-number">1</span>] * m.out_channels<br>                m.weight.data.normal_(<span class="hljs-number">0</span>, math.sqrt(<span class="hljs-number">2.</span> / n))<br>                <span class="hljs-keyword">if</span> m.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    m.bias.data.zero_()<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.BatchNorm2d):<br>                m.weight.data.fill_(<span class="hljs-number">1</span>)<br>                m.bias.data.zero_()<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>                n = m.weight.size(<span class="hljs-number">1</span>)<br>                m.weight.data.normal_(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>)<br>                m.bias.data.zero_()<br><br></code></pre></td></tr></table></figure><h2 id="ShuffleNetV2-2019"><a href="#ShuffleNetV2-2019" class="headerlink" title="ShuffleNetV2(2019)"></a><br>ShuffleNetV2(2019)</br></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> init<br><span class="hljs-keyword">import</span> math<br><br><br><span class="hljs-comment"># model = ShuffleNetV2()</span><br><span class="hljs-comment"># input: 3*?*?</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_bn</span>(<span class="hljs-params">inp, oup, stride</span>):<br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        nn.Conv2d(inp, oup, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>        nn.BatchNorm2d(oup),<br>        nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>    )<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_1x1_bn</span>(<span class="hljs-params">inp, oup</span>):<br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        nn.Conv2d(inp, oup, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>        nn.BatchNorm2d(oup),<br>        nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>    )<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">channel_shuffle</span>(<span class="hljs-params">x, groups</span>):<br>    batchsize, num_channels, height, width = x.data.size()<br><br>    channels_per_group = num_channels // groups<br>    <br>    <span class="hljs-comment"># reshape</span><br>    x = x.view(batchsize, groups, <br>        channels_per_group, height, width)<br><br>    x = torch.transpose(x, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous()<br><br>    <span class="hljs-comment"># flatten</span><br>    x = x.view(batchsize, -<span class="hljs-number">1</span>, height, width)<br><br>    <span class="hljs-keyword">return</span> x<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InvertedResidual</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inp, oup, stride, benchmodel</span>):<br>        <span class="hljs-built_in">super</span>(InvertedResidual, self).__init__()<br>        self.benchmodel = benchmodel<br>        self.stride = stride<br>        <span class="hljs-keyword">assert</span> stride <span class="hljs-keyword">in</span> [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]<br><br>        oup_inc = oup//<span class="hljs-number">2</span><br>        <br>        <span class="hljs-keyword">if</span> self.benchmodel == <span class="hljs-number">1</span>:<br>            <span class="hljs-comment">#assert inp == oup_inc</span><br>        self.banch2 = nn.Sequential(<br>                <span class="hljs-comment"># pw</span><br>                nn.Conv2d(oup_inc, oup_inc, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(oup_inc),<br>                nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>                <span class="hljs-comment"># dw</span><br>                nn.Conv2d(oup_inc, oup_inc, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, groups=oup_inc, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(oup_inc),<br>                <span class="hljs-comment"># pw-linear</span><br>                nn.Conv2d(oup_inc, oup_inc, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(oup_inc),<br>                nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            )                <br>        <span class="hljs-keyword">else</span>:                  <br>            self.banch1 = nn.Sequential(<br>                <span class="hljs-comment"># dw</span><br>                nn.Conv2d(inp, inp, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, groups=inp, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(inp),<br>                <span class="hljs-comment"># pw-linear</span><br>                nn.Conv2d(inp, oup_inc, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(oup_inc),<br>                nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            )        <br>    <br>            self.banch2 = nn.Sequential(<br>                <span class="hljs-comment"># pw</span><br>                nn.Conv2d(inp, oup_inc, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(oup_inc),<br>                nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>                <span class="hljs-comment"># dw</span><br>                nn.Conv2d(oup_inc, oup_inc, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, groups=oup_inc, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(oup_inc),<br>                <span class="hljs-comment"># pw-linear</span><br>                nn.Conv2d(oup_inc, oup_inc, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(oup_inc),<br>                nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            )<br>          <br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_concat</span>(<span class="hljs-params">x, out</span>):<br>        <span class="hljs-comment"># concatenate along channel axis</span><br>        <span class="hljs-keyword">return</span> torch.cat((x, out), <span class="hljs-number">1</span>)        <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-number">1</span>==self.benchmodel:<br>            x1 = x[:, :(x.shape[<span class="hljs-number">1</span>]//<span class="hljs-number">2</span>), :, :]<br>            x2 = x[:, (x.shape[<span class="hljs-number">1</span>]//<span class="hljs-number">2</span>):, :, :]<br>            out = self._concat(x1, self.banch2(x2))<br>        <span class="hljs-keyword">elif</span> <span class="hljs-number">2</span>==self.benchmodel:<br>            out = self._concat(self.banch1(x), self.banch2(x))<br><br>        <span class="hljs-keyword">return</span> channel_shuffle(out, <span class="hljs-number">2</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ShuffleNetV2</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_class=<span class="hljs-number">1000</span>, input_size=<span class="hljs-number">224</span>, width_mult=<span class="hljs-number">1.</span></span>):<br>        <span class="hljs-built_in">super</span>(ShuffleNetV2, self).__init__()<br>        <br>        <span class="hljs-keyword">assert</span> input_size % <span class="hljs-number">32</span> == <span class="hljs-number">0</span><br>        <br>        self.stage_repeats = [<span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">4</span>]<br>        <span class="hljs-comment"># index 0 is invalid and should never be called.</span><br>        <span class="hljs-comment"># only used for indexing convenience.</span><br>        <span class="hljs-keyword">if</span> width_mult == <span class="hljs-number">0.5</span>:<br>            self.stage_out_channels = [-<span class="hljs-number">1</span>, <span class="hljs-number">24</span>,  <span class="hljs-number">48</span>,  <span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">1024</span>]<br>        <span class="hljs-keyword">elif</span> width_mult == <span class="hljs-number">1.0</span>:<br>            self.stage_out_channels = [-<span class="hljs-number">1</span>, <span class="hljs-number">24</span>, <span class="hljs-number">116</span>, <span class="hljs-number">232</span>, <span class="hljs-number">464</span>, <span class="hljs-number">1024</span>]<br>        <span class="hljs-keyword">elif</span> width_mult == <span class="hljs-number">1.5</span>:<br>            self.stage_out_channels = [-<span class="hljs-number">1</span>, <span class="hljs-number">24</span>, <span class="hljs-number">176</span>, <span class="hljs-number">352</span>, <span class="hljs-number">704</span>, <span class="hljs-number">1024</span>]<br>        <span class="hljs-keyword">elif</span> width_mult == <span class="hljs-number">2.0</span>:<br>            self.stage_out_channels = [-<span class="hljs-number">1</span>, <span class="hljs-number">24</span>, <span class="hljs-number">224</span>, <span class="hljs-number">488</span>, <span class="hljs-number">976</span>, <span class="hljs-number">2048</span>]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<br>                <span class="hljs-string">&quot;&quot;&quot;&#123;&#125; groups is not supported for</span><br><span class="hljs-string">                       1x1 Grouped Convolutions&quot;&quot;&quot;</span>.<span class="hljs-built_in">format</span>(num_groups))<br><br>        <span class="hljs-comment"># building first layer</span><br>        input_channel = self.stage_out_channels[<span class="hljs-number">1</span>]<br>        self.conv1 = conv_bn(<span class="hljs-number">3</span>, input_channel, <span class="hljs-number">2</span>)<br>        self.maxpool = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>        self.features = []<br><br>        <span class="hljs-comment"># building inverted residual blocks</span><br>        <span class="hljs-keyword">for</span> idxstage <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(self.stage_repeats)):<br>            numrepeat = self.stage_repeats[idxstage]<br>            output_channel = self.stage_out_channels[idxstage+<span class="hljs-number">2</span>]<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(numrepeat):<br>                <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>:<br>            <span class="hljs-comment">#inp, oup, stride, benchmodel):</span><br>                    self.features.append(InvertedResidual(input_channel, output_channel, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br>                <span class="hljs-keyword">else</span>:<br>                    self.features.append(InvertedResidual(input_channel, output_channel, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>                input_channel = output_channel<br>                <br>                <br>        <span class="hljs-comment"># make it nn.Sequential</span><br>        self.features = nn.Sequential(*self.features)<br><br>        <span class="hljs-comment"># building last several layers</span><br>        self.conv_last = conv_1x1_bn(input_channel, self.stage_out_channels[-<span class="hljs-number">1</span>])<br>        self.globalpool = nn.Sequential(nn.AvgPool2d(<span class="hljs-built_in">int</span>(input_size/<span class="hljs-number">32</span>)))<br><br>        <span class="hljs-comment"># building classifier</span><br>        self.classifier = nn.Sequential(nn.Linear(self.stage_out_channels[-<span class="hljs-number">1</span>], n_class))<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv1(x)<br>        x = self.maxpool(x)<br>        x = self.features(x)<br>        x = self.conv_last(x)<br>        x = self.globalpool(x)<br>        x = x.view(-<span class="hljs-number">1</span>, self.stage_out_channels[-<span class="hljs-number">1</span>])<br>        x = self.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 图像分类 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>（持续更新中...）认真地做一个“马来人”——深度学习部分</title>
      <link href="/2022/07/05/004-MarkMan_DL/"/>
      <url>/2022/07/05/004-MarkMan_DL/</url>
      
        <content type="html"><![CDATA[<h1 id="代码常用"><a href="#代码常用" class="headerlink" title="代码常用"></a><strong>代码常用</strong></h1><h2 id="Pytorch框架下常见问题"><a href="#Pytorch框架下常见问题" class="headerlink" title="Pytorch框架下常见问题"></a>Pytorch框架下常见问题</h2><ul><li><p><strong>Pytorch中Tensor与各种图像格式的相互转化</strong><br><a href="https://blog.csdn.net/qq_36955294/article/details/82888443">https://blog.csdn.net/qq_36955294/article/details/82888443</a></p></li><li><p><strong>Pytorch查看模型细节和参数大小计算（利用torchsummary模块）</strong></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">from torchsummary import summary<br>summary(model, input_size=(C, H, W))<br><br>以UNet为例<br>from torchsummary import summary<br>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)<br>model = UNet(in_channels=3).to(device)<br>print(model)                                                               # 打印整个模型框架<br>parameters = filter(lambda p: p.requires_grad, model.parameters())         # 过滤可训练参数<br>parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000      # 参数大小估计<br>print(&#x27;Trainable Parameters: %.3fM&#x27; % parameters)<br>summary(model, input_size=(C, H, W))  # [B, num_classes] or [B, C, H, W]   # 测试模型输出<br></code></pre></td></tr></table></figure><ul><li><strong>使用torchsummary时的常见问题</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">报错：AttributeError: &#x27;list&#x27; object has no attribute &#x27;size&#x27;<br>原因：传入的是list属性，而不是torch.Tensor（具体原因未知）<br>解决办法：在torchsummary.py中加条件判断一下即可，在Line 23左右的位置<br># 这是源码<br>summary[m_key][&quot;output_shape&quot;] = [<br>    [-1] + list(o.size())[1:] for o in output<br>]<br><br># 以下为报错后修改的代码<br>summary[m_key][&quot;output_shape&quot;] = [<br>    [-1] + list(o.size())[1:] if isinstance(o, torch.Tensor) else [-1] + list(np.array(o).shape)[1:] for o in output<br>]<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">报错：IndexError: tuple index out of range<br>原因：传入的input为None<br>解决办法：添加判断input是否为None即可<br># 这是源码<br>summary[m_key][&quot;input_shape&quot;] = list(input[0].size())<br>summary[m_key][&quot;input_shape&quot;][0] = batch_size<br><br># 以下为报错后修改的代码<br>if len(input) != 0:<br>    summary[m_key][&quot;input_shape&quot;] = list(input[0].size())<br>    summary[m_key][&quot;input_shape&quot;][0] = batch_size<br>else:<br>    summary[m_key][&quot;input_shape&quot;] = input<br><br></code></pre></td></tr></table></figure><ul><li><p><strong>重写Dataset类</strong><br>所有子类应该override <strong>len</strong> 和 __getitem__，前者提供了数据集的大小，后者支持整数索引，范围从0到len(self)</p></li><li><p><strong>pytorch实用方法（持续更新）</strong></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">torch.ones_like(img) # 生成与img形状相同的全1矩阵，下同<br>torch.zeros_like(img)<br>with torch.no_grad(): .... # 不构图，即不参与正向/反向传播<br></code></pre></td></tr></table></figure><ul><li><strong>pytorch模型保存与加载（保存为.pt or .pth）</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">方式一：保存整个网络结构信息和模型参数信息:<br>＃　保存：　torch.save(model_object, &#x27;./model.pth&#x27;)<br>＃　加载：　model = torch.load(&#x27;./model.pth&#x27;)<br><br>方式二：只保存网络的模型参数-推荐使用<br>＃　保存：　torch.save(model_object.state_dict(), &#x27;./params.pth&#x27;)<br>＃　加载：　from models import AgeModel<br>　　　　　　model = AgeModel()<br>　　　　　　model.load_state_dict(torch.load(&#x27;./params.pth&#x27;))<br></code></pre></td></tr></table></figure><h2 id="Numpy矩阵运算常见问题"><a href="#Numpy矩阵运算常见问题" class="headerlink" title="Numpy矩阵运算常见问题"></a>Numpy矩阵运算常见问题</h2><ul><li>Numpy使用np.vstack()和np.hstack()拼接矩阵时匹配维度出错</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">报错：ValueError: all the inputs arrays must have same number of dimensions<br>解决方法：更换np.vstack()和np.hstack()<br>np.vstack((a, b)) -&gt; np.row_stack((a, b))<br>np.hstack((a, b)) -&gt; np.column_stack((a, b))<br></code></pre></td></tr></table></figure><h2 id="Opencv-python常见问题"><a href="#Opencv-python常见问题" class="headerlink" title="Opencv-python常见问题"></a>Opencv-python常见问题</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">报错：cv2.error: OpenCV(4.5.2) /tmp/pip-req-build-lnehsv18/opencv/modules/imgcodecs/src/loadsave.cpp:650: error: (-2:Unspecified error) could not find a writer for the specified extension in function &#x27;imwrite_&#x27;<br>解决：保存图片记得加上拓展名 cv2.imwrite(&#x27;img.png&#x27;, img)<br></code></pre></td></tr></table></figure><h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><ul><li><strong>批量升级python所有包（模块）</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"># 桌面创建python文件upgrade.py, 将如下代码拷进去，保存。 使用命令行 python upgrade.py 运行<br>import pip<br>import time<br>from pip._internal.utils.misc import get_installed_distributions<br>from subprocess import call<br><br>for dist in get_installed_distributions():<br>    print(dist.project_name)<br>print(&#x27;----------&#x27;)<br><br>for dist in get_installed_distributions():<br>    print(&quot;updating:&quot;, dist.project_name, &quot;\t&quot;)#print log<br>    print(time.asctime( time.localtime(time.time()) ))#print log<br>    call(&quot;pip3 install --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple &quot; + dist.project_name, shell=True)#调用更新命令<br></code></pre></td></tr></table></figure><ul><li><strong>导出当前环境&#x2F;目录所需包，以及按照requirements.txt批量安装指定的包（模块）</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">(1) 导出当前环境安装的所有类库（推荐）<br>pip freeze &gt; requirements.txt<br>(2) 导出当前项目目录安装的所有类库（推荐）<br>pip install pipreqs $ pipreqs /path/to/project<br>(3) 按照requirements.txt批量安装类库<br>pip install -i requirements.txt<br></code></pre></td></tr></table></figure><ul><li><strong>忽略警告信息</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import warnings<br>warnings.filterwarnings(&quot;ignore&quot;)<br></code></pre></td></tr></table></figure><ul><li><strong>利用代码批量移动文件</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import os<br>cmd = &quot;mv &#123;&#125; &#123;&#125;&quot;.format(src_dir, dst_dir) # src_dir可以利用os.listdir()获取<br>os.system(cmd)<br></code></pre></td></tr></table></figure><ul><li><strong>Tensorboard、Jupyter notebook和Visdom运行端口的指定</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"># Tensorboard<br>终端打开端口 tensorboard --logdir ./ --port=1234<br><br># Jupyter notebook<br>终端打开端口 jupyter notebook --port=1234<br><br># Visdom<br>代码中创建环境 vis = visdom.Visdom(env=&#x27;name&#x27;, port=1234)<br>终端打开端口 python -m visdom.server -port=1234<br></code></pre></td></tr></table></figure><h1 id="VSCode使用过程中的问题"><a href="#VSCode使用过程中的问题" class="headerlink" title="VSCode使用过程中的问题"></a><strong>VSCode使用过程中的问题</strong></h1><ul><li><strong>SSH远程连接服务器一直弹窗输密码</strong><br>  解决办法1：干掉重来<br>  <a href="https://blog.csdn.net/Mr_Cat123/article/details/107432070">https://blog.csdn.net/Mr_Cat123/article/details/107432070</a><br>  解决办法2：干掉.vscode-server</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">rm -rf .vscode-server<br></code></pre></td></tr></table></figure><ul><li><strong>SSH远程连接时Downloading with wget…超时</strong><br>  解决办法：干掉服务器上的.wget-hsts再重新连接<br>  <img src="https://i.loli.net/2021/07/05/4bi17ZzLHnGsVR2.png" alt="Downloading with wget超时解决办法"></li><li><strong>服务器重装系统后，HostKey发生改变，VSCode直接连的话连不上</strong><br>  解决办法：把本地的.ssh文件夹中的known_hosts文件删掉即可，再次重连服务器会自动生成新的known_hosts文件<br>  （known_hosts文件路径：C:\Users\lenovo.ssh\known_hosts）</li><li><strong>VSCode配置Python调试（Debug）环境</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">（1）添加配置（Add Configuration）<br>（2）将以下代码拷入launch.json即可<br>注：服务器上配置无需指明Python路径，在左下角选择Python解释器即可<br>&#123;<br>    // Use IntelliSense to learn about possible attributes.<br>    // Hover to view descriptions of existing attributes.<br>    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387<br>    &quot;version&quot;: &quot;0.2.0&quot;,<br>    &quot;configurations&quot;: [<br>        &#123;<br>            &quot;name&quot;: &quot;Python: Current File&quot;,<br>            &quot;type&quot;: &quot;python&quot;,<br>            &quot;request&quot;: &quot;launch&quot;,<br>            &quot;stopOnEntry&quot;: false,<br>            &quot;program&quot;: &quot;$&#123;file&#125;&quot;,<br>            &quot;cwd&quot;: &quot;$&#123;workspaceRoot&#125;&quot;,<br>            &quot;env&quot;: &#123;&#125;,<br>            &quot;envFile&quot;: &quot;$&#123;workspaceRoot&#125;/.env&quot;<br>        &#125;<br>    ]<br>&#125;<br></code></pre></td></tr></table></figure><ul><li><strong>VSCode切换中英文界面</strong><br>1）使用快捷键组合【Ctrl+Shift+p】，在搜索框中输入“configure display language”，点击确定后；<br>2）点击zh-CN&#x2F;en，或者直接修改locale.json文件下的属性“locale”为“zh-CN”;<br>3）重启vscode即可；</li></ul><h1 id="Jupyter-Notebook插件配置"><a href="#Jupyter-Notebook插件配置" class="headerlink" title="Jupyter Notebook插件配置"></a><strong>Jupyter Notebook插件配置</strong></h1><ul><li><strong>插件库安装及配置</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">（1）安装nbextensions插件<br>pip install jupyter_contrib_nbextensions<br>（2）配置（注：确保已关闭 Jupyter Notebook）<br>jupyter contrib nbextension install --user --skip-running-check<br>（3）启动Jupyter Notebook，在Nbextensions的众多选项中勾选Hinterland，即可<br></code></pre></td></tr></table></figure><ul><li><strong>必用插件</strong><br>（1）代码块折叠 Codefolding<br>（2）显示运行时间 ExecuteTime<br>（3）变量高亮显示 Highlight selected word<br>（4）代码提示 Hinterland<br>（5）代码行号显示 Toggle all line numbers</li></ul><h2 id="Linux服务器常用指令"><a href="#Linux服务器常用指令" class="headerlink" title="Linux服务器常用指令"></a><b>Linux服务器常用指令</b></h2><ul><li><strong>压缩包解压命令</strong><br><b>*.tar文件</b></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">tar -xvf filename.tar<br></code></pre></td></tr></table></figure><p><b>*.tar.gz和*.tgz文件</b></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">tar -xzvf filename.tar.gz<br></code></pre></td></tr></table></figure><p><b>*.gz文件</b></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">gunzip filename.gz<br>gzip -d filename.gz<br></code></pre></td></tr></table></figure><p><b>*.bz2文件</b></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">bunzip2 filename.bz2<br>bzip2 -d filename.bz2<br></code></pre></td></tr></table></figure><p><b>*.tar.bz2文件</b></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">tar jxvf filename.tar.bz2<br>tar --bzip xvf filename.tar.bz2<br></code></pre></td></tr></table></figure><p><b>*.zip文件</b></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">unzip filename.zip<br></code></pre></td></tr></table></figure><p><b>*.rar文件</b></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">unrar e filename.rar<br></code></pre></td></tr></table></figure><ul><li><strong>查看磁盘占用情况</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">du -h --max-depth=0 *<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 备忘录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实验室集群环境配置（包括Anaconda环境搭建、Pytorch框架的安装和Jupyter Notebook的配置）</title>
      <link href="/2022/07/05/003-ServerEnvironmentConfig/"/>
      <url>/2022/07/05/003-ServerEnvironmentConfig/</url>
      
        <content type="html"><![CDATA[<h2 id="Anaconda环境搭建"><a href="#Anaconda环境搭建" class="headerlink" title="Anaconda环境搭建"></a><b>Anaconda环境搭建</b></h2><h3 id="1-复制压缩包"><a href="#1-复制压缩包" class="headerlink" title="1 复制压缩包"></a>1 复制压缩包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">home/xx <span class="hljs-built_in">cp</span> Anaconda3-5.2.0-Linux-x86_64.sh  home/xx<br>(也可直接手动复制粘贴文件)<br></code></pre></td></tr></table></figure><h3 id="2-安装"><a href="#2-安装" class="headerlink" title="2 安装"></a>2 安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash Anaconda3-5.2.0-Linux-x86_64.sh<br></code></pre></td></tr></table></figure><h3 id="3-添加到到PATH"><a href="#3-添加到到PATH" class="headerlink" title="3 添加到到PATH"></a>3 添加到到PATH</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br>vi ~/.bashrc(查看PATH)<br></code></pre></td></tr></table></figure><h2 id="Pytorch框架和虚拟环境安装（在Anaconda-Base下）"><a href="#Pytorch框架和虚拟环境安装（在Anaconda-Base下）" class="headerlink" title="Pytorch框架和虚拟环境安装（在Anaconda Base下）"></a><b>Pytorch框架和虚拟环境安装（在Anaconda Base下）</b></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> activate base<br></code></pre></td></tr></table></figure><h3 id="1-查看CUDA版本"><a href="#1-查看CUDA版本" class="headerlink" title="1 查看CUDA版本"></a>1 查看CUDA版本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvcc -V<br></code></pre></td></tr></table></figure><h3 id="2-官网pytorch-org查看pytorch安装指令"><a href="#2-官网pytorch-org查看pytorch安装指令" class="headerlink" title="2 官网pytorch.org查看pytorch安装指令"></a>2 官网pytorch.org查看pytorch安装指令</h3><p>根据所在结点的CUDA版本，安装对应支持的pytorch版本，直接拷贝如下代码即可安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># CUDA 9.0</span><br>conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=9.0 -c pytorch<br><span class="hljs-comment"># CUDA 11.0+</span><br>pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html<br><br></code></pre></td></tr></table></figure><h3 id="3-检查是否安装成功"><a href="#3-检查是否安装成功" class="headerlink" title="3 检查是否安装成功"></a>3 检查是否安装成功</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br>torch.cuda.is_available()<br>&gt;&gt;&gt;<span class="hljs-literal">True</span><br>(输出<span class="hljs-literal">True</span>代表安装成功)<br></code></pre></td></tr></table></figure><h3 id="4-创建conda虚拟环境-可以不安装"><a href="#4-创建conda虚拟环境-可以不安装" class="headerlink" title="4 创建conda虚拟环境(可以不安装)"></a>4 创建conda虚拟环境(可以不安装)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda create -n name python=3.X<br></code></pre></td></tr></table></figure><h3 id="5-退出conda环境"><a href="#5-退出conda环境" class="headerlink" title="5 退出conda环境"></a>5 退出conda环境</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> deactivate<br></code></pre></td></tr></table></figure><h2 id="集群实时进程查看"><a href="#集群实时进程查看" class="headerlink" title="集群实时进程查看"></a><b>集群实时进程查看</b></h2><h3 id="1-nvidia-smi命令"><a href="#1-nvidia-smi命令" class="headerlink" title="1 nvidia-smi命令"></a>1 nvidia-smi命令</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">用于查看GPU使用情况，其中GPU Memory Usage决定了你是否还能挤一挤<br>使用 nvidia-smi -l 可实时刷新（每隔五秒）<br></code></pre></td></tr></table></figure><h3 id="2-htop命令"><a href="#2-htop命令" class="headerlink" title="2 htop命令"></a>2 htop命令</h3><p><img src="https://i.loli.net/2021/07/05/8v6yGuxiVkdegwJ.png" alt="htop"><br>其中各字段含义：<br>（1）红线上方从左至右、从上至下为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">左边部分为：cpu、内存、交换分区的使用情况；<br>右边部分为：Tasks为进程总数(87)，线程总数(400)当前运行的进程数、<br>Load average为系统1/5/10分钟的平均负载情况、Uptime为系统运行的时间。<br></code></pre></td></tr></table></figure><p>（2）红线下方各字段的意义如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">PID：进行的标识号<br>USER：运行此进程的用户<br>PRI：进程的优先级<br>NI：进程的优先级别值，默认的为0，可以进行调整, 越小优先级越高，最小-20，最大20（用户设置最大19）<br>VIRT：进程占用的虚拟内存值<br>RES：进程占用的物理内存值<br>SHR：进程占用的共享内存值<br>S：进程的运行状况，R（Running，进程正在运行）、S（Sleep，进程休眠）、T（Traced，进程停止）、Z（Zombie，僵尸进程）和D（Disk sleep，硬盘休眠）<br>CPU%：该进程占用的CPU使用率<br>MEM%：该进程占用的物理内存和总内存的百分比<br>TIME+：该进程启动后占用的总的CPU时间<br>COMMAND：进程启动的启动命令名称<br></code></pre></td></tr></table></figure><h2 id="Jupyter-notebook配置"><a href="#Jupyter-notebook配置" class="headerlink" title="Jupyter notebook配置"></a><b>Jupyter notebook配置</b></h2><h3 id="1-创建默认config配置文件"><a href="#1-创建默认config配置文件" class="headerlink" title="1 创建默认config配置文件"></a>1 创建默认config配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">jupyter notebook --generate-config<br></code></pre></td></tr></table></figure><h3 id="2-生成访问密码-新开一个终端，根目录下输入ipython进入编译"><a href="#2-生成访问密码-新开一个终端，根目录下输入ipython进入编译" class="headerlink" title="2 生成访问密码(新开一个终端，根目录下输入ipython进入编译)"></a>2 生成访问密码(新开一个终端，根目录下输入ipython进入编译)</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">In[1]: from notebook.auth import passwd<br>In[2]: passwd()<br>Enter: XXXXXX<br>Verify: XXXXXX<br>Out[2]: sha1: &#x27;xxxxxxxxxxxxxxxx&#x27;<br>(窗口暂时不要关闭，将引号内的字符串内容拷贝出来，下一步会用到)<br></code></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/07/05/PKaOJh7fB2tHijE.png"></p><h3 id="3-修改配置文件jupyter-notebook-config-py"><a href="#3-修改配置文件jupyter-notebook-config-py" class="headerlink" title="3 修改配置文件jupyter_notebook_config.py"></a>3 修改配置文件jupyter_notebook_config.py</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">设置所有ip可访问<br>Line204: c.NotebookApp.ip=&#x27;*&#x27;<br>设置禁止自动打开浏览器<br>Line272: c.NotebookApp.open_browser = False <br>设置访问密码<br>Line281: c.NotebookApp.password = u&#x27;xxxxxxxxxxxxxxxx&#x27; （将上一步中拷贝过来的字符串放入引号中即可）<br>设置端口号，可自定义，尽量不要设置8888，可能会和其他人冲突<br>Line292: c.NotebookApp.port = 1234  <br></code></pre></td></tr></table></figure><h3 id="4-访问"><a href="#4-访问" class="headerlink" title="4 访问"></a>4 访问</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">在环境下输入jupyter notebook<br>打开浏览器地址栏输入: http://ip:1234  <br></code></pre></td></tr></table></figure><h3 id="5-插件安装"><a href="#5-插件安装" class="headerlink" title="5 插件安装"></a>5 插件安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">python -m pip install jupyter_contrib_nbextensions<br>jupyter contrib nbextensions install --user --skip-running-check<br></code></pre></td></tr></table></figure><p>安装完成后Jupyter页面会出现第四栏Nbextensions，勾选相应插件即可使用！<br>常用插件：</p><ul><li>变量高亮 Highlight selected word</li><li>代码块折叠 Codefolding</li><li>程序执行时间 Execute time</li><li>代码提示 Hinterland<br>等等……<br><img src="https://i.loli.net/2021/07/14/4vCz8BYl1NtPViu.png"></li></ul><h2 id="补充：VS-Code配置：使用VS-Code-SSH进行远程开发"><a href="#补充：VS-Code配置：使用VS-Code-SSH进行远程开发" class="headerlink" title="补充：VS Code配置：使用VS Code + SSH进行远程开发"></a><b>补充：VS Code配置：使用VS Code + SSH进行远程开发</b></h2><p>PS. 只是因为我喜欢用VS Code写代码（教程太长，甩链接）<br>Part1. <a href="https://blog.csdn.net/Mculover666/article/details/90439669">https://blog.csdn.net/Mculover666/article/details/90439669</a><br>（看这个足够了）<br>Part2. <a href="https://blog.csdn.net/maybeYoc/article/details/89979223">https://blog.csdn.net/maybeYoc/article/details/89979223</a><br>（备用）</p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 环境配置 </tag>
            
            <tag> Conda </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Jupyter </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习算法中的Hello-World：用LeNet模型实现手写数字识别</title>
      <link href="/2022/07/05/002-LeNet_Mnist/"/>
      <url>/2022/07/05/002-LeNet_Mnist/</url>
      
        <content type="html"><![CDATA[<h2 id="一、Mnist数据集介绍"><a href="#一、Mnist数据集介绍" class="headerlink" title="一、Mnist数据集介绍"></a>一、Mnist数据集介绍</h2><p>数据集的内容：包含0-9的手写数字<br>数据集的数量：60000个训练集&#x2F;10000个测试集<br>数据集的格式：28*28<br>数据集通道数：灰度图（单通道通道）</p><h2 id="二、LeNet七层模型"><a href="#二、LeNet七层模型" class="headerlink" title="二、LeNet七层模型"></a>二、LeNet七层模型</h2><h3 id="1、C1卷积层：6个卷积核，大小为5-5，激活函数ReLu"><a href="#1、C1卷积层：6个卷积核，大小为5-5，激活函数ReLu" class="headerlink" title="1、C1卷积层：6个卷积核，大小为5*5，激活函数ReLu"></a>1、C1卷积层：6个卷积核，大小为5*5，激活函数ReLu</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">model.add(Conv2D(<span class="hljs-number">6</span>, kernel_size = (<span class="hljs-number">5</span>,<span class="hljs-number">5</span>), activation = <span class="hljs-string">&#x27;relu&#x27;</span>, input_shape = (<span class="hljs-number">28</span>,<span class="hljs-number">28</span>,<span class="hljs-number">1</span>)))<br></code></pre></td></tr></table></figure><h3 id="2、S2池化层：最大池化"><a href="#2、S2池化层：最大池化" class="headerlink" title="2、S2池化层：最大池化"></a>2、S2池化层：最大池化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">model.add(MaxPooling2D(pool_size = (<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)))<br></code></pre></td></tr></table></figure><h3 id="3、C3卷积层：16个卷积核，大小为5-5，激活函数ReLu"><a href="#3、C3卷积层：16个卷积核，大小为5-5，激活函数ReLu" class="headerlink" title="3、C3卷积层：16个卷积核，大小为5*5，激活函数ReLu"></a>3、C3卷积层：16个卷积核，大小为5*5，激活函数ReLu</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">model.add(Conv2D(<span class="hljs-number">16</span>, kernel_size = (<span class="hljs-number">5</span>,<span class="hljs-number">5</span>), activation = <span class="hljs-string">&#x27;relu&#x27;</span>))<br></code></pre></td></tr></table></figure><h3 id="4、S4池化层：最大池化"><a href="#4、S4池化层：最大池化" class="headerlink" title="4、S4池化层：最大池化"></a>4、S4池化层：最大池化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">model.add(MaxPooling2D(pool_size = (<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)))<br></code></pre></td></tr></table></figure><h3 id="5、C5全连接层：参数扁平化，在LeNet5称之为卷积层，实际上这一层是一维向量，和全连接层一样"><a href="#5、C5全连接层：参数扁平化，在LeNet5称之为卷积层，实际上这一层是一维向量，和全连接层一样" class="headerlink" title="5、C5全连接层：参数扁平化，在LeNet5称之为卷积层，实际上这一层是一维向量，和全连接层一样"></a>5、C5全连接层：参数扁平化，在LeNet5称之为卷积层，实际上这一层是一维向量，和全连接层一样</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">model.add(Flatten())<br>model.add(Dense(<span class="hljs-number">120</span>, activation = <span class="hljs-string">&#x27;relu&#x27;</span>))<br></code></pre></td></tr></table></figure><h3 id="6、F6全连接层：输出节点为84个"><a href="#6、F6全连接层：输出节点为84个" class="headerlink" title="6、F6全连接层：输出节点为84个"></a>6、F6全连接层：输出节点为84个</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">model.add(Dense(<span class="hljs-number">84</span>, activation = <span class="hljs-string">&#x27;relu&#x27;</span>))<br></code></pre></td></tr></table></figure><h3 id="7、Output输出层：用softmax激活函数计算分类概率"><a href="#7、Output输出层：用softmax激活函数计算分类概率" class="headerlink" title="7、Output输出层：用softmax激活函数计算分类概率"></a>7、Output输出层：用softmax激活函数计算分类概率</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">model.add(Dense(<span class="hljs-number">10</span>, activation = <span class="hljs-string">&#x27;softmax&#x27;</span>))<br></code></pre></td></tr></table></figure><h2 id="三、模型训练"><a href="#三、模型训练" class="headerlink" title="三、模型训练"></a>三、模型训练</h2><h3 id="1、数据加载"><a href="#1、数据加载" class="headerlink" title="1、数据加载"></a>1、数据加载</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">(train_x,train_y),(test_x,test_y) = mnist.load_data()<br></code></pre></td></tr></table></figure><h3 id="2、输入数据为mnist数据集"><a href="#2、输入数据为mnist数据集" class="headerlink" title="2、输入数据为mnist数据集"></a>2、输入数据为mnist数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Python">train_x = train_x.reshape(train_x.shape[<span class="hljs-number">0</span>], <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>) / <span class="hljs-number">255</span><br>test_x = test_x.reshape(test_x.shape[<span class="hljs-number">0</span>], <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>) / <span class="hljs-number">255</span><br>train_y = keras.utils.to_categorical(train_y, <span class="hljs-number">10</span>)<br>test_y = keras.utils.to_categorical(test_y, <span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><h3 id="3、设置损失函数和优化器配置"><a href="#3、设置损失函数和优化器配置" class="headerlink" title="3、设置损失函数和优化器配置"></a>3、设置损失函数和优化器配置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">model.<span class="hljs-built_in">compile</span>(loss = keras.metrics.categorical_crossentropy, optimizer = keras.optimizers.Adam(), metrics = [<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><h3 id="4、传入训练数据进行训练"><a href="#4、传入训练数据进行训练" class="headerlink" title="4、传入训练数据进行训练"></a>4、传入训练数据进行训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">model.fit(train_x, train_y, batch_size = <span class="hljs-number">128</span>, epochs = <span class="hljs-number">5</span>, verbose = <span class="hljs-number">1</span>, validation_data = (test_x,test_y))<br></code></pre></td></tr></table></figure><h3 id="5、对结果进行评估"><a href="#5、对结果进行评估" class="headerlink" title="5、对结果进行评估"></a>5、对结果进行评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">score = model.evaluate(test_x, test_y)<br></code></pre></td></tr></table></figure><h2 id="四、训练结果及准确率"><a href="#四、训练结果及准确率" class="headerlink" title="四、训练结果及准确率"></a>四、训练结果及准确率</h2><p><img src="https://i.loli.net/2021/07/05/uVxbZ7TaRqkijHf.png"></p>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 图像分类 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> Keras </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网球相关</title>
      <link href="/2022/07/02/031-Tennis/"/>
      <url>/2022/07/02/031-Tennis/</url>
      
        <content type="html"><![CDATA[<h1 id="球拍（球拍参数查询）"><a href="#球拍（球拍参数查询）" class="headerlink" title="球拍（球拍参数查询）"></a>球拍（<a href="https://www.racketlogger.com/racket">球拍参数查询</a>）</h1><ul><li>空拍重量</li><li>穿线重量</li><li>平衡点<br>X pts HL，HL前面的数值X越大其挥重越低，而数值越小其挥重越大。X pts HH，数值越大挥重越大，数值越小挥重越小。</li><li>拍面尺寸</li><li>硬度<br>球拍被击中后弯曲的程度，用“RA”评级来衡量。比较硬的球拍能给球带来更多的能量，但对手臂的打击更大，振动更大，与比较灵活的球拍相比，感觉和控制力更弱。</li><li>挥重<br>拍子挥起来的重量有多大，而挥重的大小则取决于拍子的平衡点。挥重更大的拍子更难去挥动，灵活性也会变差，但是其击球力道会更强，而挥重低的拍子灵活性更好，挥拍速度更快，但是球质会稍差一些。挥重低的拍子也可以通过增加拍子自身更大的重量来弥补。</li><li>线床</li><li>球拍长度</li><li>拍框厚度<br>三个数字，第一个数字代表的是拍头部分拍框的厚度，而第二个数字代表的是拍子甜区左右两侧（3点 &amp; 9点）拍框的厚度，最后一个数字代表的三角区（拍喉）的厚度。更厚的拍框拥有更好的避震和力量，而更薄的拍框拥有更好的手感反馈和控制。</li></ul><h1 id="球拍线（球线测评网站）"><a href="#球拍线（球线测评网站）" class="headerlink" title="球拍线（球线测评网站）"></a>球拍线（<a href="https://www.stringforum.net/stringsearch.php">球线测评网站</a>）</h1><p><strong>（注意事项）</strong>剪线顺序：从下往上竖直剪至甜点处，再向左右两侧横剪，最后竖直剪至拍头，最大限度地保护拍框。</p><h2 id="球线类型"><a href="#球线类型" class="headerlink" title="球线类型"></a>球线类型</h2><ul><li>硬线（聚酯线）<br>弹力稍差，回旋没有软线强劲，打起来球速稍慢，但是非常稳，使用寿命很长。</li><li>软线（仿羊肠线等）<br>弹性强，球速快，适合打强力上旋的选手使用，但寿命短，容易崩线。</li><li>子母线<br>横竖采用不同的网球线。通常搭配为：竖线-聚酯线（硬线），横线-（仿羊肠线&#x2F;羊肠线）</li></ul><h2 id="球线特性"><a href="#球线特性" class="headerlink" title="球线特性"></a>球线特性</h2><ul><li>磅数</li><li>手感</li><li>力量</li><li>控制</li><li>旋转</li></ul><p>要点：<br>（1）线径越粗，耐打性越好，掉磅越慢。<br>（2）密集线床（18×20）更容易打平击，稀疏线床（16×19）更适合打上旋。<br>（3）穿线时，相同磅数穿完后，横线会低2磅，因此如果想要横竖相同，则横线需要横线需要比竖线高2磅。（子母线差1磅）<br>（4）竖线主要是力量和旋转，因此会选择竖线拉更高的磅数。<br>（5）穿线后24小时内磅数会衰减约10%，然后逐日衰减至稳定在40~50磅之间。<br><strong>练习网球建议使用硬线，增加手感。擅长进攻，使用软线；如果你适合稳健防守反击，建议硬线。</strong></p><h1 id="网球"><a href="#网球" class="headerlink" title="网球"></a>网球</h1><h1 id="正手击球"><a href="#正手击球" class="headerlink" title="正手击球"></a>正手击球</h1><h1 id="双反击球"><a href="#双反击球" class="headerlink" title="双反击球"></a>双反击球</h1>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网球 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>转载：Awesome Visual-Transformer</title>
      <link href="/2022/07/01/023-Visual-Transformers/"/>
      <url>/2022/07/01/023-Visual-Transformers/</url>
      
        <content type="html"><![CDATA[<p>转载自：[<a href="https://github.com/dk-liang/Awesome-Visual-Transformer">Awesome Visual-Transformer</a>]<br>Collect some Transformer with Computer-Vision (CV) papers. </p><hr><h1 id="Awesome-Visual-Transformer"><a href="#Awesome-Visual-Transformer" class="headerlink" title="Awesome Visual-Transformer"></a>Awesome Visual-Transformer</h1><h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h2><h3 id="Transformer-original-paper"><a href="#Transformer-original-paper" class="headerlink" title="Transformer original paper"></a>Transformer original paper</h3><ul><li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> (NIPS 2017)</li></ul><h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><ul><li>Transformers in Vision: A Survey [<a href="https://arxiv.org/abs/2101.01169">paper</a>]   - 2021.02.22</li><li>A Survey on Visual Transformer [<a href="https://arxiv.org/abs/2012.12556">paper</a>]   - 2020.1.30</li><li>A Survey of Transformers  [<a href="https://arxiv.org/abs/2106.04554">paper</a>]   - 2020.6.09</li></ul><h3 id="arXiv-papers"><a href="#arXiv-papers" class="headerlink" title="arXiv papers"></a>arXiv papers</h3><ul><li><strong>[CrossFormer]</strong> CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention [<a href="https://arxiv.org/abs/2108.00154v1">paper</a>] [<a href="https://github.com/cheerss/CrossFormer">code</a>]</li><li><strong>[Styleformer]</strong> Styleformer: Transformer based Generative Adversarial Networks with Style Vector [<a href="https://arxiv.org/abs/2106.07023">paper</a>] [<a href="https://github.com/Jeeseung-Park/Styleformer">code</a>]</li><li><strong>[CMT]</strong> CMT: Convolutional Neural Networks Meet Vision Transformers [<a href="https://arxiv.org/abs/2107.06263">paper</a>]</li><li><strong>[TransAttUnet]</strong> TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation [<a href="https://arxiv.org/abs/2107.05274">paper</a>]</li><li>TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation [<a href="https://arxiv.org/abs/2107.05188">paper</a>]</li><li><strong>[ViTGAN]</strong> ViTGAN: Training GANs with Vision Transformers [<a href="https://arxiv.org/abs/2107.04589">paper</a>]</li><li>What Makes for Hierarchical Vision Transformer? [<a href="https://arxiv.org/abs/2107.02174">paper</a>]</li><li>CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows [<a href="https://arxiv.org/abs/2107.00652">paper</a>] [<a href="https://github.com/microsoft/CSWin-Transformer">code</a>]</li><li><strong>[Trans4Trans]</strong> Trans4Trans: Efficient Transformer for Transparent Object Segmentation to Help Visually Impaired People Navigate in the Real World [<a href="https://arxiv.org/abs/2107.03172">paper</a>] </li><li><strong>[FFVT]</strong> Feature Fusion Vision Transformer for Fine-Grained Visual Categorization [<a href="https://arxiv.org/abs/2107.02341">paper</a>] </li><li><strong>[TransformerFusion]</strong> TransformerFusion: Monocular RGB Scene Reconstruction using Transformers [<a href="https://arxiv.org/abs/2107.02191">paper</a>]</li><li>Escaping the Big Data Paradigm with Compact Transformers [<a href="https://arxiv.org/pdf/2104.05704.pdf">paper</a>]</li><li>How to train your ViT? Data, Augmentation,and Regularization in Vision Transformers [<a href="https://arxiv.org/pdf/2106.10270.pdf">paper</a>]</li><li>Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks [<a href="https://arxiv.org/pdf/2105.02358.pdf">paper</a>]</li><li><strong>[XCiT]</strong> XCiT: Cross-Covariance Image Transformers [<a href="https://arxiv.org/pdf/2106.09681.pdf">paper</a>] [<a href="https://github.com/facebookresearch/xcit">code</a>]</li><li>Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer [<a href="https://arxiv.org/abs/2106.03650">paper</a>] [<a href="https://github.com/mulinmeng/Shuffle-Transformer">code</a>]</li><li>Video Swin Transformer [<a href="https://arxiv.org/abs/2106.13230">paper</a>] [<a href="https://github.com/SwinTransformer/Video-Swin-Transformer">code</a>]</li><li><strong>[VOLO]</strong> VOLO: Vision Outlooker for Visual Recognition [<a href="https://arxiv.org/abs/2106.13112">paper</a>] [<a href="https://github.com/sail-sg/volo">code</a>]</li><li>Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images [<a href="https://arxiv.org/abs/2106.12413">paper</a>] </li><li><strong>[P2T]</strong> P2T: Pyramid Pooling Transformer for Scene Understanding [<a href="https://arxiv.org/abs/2106.12011">paper</a>]</li><li><strong>[DocFormer]</strong> DocFormer: End-to-End Transformer for Document Understanding [<a href="https://arxiv.org/abs/2106.11539">paper</a>]</li><li>End-to-end Temporal Action Detection with Transformer [<a href="https://arxiv.org/abs/2106.10271">paper</a>] [<a href="https://github.com/xlliu7/TadTR">code</a>]</li><li>How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers [<a href="https://arxiv.org/abs/2106.10270">paper</a>]</li><li>Efficient Self-supervised Vision Transformers for Representation Learning [<a href="https://arxiv.org/abs/2106.09785">paper</a>]</li><li>Space-time Mixing Attention for Video Transformer [<a href="https://arxiv.org/abs/2106.05968">paper</a>]</li><li>Transformed CNNs: recasting pre-trained convolutional layers with self-attention [<a href="https://arxiv.org/abs/2106.05795">paper</a>]</li><li><strong>[CAT]</strong> CAT: Cross Attention in Vision Transformer [<a href="https://arxiv.org/abs/2106.05786">paper</a>]</li><li>Scaling Vision Transformers [<a href="https://arxiv.org/abs/2106.04560">paper</a>]</li><li><strong>[DETReg]</strong> DETReg: Unsupervised Pretraining with Region Priors for Object Detection [<a href="https://arxiv.org/abs/2106.04550">paper</a>] [<a href="https://amirbar.net/detreg">code</a>]</li><li>Chasing Sparsity in Vision Transformers:An End-to-End Exploration [<a href="https://arxiv.org/abs/2106.04533">paper</a>]</li><li><strong>[MViT]</strong> MViT: Mask Vision Transformer for Facial Expression Recognition in the wild [<a href="https://arxiv.org/abs/2106.04520">paper</a>]</li><li>Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight [<a href="https://arxiv.org/abs/2106.04263">paper</a>]</li><li>On Improving Adversarial Transferability of Vision Transformers [<a href="https://arxiv.org/abs/2106.04169">paper</a>]</li><li>Fully Transformer Networks for Semantic ImageSegmentation [<a href="https://arxiv.org/abs/2106.04108">paper</a>]</li><li>Visual Transformer for Task-aware Active Learning [<a href="https://arxiv.org/abs/2106.03801">paper</a>] [<a href="https://github.com/razvancaramalau/Visual-Transformer-for-Task-aware-Active-Learning">code</a>]</li><li>Efficient Training of Visual Transformers with Small-Size Datasets [<a href="https://arxiv.org/abs/2106.03746">paper</a>] </li><li>Reveal of Vision Transformers Robustness against Adversarial Attacks [<a href="https://arxiv.org/abs/2106.03734">paper</a>]</li><li>Person Re-Identification with a Locally Aware Transformer [<a href="https://arxiv.org/abs/2106.03720">paper</a>]</li><li><strong>[Refiner]</strong> Refiner: Refining Self-attention for Vision Transformers [<a href="https://arxiv.org/abs/2106.03714">paper</a>]</li><li><strong>[ViTAE]</strong> ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias [<a href="https://arxiv.org/abs/2106.03348">paper</a>]</li><li>Video Instance Segmentation using Inter-Frame Communication Transformers [<a href="https://arxiv.org/abs/2106.03299">paper</a>]</li><li>Transformer in Convolutional Neural Networks [<a href="https://arxiv.org/abs/2106.03180">paper</a>] [<a href="https://github.com/yun-liu/TransCNN">code</a>]</li><li>Oriented Object Detection with Transformer [<a href="https://arxiv.org/abs/2106.03146">paper</a>]</li><li><strong>[Uformer]</strong> Uformer: A General U-Shaped Transformer for Image Restoration [<a href="https://arxiv.org/abs/2106.03106">paper</a>] [<a href="https://github.com/ZhendongWang6/Uformer">code</a>]</li><li>Patch Slimming for Efficient Vision Transformers [<a href="https://arxiv.org/abs/2106.02852">paper</a>]</li><li><strong>[RegionViT]</strong> RegionViT: Regional-to-Local Attention for Vision Transformers [<a href="https://arxiv.org/abs/2106.02689">paper</a>]</li><li>Associating Objects with Transformers for Video Object Segmentation [<a href="https://arxiv.org/abs/2106.02638">paper</a>] [<a href="https://github.com/z-x-yang/AOT">code</a>]</li><li>Few-Shot Segmentation via Cycle-Consistent Transformer [<a href="https://arxiv.org/abs/2106.02320">paper</a>]</li><li>Glance-and-Gaze Vision Transformer [<a href="https://arxiv.org/abs/2106.02277">paper</a>] [<a href="https://github.com/yucornetto/GG-Transformer">code</a>]</li><li>Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers [<a href="https://arxiv.org/pdf/2105.08059.pdf">paper</a>]</li><li>Anticipative Video Transformer [<a href="https://arxiv.org/abs/2106.02036">paper</a>] [<a href="http://facebookresearch.github.io/AVT">code</a>]</li><li><strong>[DynamicViT]</strong> DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification [<a href="https://arxiv.org/abs/2106.02034">paper</a>] [<a href="https://dynamicvit.ivg-research.xyz/">code</a>]</li><li>When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations [<a href="https://arxiv.org/abs/2106.01548">paper</a>] [<a href="">code</a>]</li><li><strong>[Container]</strong> Container: Context Aggregation Network [<a href="https://arxiv.org/abs/2106.01401">paper</a>]</li><li>Unsupervised Out-of-Domain Detection via Pre-trained Transformers [<a href="https://arxiv.org/abs/2106.00948">paper</a>]</li><li><strong>[TransMIL]</strong> TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classication [<a href="https://arxiv.org/abs/2106.00908">paper</a>]</li><li><strong>[YOLOS]</strong> You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection [<a href="https://arxiv.org/abs/2106.00666">paper</a>] [<a href="https://github.com/hustvl/YOLOS">code</a>]</li><li><strong>[TransVOS]</strong>  TransVOS: Video Object Segmentation with Transformers [<a href="https://arxiv.org/abs/2106.00588">paper</a>]</li><li><strong>[KVT]</strong> KVT: k-NN Attention for Boosting Vision Transformers [<a href="https://arxiv.org/abs/2106.00515">paper</a>] </li><li><strong>[MSG-Transformer]</strong> MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens [<a href="https://arxiv.org/abs/2105.15168">paper</a>] [<a href="https://github.com/hustvl/MSG-Transformer">code</a>]</li><li><strong>[SegFormer]</strong> SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers [<a href="https://arxiv.org/abs/2105.15203">paper</a>] [<a href="https://github.com/NVlabs/SegFormer">code</a>]</li><li><strong>[SDNet]</strong> SDNet: mutil-branch for single image deraining using swin [<a href="https://arxiv.org/abs/2105.15077">paper</a>] [<a href="https://github.com/H-tfx/SDNet">code</a>]</li><li><strong>[DVT]</strong> Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length [<a href="https://arxiv.org/abs/2105.15075">paper</a>]</li><li>Dual-stream Network for Visual Recognition [<a href="https://arxiv.org/abs/2105.14734">paper</a>]</li><li><strong>[GazeTR]</strong> Gaze Estimation using Transformer [<a href="https://arxiv.org/abs/2105.14424">paper</a>] [<a href="https://github.com/yihuacheng/GazeTR">code</a>]</li><li>Transformer-Based Deep Image Matching for Generalizable Person Re-identification [<a href="https://arxiv.org/abs/2105.14432">paper</a>]</li><li>Less is More: Pay Less Attention in Vision Transformers [<a href="https://arxiv.org/abs/2105.14217">paper</a>] </li><li><strong>[FoveaTer]</strong> FoveaTer: Foveated Transformer for Image Classification [<a href="https://arxiv.org/abs/2105.14173">paper</a>]</li><li><strong>[TransDA]</strong> Transformer-Based Source-Free Domain Adaptation [<a href="https://arxiv.org/abs/2105.14138">paper</a>] [<a href="https://github.com/ygjwd12345/TransDA">code</a>]</li><li>An Attention Free Transformer [<a href="https://arxiv.org/abs/2105.14103">paper</a>]</li><li><strong>[PTNet]</strong> PTNet: A High-Resolution Infant MRI Synthesizer Based on Transformer [<a href="https://arxiv.org/abs/2105.13993">paper</a>]</li><li><strong>[ResT]</strong> ResT: An Efficient Transformer for Visual Recognition [<a href="https://arxiv.org/abs/2105.13677">paper</a>] [<a href="https://github.com/wofmanaf/ResT">code</a>]</li><li><strong>[CogView]</strong> CogView: Mastering Text-to-Image Generation via Transformers [<a href="https://arxiv.org/abs/2105.13290">paper</a>]</li><li><strong>[NesT]</strong> Aggregating Nested Transformers [<a href="https://arxiv.org/abs/2105.12723">paper</a>] </li><li><strong>[TAPG]</strong> Temporal Action Proposal Generation with Transformers [<a href="https://arxiv.org/abs/2105.12043">paper</a>] </li><li>Boosting Crowd Counting with Transformers [<a href="https://arxiv.org/abs/2105.10926">paper</a>] </li><li><strong>[COTR]</strong> COTR: Convolution in Transformer Network for End to End Polyp Detection [<a href="https://arxiv.org/abs/2105.10925">paper</a>]</li><li><strong>[TransVOD]</strong> End-to-End Video Object Detection with Spatial-Temporal Transformers [<a href="https://arxiv.org/abs/2105.10920">paper</a>] [<a href="https://github.com/SJTU-LuHe/TransVOD">code</a>]</li><li>Intriguing Properties of Vision Transformers [<a href="https://arxiv.org/abs/2105.10497">paper</a>] [<a href="https://git.io/Js15X">code</a>] </li><li>Combining Transformer Generators with Convolutional Discriminators [<a href="https://arxiv.org/abs/2105.10189">paper</a>]</li><li>Rethinking the Design Principles of Robust Vision Transformer [<a href="https://arxiv.org/abs/2105.07926">paper</a>]</li><li>Vision Transformers are Robust Learners [<a href="https://arxiv.org/abs/2105.07581">paper</a>] [<a href="https://git.io/J3VO0">code</a>]</li><li>Manipulation Detection in Satellite Images Using Vision Transformer [<a href="https://arxiv.org/abs/2105.06373">paper</a>]</li><li><strong>[Segmenter]</strong> Segmenter: Transformer for Semantic Segmentation [<a href="https://arxiv.org/abs/2105.05633">paper</a>] [<a href="https://github.com/rstrudel/segmenter">code</a>]</li><li><strong>[Swin-Unet]</strong> Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation [<a href="https://arxiv.org/abs/2105.05537">paper</a>] [<a href="https://github.com/HuCaoFighting/Swin-Unet">code</a>]</li><li>Self-Supervised Learning with Swin Transformers [<a href="https://arxiv.org/abs/2105.04553">paper</a>] [<a href="https://github.com/SwinTransformer/Transformer-SSL">code</a>]</li><li><strong>[SCTN]</strong> SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation [<a href="https://arxiv.org/abs/2105.04447">paper</a>] </li><li><strong>[RelationTrack]</strong> RelationTrack: Relation-aware Multiple Object Tracking with Decoupled Representation [<a href="https://arxiv.org/abs/2105.04322">paper</a>]</li><li><strong>[VGTR]</strong> Visual Grounding with Transformers [<a href="https://arxiv.org/abs/2105.04281">paper</a>]</li><li><strong>[PST]</strong> Visual Composite Set Detection Using Part-and-Sum Transformers [<a href="https://arxiv.org/abs/2105.02170">paper</a>] </li><li><strong>[TrTr]</strong> TrTr: Visual Tracking with Transformer [<a href="https://arxiv.org/abs/2105.03817">paper</a>] [<a href="https://github.com/tongtybj/TrTr">code</a>]</li><li><strong>[MOTR]</strong> MOTR: End-to-End Multiple-Object Tracking with TRansformer [<a href="https://arxiv.org/abs/2105.03247">paper</a>] [<a href="https://github.com/megvii-model/MOTR">code</a>]</li><li>Attention for Image Registration (AiR): an unsupervised Transformer approach [<a href="https://arxiv.org/abs/2105.02282">paper</a>] </li><li><strong>[TransHash]</strong> TransHash: Transformer-based Hamming Hashing for Efficient Image Retrieval [<a href="https://arxiv.org/abs/2105.01823">paper</a>]</li><li><strong>[ISTR]</strong> ISTR: End-to-End Instance Segmentation with Transformers [<a href="https://arxiv.org/abs/2105.00637">paper</a>] [<a href="https://github.com/hujiecpp/ISTR">code</a>]</li><li><strong>[CAT]</strong> CAT: Cross-Attention Transformer for One-Shot Object Detection [<a href="https://arxiv.org/abs/2104.14984">paper</a>] </li><li><strong>[CoSformer]</strong> CoSformer: Detecting Co-Salient Object with Transformers [<a href="https://arxiv.org/abs/2104.14729">paper</a>]</li><li>End-to-End Attention-based Image Captioning [<a href="https://arxiv.org/abs/2104.14721">paper</a>]</li><li><strong>[PMTrans]</strong> Pyramid Medical Transformer for Medical Image Segmentation [<a href="https://arxiv.org/abs/2104.14702">paper</a>]</li><li><strong>[HandsFormer]</strong> HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation ofHands and Object in Interaction [<a href="https://arxiv.org/abs/2104.14639">paper</a>]</li><li><strong>[GasHis-Transformer]</strong> GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification [<a href="https://arxiv.org/abs/2104.14528">paper</a>] </li><li>Emerging Properties in Self-Supervised Vision Transformers [<a href="https://arxiv.org/abs/2104.14294">paper</a>]</li><li><strong>[InTra]</strong> Inpainting Transformer for Anomaly Detection [<a href="https://arxiv.org/abs/2104.13897">paper</a>] </li><li><strong>[Twins]</strong> Twins: Revisiting Spatial Attention Design in Vision Transformers [<a href="https://arxiv.org/abs/2104.13840">paper</a>] [<a href="https://github.com/Meituan-AutoML/Twins">code</a>]</li><li><strong>[MLMSPT]</strong> Point Cloud Learning with Transformer [<a href="https://arxiv.org/abs/2104.13636">paper</a>]</li><li>Medical Transformer: Universal Brain Encoder for 3D MRI Analysis [<a href="https://arxiv.org/abs/2104.13633">paper</a>]</li><li><strong>[ConTNet]</strong> ConTNet: Why not use convolution and transformer at the same time? [<a href="https://arxiv.org/abs/2104.13497">paper</a>] [<a href="https://github.com/yan-hao-tian/ConTNet">code</a>]</li><li><strong>[DTNet]</strong> Dual Transformer for Point Cloud Analysis [<a href="https://arxiv.org/abs/2104.13044">paper</a>] </li><li>Improve Vision Transformers Training by Suppressing Over-smoothing [<a href="https://arxiv.org/abs/2104.12753">paper</a>] [<a href="https://github.com/ChengyueGongR/PatchVisionTransformer">code</a>]</li><li><strong>[Visformer]</strong> Visformer: The Vision-friendly Transformer [<a href="https://arxiv.org/abs/2104.12533">paper</a>] [<a href="https://github.com/danczs/Visformer">code</a>]</li><li>Transformer Meets DCFAM: A Novel Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images [<a href="https://arxiv.org/abs/2104.12137">paper</a>]</li><li><strong>[VST]</strong> Visual Saliency Transformer [<a href="https://arxiv.org/abs/2104.12099">paper</a>] </li><li><strong>[M3DeTR]</strong> M3DeTR: Multi-representation, Multi-scale, Mutual-relation 3D Object Detection with Transformers [<a href="https://arxiv.org/abs/2104.11896">paper</a>] [<a href="https://github.com/rayguan97/M3DeTR">code</a>]</li><li><strong>[VidTr]</strong> VidTr: Video Transformer Without Convolutions [<a href="https://arxiv.org/abs/2104.11746">paper</a>] </li><li><strong>[Skeletor]</strong> Skeletor: Skeletal Transformers for Robust Body-Pose Estimation [<a href="https://arxiv.org/abs/2104.11712">paper</a>] </li><li><strong>[FaceT]</strong> Learning to Cluster Faces via Transformer [<a href="https://arxiv.org/abs/2104.11502">paper</a>]</li><li><strong>[MViT]</strong> Multiscale Vision Transformers [<a href="https://arxiv.org/abs/2104.11227">paper</a>] [<a href="https://github.com/facebookresearch/SlowFast">code</a>]</li><li><strong>[VATT]</strong> VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text [<a href="https://arxiv.org/abs/2104.11178">paper</a>]</li><li><strong>[So-ViT]</strong> So-ViT: Mind Visual Tokens for Vision Transformer [<a href="https://arxiv.org/abs/2104.10935">paper</a>] [<a href="https://github.com/jiangtaoxie/So-ViT">code</a>]</li><li>Token Labeling: Training a 85.5% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet [<a href="https://arxiv.org/abs/2104.10858">paper</a>] [<a href="https://github.com/zihangJiang/TokenLabeling">code</a>]</li><li><strong>[TransRPPG]</strong> TransRPPG: Remote Photoplethysmography Transformer for 3D Mask Face Presentation Attack Detection [<a href="https://arxiv.org/abs/2104.07419">paper</a>]</li><li><strong>[VideoGPT]</strong> VideoGPT: Video Generation using VQ-VAE and Transformers [<a href="https://arxiv.org/abs/2104.10157">paper</a>]</li><li><strong>[M2TR]</strong> M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection [<a href="https://arxiv.org/abs/2104.09770">paper</a>]</li><li>Transformer Transforms Salient Object Detection and Camouflaged Object Detection [<a href="https://arxiv.org/abs/2104.10127">paper</a>]</li><li><strong>[TransCrowd]</strong> TransCrowd: Weakly-Supervised Crowd Counting with Transformer [<a href="https://arxiv.org/abs/2104.09116">paper</a>] [<a href="https://github.com/dk-liang/TransCrowd">code</a>]</li><li><strong>[TransVG]</strong> TransVG: End-to-End Visual Grounding with Transformers [<a href="https://arxiv.org/abs/2104.08541">paper</a>]</li><li>Visual Transformer Pruning [<a href="https://arxiv.org/abs/2104.08500">paper</a>]</li><li>Self-supervised Video Retrieval Transformer Network [<a href="https://arxiv.org/abs/2104.07993">paper</a>]</li><li>Vision Transformer using Low-level Chest X-ray Feature Corpus for COVID-19 Diagnosis and Severity Quantification [<a href="https://arxiv.org/abs/2104.07235">paper</a>]</li><li><strong>[TransGAN]</strong> TransGAN: Two Transformers Can Make One Strong GAN [<a href="https://arxiv.org/abs/2102.07074">paper</a>] [<a href="https://github.com/VITA-Group/TransGAN">code</a>]</li><li>Geometry-Free View Synthesis: Transformers and no 3D Priors [<a href="https://arxiv.org/abs/2104.07652">paper</a>] [<a href="https://git.io/JOnwn">code</a>]</li><li><strong>[CoaT]</strong> Co-Scale Conv-Attentional Image Transformers [<a href="https://arxiv.org/abs/2104.06399">paper</a>] [<a href="https://github.com/mlpc-ucsd/CoaT">code</a>]</li><li><strong>[LocalViT]</strong> LocalViT: Bringing Locality to Vision Transformers [<a href="https://arxiv.org/abs/2104.05707">paper</a>] [<a href="https://github.com/ofsoundof/LocalViT">code</a>]</li><li><strong>[ACTOR]</strong> Action-Conditioned 3D Human Motion Synthesis with Transformer VAE [<a href="https://arxiv.org/abs/2104.05670">paper</a>]</li><li><strong>[CIT]</strong> Cloth Interactive Transformer for Virtual Try-On [<a href="https://arxiv.org/abs/2104.05519">paper</a>] [<a href="https://arxiv.org/abs/2104.05519">code</a>]</li><li>Handwriting Transformers [<a href="https://arxiv.org/abs/2104.03964">paper</a>]</li><li><strong>[SiT]</strong> SiT: Self-supervised vIsion Transformer [<a href="https://arxiv.org/abs/2104.03602">paper</a>] [<a href="https://github.com/Sara-Ahmed/SiT">code</a>]</li><li>On the Robustness of Vision Transformers to Adversarial Examples [<a href="https://arxiv.org/abs/2104.02610">paper</a>]</li><li>An Empirical Study of Training Self-Supervised Visual Transformers [<a href="https://arxiv.org/abs/2104.02057">paper</a>]</li><li>A Video Is Worth Three Views: Trigeminal Transformers for Video-based Person Re-identification [<a href="https://arxiv.org/abs/2104.01745">paper</a>]</li><li><strong>[AOT-GAN]</strong> Aggregated Contextual Transformations for High-Resolution Image Inpainting [<a href="https://arxiv.org/abs/2104.01431">paper</a>] [<a href="https://github.com/researchmm/AOT-GAN-for-Inpainting">code</a>]</li><li>Deepfake Detection Scheme Based on Vision Transformer and Distillation [<a href="https://arxiv.org/abs/2104.01353">paper</a>]</li><li><strong>[ATAG]</strong> Augmented Transformer with Adaptive Graph for Temporal Action Proposal Generation [<a href="https://arxiv.org/pdf/2103.16024">paper</a>] </li><li><strong>[LeViT]</strong> LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference [<a href="https://arxiv.org/abs/2104.01136">paper</a>] </li><li><strong>[TubeR]</strong> TubeR: Tube-Transformer for Action Detection [<a href="https://arxiv.org/abs/2104.00969">paper</a>]</li><li><strong>[AAformer]</strong> AAformer: Auto-Aligned Transformer for Person Re-Identification [<a href="https://arxiv.org/abs/2104.00921">paper</a>]</li><li><strong>[TFill]</strong> TFill: Image Completion via a Transformer-Based Architecture [<a href="https://arxiv.org/abs/2104.00845">paper</a>]</li><li>Group-Free 3D Object Detection via Transformers [<a href="https://arxiv.org/abs/2104.00678">paper</a>] [<a href="https://github.com/zeliu98/Group-Free-3D">code</a>]</li><li><strong>[STGT]</strong> Spatial-Temporal Graph Transformer for Multiple Object Tracking [<a href="https://arxiv.org/abs/2104.00194">paper</a>] </li><li><strong>[YOGO]</strong> You Only Group Once: Efficient Point-Cloud Processing with Token<br>Representation and Relation Inference Module[<a href="https://arxiv.org/abs/2103.09975">paper</a>] [<a href="https://github.com/chenfengxu714/YOGO.git">code</a>]</li><li>Going deeper with Image Transformers[<a href="https://arxiv.org/abs/2103.17239">paper</a>] </li><li><strong>[Stark]</strong> Learning Spatio-Temporal Transformer for Visual Tracking [<a href="https://arxiv.org/abs/2103.17154">paper</a>] [<a href="https://github.com/researchmm/Stark">code</a>]</li><li><strong>[Meta-DETR]</strong> Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning [<a href="https://arxiv.org/abs/2103.11731">paper</a> [<a href="https://github.com/ZhangGongjie/Meta-DETR">code</a>]</li><li><strong>[DA-DETR]</strong> DA-DETR: Domain Adaptive Detection Transformer by Hybrid Attention [<a href="https://arxiv.org/abs/2103.17084">paper</a>]</li><li>Robust Facial Expression Recognition with Convolutional Visual Transformers [<a href="https://arxiv.org/abs/2103.16854">paper</a>]</li><li>Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers [<a href="https://arxiv.org/abs/2103.16553">paper</a>]</li><li>Spatiotemporal Transformer for Video-based Person Re-identification[<a href="https://arxiv.org/abs/2103.16469">paper</a>] </li><li><strong>[PiT]</strong> Rethinking Spatial Dimensions of Vision Transformers [<a href="https://arxiv.org/abs/2103.16302">paper</a>] [<a href="https://github.com/naver-ai/pit">code</a>]</li><li><strong>[TransUNet]</strong> TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation [<a href="https://arxiv.org/abs/2102.04306">paper</a>] [<a href="https://github.com/Beckschen/TransUNet">code</a>]</li><li><strong>[CvT]</strong> CvT: Introducing Convolutions to Vision Transformers [<a href="https://arxiv.org/abs/2103.15808">paper</a>] [<a href="https://github.com/leoxiaobin/CvT">code</a>]</li><li>Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding [<a href="https://arxiv.org/abs/2103.15358">paper</a>]</li><li><strong>[TFPose]</strong> TFPose: Direct Human Pose Estimation with Transformers [<a href="https://arxiv.org/abs/2103.15320">paper</a>]</li><li><strong>[TransCenter]</strong> TransCenter: Transformers with Dense Queries for Multiple-Object Tracking [<a href="https://arxiv.org/abs/2103.15145">paper</a>]</li><li><strong>[ViViT]</strong> ViViT: A Video Vision Transformer [<a href="https://arxiv.org/abs/2103.15691">paper</a>]</li><li><strong>[CrossViT]</strong> CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification [<a href="https://arxiv.org/abs/2103.14899">paper</a>]</li><li><strong>[TS-CAM]</strong> TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization [<a href="https://arxiv.org/abs/2103.14862">paper</a>] [<a href="https://github.com/vasgaowei/TS-CAM.git">code</a>]</li><li>Face Transformer for Recognition [<a href="https://arxiv.org/abs/2103.14803">paper</a>]</li><li>On the Adversarial Robustness of Visual Transformers [<a href="https://arxiv.org/abs/2103.15670">paper</a>]</li><li>Understanding Robustness of Transformers for Image Classification [<a href="https://arxiv.org/abs/2103.14586">paper</a>]</li><li>Lifting Transformer for 3D Human Pose Estimation in Video [<a href="https://arxiv.org/abs/2103.14304">paper</a>]</li><li><strong>[GSA-Net]</strong> Global Self-Attention Networks for Image Recognition[<a href="https://arxiv.org/abs/2010.03019">paper</a>]</li><li>High-Fidelity Pluralistic Image Completion with Transformers [<a href="https://arxiv.org/abs/2103.14031">paper</a>] [<a href="http://raywzy.com/ICT">code</a>]</li><li>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows [<a href="https://arxiv.org/abs/2103.14030">paper</a>] [<a href="https://github.com/microsoft/Swin-Transformer">code</a>]</li><li><strong>[DPT]</strong> Vision Transformers for Dense Prediction [<a href="https://arxiv.org/abs/2103.13413">paper</a>] [<a href="https://github.com/intel-isl/DPT">code</a>]</li><li><strong>[TransFG]</strong> TransFG: A Transformer Architecture for Fine-grained Recognition? [<a href="https://arxiv.org/abs/2103.07976">paper</a>]</li><li><strong>[TimeSformer]</strong> Is Space-Time Attention All You Need for Video Understanding? [<a href="https://arxiv.org/abs/2102.05095">paper</a>]</li><li>Multi-view 3D Reconstruction with Transformer [<a href="https://arxiv.org/abs/2103.12957">paper</a>] </li><li>Can Vision Transformers Learn without Natural Images? [<a href="https://arxiv.org/abs/2103.13023">paper</a>] [<a href="https://hirokatsukataoka16.github.io/Vision-Transformers-without-Natural-Images/">code</a>]</li><li>Transformers Solve the Limited Receptive Field for Monocular Depth Prediction [<a href="https://arxiv.org/abs/2103.12091">paper</a>] [<a href="https://github.com/ygjwd12345/TransDepth">code</a>]</li><li>End-to-End Trainable Multi-Instance Pose Estimation with Transformers [<a href="https://arxiv.org/abs/2103.12115">paper</a>] </li><li>Instance-level Image Retrieval using Reranking Transformers [<a href="https://arxiv.org/abs/2103.12424">paper</a>] [<a href="https://arxiv.org/abs/2103.12236">code</a>]</li><li><strong>[BossNAS]</strong> BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search [<a href="https://arxiv.org/abs/2103.12424">paper</a>] [<a href="https://github.com/changlin31/BossNAS">code</a>]</li><li><strong>[CeiT]</strong> Incorporating Convolution Designs into Visual Transformers [<a href="https://arxiv.org/abs/2103.11816">paper</a>] </li><li><strong>[DeepViT]</strong> DeepViT: Towards Deeper Vision Transformer [<a href="https://arxiv.org/abs/2103.11886">paper</a>] </li><li><strong>[TNT]</strong> Transformer in Transformer [<a href="https://arxiv.org/abs/2103.00112">paper</a>] [<a href="https://github.com/huawei-noah/noah-research/tree/master/TNT">code</a>]</li><li>Enhancing Transformer for Video Understanding Using Gated Multi-Level Attention and Temporal Adversarial Training [<a href="https://arxiv.org/abs/2103.10043">paper</a>] </li><li>3D Human Pose Estimation with Spatial and Temporal Transformers [<a href="https://arxiv.org/abs/2103.10455">paper</a>] [<a href="https://github.com/zczcwh/PoseFormer">code</a>]</li><li><strong>[SUNETR]</strong> SUNETR: Transformers for 3D Medical Image Segmentation [<a href="https://arxiv.org/abs/2103.10504">paper</a>] </li><li>Scalable Visual Transformers with Hierarchical Pooling [<a href="https://arxiv.org/abs/2103.10619">paper</a>] </li><li><strong>[ConViT]</strong> ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases [<a href="https://arxiv.org/abs/2103.10697">paper</a>] </li><li><strong>[TransMed]</strong> TransMed: Transformers Advance Multi-modal Medical Image Classification [<a href="https://arxiv.org/abs/2103.05940">paper</a>] </li><li><strong>[U-Transformer]</strong> U-Net Transformer: Self and Cross Attention for Medical Image Segmentation [<a href="https://arxiv.org/abs/2103.06104">paper</a>] </li><li><strong>[SpecTr]</strong> SpecTr: Spectral Transformer for Hyperspectral Pathology Image Segmentation [<a href="https://arxiv.org/abs/2103.03604">paper</a>] [<a href="https://github.com/hfut-xc-yun/SpecTr">code</a>]</li><li><strong>[TransBTS]</strong> TransBTS: Multimodal Brain Tumor Segmentation Using Transformer [<a href="https://arxiv.org/abs/2103.04430">paper</a>] [<a href="https://github.com/Wenxuan-1119/TransBTS">code</a>]</li><li><strong>[SSTN]</strong> SSTN: Self-Supervised Domain Adaptation Thermal<br>Object Detection for Autonomous Driving [<a href="https://arxiv.org/abs/2103.03150">paper</a>] </li><li><strong>[GANsformer]</strong> Generative Adversarial Transformers [<a href="https://arxiv.org/abs/2103.01209">paper</a>] [<a href="https://github.com/dorarad/gansformer">code</a>]</li><li><strong>[PVT]</strong> Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions [<a href="https://arxiv.org/abs/2102.12122">paper</a>] [<a href="https://github.com/whai362/PVT">code</a>]</li><li>Transformer is All You Need:<br>Multimodal Multitask Learning with a Unified Transformer [<a href="https://arxiv.org/abs/2102.10772">paper</a>] [<a href="https://mmf.sh/">code</a>]</li><li><strong>[CPVT]</strong> Do We Really Need Explicit Position Encodings for Vision Transformers? [<a href="https://arxiv.org/abs/2102.10882">paper</a>] [<a href="https://github.com/Meituan-AutoML/CPVT">code</a>]</li><li>Deepfake Video Detection Using Convolutional Vision Transformer[<a href="https://arxiv.org/abs/2102.11126">paper</a>]</li><li>Training Vision Transformers for Image Retrieval[<a href="https://arxiv.org/abs/2102.05644">paper</a>]</li><li><strong>[TransReID]</strong> TransReID: Transformer-based Object Re-Identification[<a href="https://arxiv.org/abs/2102.04378">paper</a>]</li><li><strong>[VTN]</strong> Video Transformer Network[<a href="https://arxiv.org/abs/2102.00719">paper</a>]</li><li><strong>[T2T-ViT]</strong> Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet [<a href="https://arxiv.org/abs/2101.11986">paper</a>] [<a href="https://github.com/yitu-opensource/T2T-ViT">code</a>]</li><li><strong>[BoTNet]</strong> Bottleneck Transformers for Visual Recognition [<a href="https://arxiv.org/abs/2101.11605">paper</a>]</li><li><strong>[CPTR]</strong> CPTR: Full Transformer Network for Image Captioning [<a href="https://arxiv.org/abs/2101.10804">paper</a>]</li><li>Learn to Dance with AIST++: Music Conditioned 3D Dance Generation [<a href="https://arxiv.org/abs/2101.08779">paper</a>] [<a href="https://google.github.io/aichoreographer/">code</a>]</li><li><strong>[Trans2Seg]</strong>  Segmenting Transparent Object in the Wild with Transformer [<a href="https://arxiv.org/abs/2101.08461">paper</a>] [<a href="https://github.com/xieenze/Trans2Seg">code</a>]</li><li>Investigating the Vision Transformer Model for Image Retrieval Tasks [<a href="https://arxiv.org/abs/2101.03771">paper</a>]</li><li><strong>[Trear]</strong> Trear: Transformer-based RGB-D Egocentric Action Recognition [<a href="https://arxiv.org/abs/2101.03904">paper</a>]</li><li><strong>[VisualSparta]</strong> VisualSparta: Sparse Transformer Fragment-level Matching for Large-scale Text-to-Image Search [<a href="https://arxiv.org/abs/2101.00265">paper</a>]</li><li><strong>[TrackFormer]</strong> TrackFormer: Multi-Object Tracking with Transformers [<a href="https://arxiv.org/abs/2101.02702">paper</a>]</li><li><strong>[LETR]</strong> Line Segment Detection Using Transformers without Edges [<a href="https://arxiv.org/abs/2101.01909">paper</a>]</li><li><strong>[TAPE]</strong> Transformer Guided Geometry Model for Flow-Based Unsupervised Visual Odometry [<a href="https://arxiv.org/abs/2101.02143">paper</a>]</li><li><strong>[TRIQ]</strong> Transformer for Image Quality Assessment [<a href="https://arxiv.org/abs/2101.01097">paper</a>] [<a href="https://github.com/junyongyou/triq">code</a>]</li><li><strong>[TransTrack]</strong> TransTrack: Multiple-Object Tracking with Transformer [<a href="https://arxiv.org/abs/2012.15460">paper</a>] [<a href="https://github.com/PeizeSun/TransTrack">code</a>]</li><li><strong>[TransPose]</strong> TransPose: Towards Explainable Human Pose Estimation by Transformer [<a href="https://arxiv.org/abs/2012.14214">paper</a>] </li><li><strong>[DeiT]</strong> Training data-efficient image transformers &amp; distillation through attention [<a href="https://arxiv.org/abs/2012.12877">paper</a>] [<a href="https://github.com/facebookresearch/deit">code</a>]</li><li><strong>[Pointformer]</strong> 3D Object Detection with Pointformer [<a href="https://arxiv.org/abs/2012.11409">paper</a>] </li><li><strong>[ViT-FRCNN]</strong> Toward Transformer-Based Object Detection [<a href="https://arxiv.org/abs/2012.09958">paper</a>] </li><li><strong>[Taming-transformers]</strong> Taming Transformers for High-Resolution Image Synthesis [<a href="https://arxiv.org/abs/2012.09841">paper</a>] [<a href="https://compvis.github.io/taming-transformers/">code</a>]</li><li><strong>[SceneFormer]</strong> SceneFormer: Indoor Scene Generation with Transformers [<a href="https://arxiv.org/abs/2012.09793">paper</a>] </li><li><strong>[PCT]</strong> PCT: Point Cloud Transformer [<a href="https://arxiv.org/abs/2012.09688">paper</a>] </li><li><strong>[METRO]</strong> End-to-End Human Pose and Mesh Reconstruction with Transformers [<a href="https://arxiv.org/abs/2012.09760">paper</a>]</li><li><strong>[PointTransformer]</strong> Point Transformer [<a href="https://arxiv.org/abs/2012.09164">paper</a>]</li><li><strong>[PED]</strong> DETR for Pedestrian Detection[<a href="https://arxiv.org/abs/2012.06785">paper</a>]</li><li>Transformer Guided Geometry Model for Flow-Based Unsupervised Visual Odometry[<a href="https://arxiv.org/abs/2101.02143">paper</a>]</li><li><strong>[C-Tran]</strong> General Multi-label Image Classification with Transformers [<a href="https://arxiv.org/abs/2011.14027">paper</a>]</li><li><strong>[TSP-FCOS]</strong> Rethinking Transformer-based Set Prediction for Object Detection [<a href="https://arxiv.org/abs/2011.10881">paper</a>]</li><li><strong>[ACT]</strong> End-to-End Object Detection with Adaptive Clustering Transformer [<a href="https://arxiv.org/abs/2011.09315">paper</a>]</li><li><strong>[STTR]</strong> Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers [<a href="https://arxiv.org/abs/2011.02910v2">paper</a>] [<a href="https://github.com/mli0603/stereo-transformer">code</a>]</li><li><strong>[VTs]</strong> Visual Transformers: Token-based Image Representation and Processing for Computer Vision [<a href="https://arxiv.org/abs/2006.03677">paper</a>]</li></ul><h3 id="已见刊-2021"><a href="#已见刊-2021" class="headerlink" title="已见刊 (2021)"></a>已见刊 (2021)</h3><ul><li>Vision Transformer with Progressive Sampling (<strong>ICCV</strong>)[<a href="https://arxiv.org/abs/2108.01684v1">paper</a>]</li><li><strong>[SMCA]</strong>  Fast Convergence of DETR with Spatially Modulated Co-Attention (<strong>ICCV</strong>)[<a href="https://arxiv.org/abs/2101.07448">paper</a>] [<a href="https://github.com/abc403/SMCA-replication">code</a>]</li><li><strong>[AutoFormer]</strong> AutoFormer: Searching Transformers for Visual Recognition (<strong>ICCV</strong>)[<a href="https://arxiv.org/pdf/2107.00651.pdf">paper</a>] [<a href="https://github.com/microsoft/AutoML">code</a>]</li><li><strong>[NDT-Transformer]</strong> NDT-Transformer: Large-Scale 3D Point Cloud Localisation using the Normal Distribution Transform Representation (<strong>ICRA</strong>)[<a href="https://arxiv.org/abs/2103.12292">paper</a>] </li><li><strong>[DPT]</strong> DPT: Deformable Patch-based Transformer for Visual Recognition (<strong>ACM MM</strong>) [<a href="http://arxiv.org/abs/2107.14467v1">paper</a>]</li><li><strong>[HAT]</strong> HAT: Hierarchical Aggregation Transformers for Person Re-identification (<strong>ACM MM</strong>) [<a href="https://arxiv.org/abs/2107.05946">paper</a>]</li><li><strong>[UTNet]</strong> UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation (<strong>MICCAI</strong>) [<a href="https://arxiv.org/abs/2107.00781">paper</a>] </li><li><strong>[MedT]</strong> Medical Transformer: Gated Axial-Attention for Medical Image Segmentation (<strong>MICCAI</strong>) [<a href="https://arxiv.org/abs/2102.10662">paper</a>] [<a href="https://github.com/jeya-maria-jose/Medical-Transformer">code</a>]</li><li><strong>[MCTrans]</strong> Multi-Compound Transformer for Accurate Biomedical Image Segmentation (<strong>MICCAI</strong>) [<a href="https://arxiv.org/abs/2106.14385">paper</a>]</li><li><strong>[PNS-Net]</strong> Progressively Normalized Self-Attention Network for Video Polyp Segmentation (<strong>MICCAI</strong>) [<a href="https://arxiv.org/abs/2105.08468">paper</a>] [<a href="https://github.com/GewelsJI/PNS-Net">code</a>]</li><li><strong>[MBT-Net]</strong> A Multi-Branch Hybrid Transformer Networkfor Corneal Endothelial Cell Segmentation [<a href="https://arxiv.org/abs/2106.07557">paper</a>]</li><li>VT-ADL: A Vision Transformer Network for Image Anomaly Detection and Localization (<strong>ISIE</strong>) [<a href="https://arxiv.org/abs/2104.10036">paper</a>]</li><li>Medical Image Segmentation using Squeeze-and-Expansion Transformers  (<strong>IJCAI</strong>) [<a href="https://arxiv.org/abs/2105.09511">paper</a>]</li><li>Vision Transformer for Fast and Efficient Scene Text Recognition (<strong>ICDAR</strong>) [<a href="https://arxiv.org/abs/2105.08582">paper</a>]</li><li>Diverse Part Discovery: Occluded Person Re-identification with Part-Aware Transformer (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2106.04095">paper</a>]</li><li><strong>[HOTR]</strong> HOTR: End-to-End Human-Object Interaction Detection with Transformers (<strong>CVPR oral</strong>) [<a href="https://arxiv.org/abs/2104.13682">paper</a>] </li><li>High-Resolution Complex Scene Synthesis with Transformers (<strong>CVPRW</strong>) [<a href="https://arxiv.org/abs/2105.06458">paper</a>]</li><li><strong>[TransFuser]</strong> Multi-Modal Fusion Transformer for End-to-End Autonomous Driving (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2104.09224">paper</a>] [<a href="https://github.com/autonomousvision/transfuser">code</a>]</li><li>Pose Recognition with Cascade Transformers  (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2104.06976">paper</a>]</li><li>Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning  (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2104.03135">paper</a>]</li><li><strong>[LoFTR]</strong> LoFTR: Detector-Free Local Feature Matching with Transformers (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2104.00680">paper</a>] [<a href="https://zju3dv.github.io/loftr/">code</a>]</li><li>Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2103.16553">paper</a>] </li><li><strong>[SETR]</strong> Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2012.15840">paper</a>] [<a href="https://fudan-zvg.github.io/SETR/">code</a>]</li><li><strong>[TransT]</strong> Transformer Tracking  (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2103.15436">paper</a>] [<a href="https://github.com/chenxin-dlut/TransT">code</a>]</li><li>Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking (<strong>CVPR oral</strong>) [<a href="https://arxiv.org/abs/2103.11681">paper</a>]</li><li><strong>[VisTR]</strong> End-to-End Video Instance Segmentation with Transformers (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2011.14503">paper</a>]</li><li>Transformer Interpretability Beyond Attention Visualization (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2012.09838">paper</a>] [<a href="https://github.com/hila-chefer/Transformer-Explainability">code</a>]</li><li><strong>[IPT]</strong> Pre-Trained Image Processing Transformer (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2012.00364">paper</a>]</li><li><strong>[UP-DETR]</strong> UP-DETR: Unsupervised Pre-training for Object Detection with Transformers (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2011.09094">paper</a>]</li><li><strong>[IQT]</strong> Perceptual Image Quality Assessment with Transformers (<strong>CVPRW</strong>) [<a href="https://arxiv.org/abs/2104.14730">paper</a>]</li><li><strong>[VTNet]</strong> VTNet: Visual Transformer Network for Object Goal Navigation (<strong>ICLR</strong>)[<a href="https://arxiv.org/abs/2105.09447">paper</a>]</li><li><strong>[Vision Transformer]</strong> An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (<strong>ICLR</strong>)[<a href="https://arxiv.org/abs/2010.11929">paper</a>] [<a href="https://github.com/google-research/vision_transformer">code</a>]</li><li><strong>[Deformable DETR]</strong> Deformable DETR: Deformable Transformers for End-to-End Object Detection (<strong>ICLR</strong>)[<a href="https://arxiv.org/abs/2010.04159">paper</a>] [<a href="https://github.com/fundamentalvision/Deformable-DETR">code</a>]</li><li><strong>[LAMBDANETWORKS]</strong> MODELING LONG-RANGE INTERACTIONS WITHOUT ATTENTION (<strong>ICLR</strong>) <a href="https://openreview.net/pdf?id=xTJEN-ggl1b">paper</a>] [<a href="https://github.com/lucidrains/lambda-networks">code</a>]</li><li><strong>[LSTR]</strong> End-to-end Lane Shape Prediction with Transformers (<strong>WACV</strong>) [<a href="https://arxiv.org/abs/2011.04233">paper</a>] [<a href="https://github.com/liuruijin17/LSTR">code</a>]</li></ul><h3 id="已见刊-2020"><a href="#已见刊-2020" class="headerlink" title="已见刊 (2020)"></a>已见刊 (2020)</h3><ul><li><strong>[DETR]</strong> End-to-End Object Detection with Transformers (<strong>ECCV</strong>) [<a href="https://arxiv.org/abs/2005.12872">paper</a>] [<a href="https://github.com/facebookresearch/detr">code</a>]</li><li>[<strong>FPT</strong>] Feature Pyramid Transformer (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2007.09451">paper</a>] [<a href="https://github.com/ZHANGDONG-NJUST/FPT">code</a>]</li><li><strong>[TTSR]</strong> Learning Texture Transformer Network for Image Super-Resolution (<strong>CVPR</strong>) [<a href="https://arxiv.org/abs/2006.04139">paper</a>] [<a href="https://github.com/researchmm/TTSR">code</a>]</li><li><strong>[STTN]</strong> Learning Joint Spatial-Temporal Transformations for Video Inpainting (<strong>ECCV</strong>) [<a href="https://arxiv.org/abs/2007.10247">paper</a>] [<a href="https://github.com/researchmm/STTN">code</a>]</li></ul>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>U-Net系列文献综述</title>
      <link href="/2022/07/01/014-UNet_Family/"/>
      <url>/2022/07/01/014-UNet_Family/</url>
      
        <content type="html"><![CDATA[<h1 id="一、U-Net"><a href="#一、U-Net" class="headerlink" title="一、U-Net"></a><strong>一、U-Net</strong></h1><h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a><strong>Contribution</strong></h2><ul><li>更规整的网络结构</li><li>通过将编码器的每层结果拼接到译码器中得到更好的结果</li></ul><h2 id="1、Architecture"><a href="#1、Architecture" class="headerlink" title="1、Architecture"></a><strong>1、Architecture</strong></h2><p><img src="https://i.loli.net/2021/07/05/ahHDQsSkVb32qTn.png" alt="U-Net"></p><h1 id="二、3D-U-Net"><a href="#二、3D-U-Net" class="headerlink" title="二、3D U-Net"></a><strong>二、3D U-Net</strong></h1><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a><strong>Architecture</strong></h2><p><img src="https://i.loli.net/2021/07/05/5cxAgn1IiGZJdEC.png" alt="3D U-Net"></p><h2 id="3、Tricks"><a href="#3、Tricks" class="headerlink" title="3、Tricks"></a><strong>3、Tricks</strong></h2><ul><li>两种分割方式<br>（1）半自动分割<br>半自动分割网络允许用户输入几个注释的2维切片，来得到整个三维体的分割。在半自动分割方法中，使用了IoU指标作为精度度量，得出结论3D U-Net能够从很少的带标注的切片中推广到非常精确的三维分割，而不需要太多的标注工作。<br>（2）全自动分割<br>已经有一个在具有代表性的训练集上（带注释的切片）进行训练得到的网络，用户可以使用这个网络在没有注释的体积卷上运行,来得到整个三维体的分割。</li><li>通道数翻倍的时机和反卷积操作。在2D Unet中，通道数翻倍的时机在下采样后的第一次卷积时；而在3D U-Net中，通道数翻倍发生在下采样或上采样前的卷积中。对于反卷积操作，区别在于通道数是否减半，2D U-Net中通道数减半，而3D U-Net中通道数不变。</li><li>使用Batch Normalization来加快收敛和避免网络结构的瓶颈。</li><li>使用了旋转、缩放和灰度增强等数据增强方法，此外在训练数据和正确标注数据上运用平滑的密集变形场，即从一个标准差为4的正态分布中抽取随机向量，每个方向的间距为32个体素，然后应用b样条插值。</li><li>使用带加权交叉熵损失的Softmax函数对网络输出和正确标注数据进行比较，对经常出现的背景减少权重，对标注到的图像数据部分增加权重，以平衡微管和背景体素对损失的影响。未标记的像素不参与损失计算，即权重为0。这样可以让网络可以更多地仅仅学习标注到的像素点，从而达到普适性地特点。</li></ul><h1 id="三、U-Net"><a href="#三、U-Net" class="headerlink" title="三、U-Net++"></a><strong>三、U-Net++</strong></h1><p>作者知乎解读：<a href="https://zhuanlan.zhihu.com/p/44958351">https://zhuanlan.zhihu.com/p/44958351</a><br>小总结：<a href="https://blog.csdn.net/u013066730/article/details/84954229?biz_id=102&utm_term=UNet++&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-84954229&spm=1018.2118.3001.4187">https://blog.csdn.net/u013066730/article/details/84954229?biz_id=102&amp;utm_term=UNet++&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-84954229&amp;spm=1018.2118.3001.4187</a></p><h2 id="1、Architecture-1"><a href="#1、Architecture-1" class="headerlink" title="1、Architecture"></a><strong>1、Architecture</strong></h2><ul><li>黑色部分代表的就是原始U-Net结构,绿色代表添加的卷积层，蓝色代表改进的Skip Connection。</li><li>以X(1,2)为例说明，它是由X(1,0)、X(1,1)和上采样后的X(2,1)拼接之后，再经过一次Conv与ReLU得到。<br><img src="https://i.loli.net/2021/07/05/SDgP3rRY4cUsd7b.png" alt="U-Net++"></li></ul><h1 id="四、Res-U-Net和Dense-U-Net"><a href="#四、Res-U-Net和Dense-U-Net" class="headerlink" title="四、Res U-Net和Dense U-Net"></a><strong>四、Res U-Net和Dense U-Net</strong></h1><h2 id="1、Contribution"><a href="#1、Contribution" class="headerlink" title="1、Contribution"></a><strong>1、Contribution</strong></h2><ul><li>Res U-Net和Dense U-Net分别受到残差连接和密集连接的启发，将U-Net的每一个子模块分别替换为具有残差连接和密集连接的形式。</li></ul><h2 id="2、Architecture"><a href="#2、Architecture" class="headerlink" title="2、Architecture"></a><strong>2、Architecture</strong></h2><p><img src="https://i.loli.net/2021/07/05/s9N3gaDQdAjorvn.png" alt="Res U-Net"></p><h1 id="五、MultiRes-U-Net"><a href="#五、MultiRes-U-Net" class="headerlink" title="五、MultiRes U-Net"></a><strong>五、MultiRes U-Net</strong></h1><h2 id="1、Contribution-1"><a href="#1、Contribution-1" class="headerlink" title="1、Contribution"></a><strong>1、Contribution</strong></h2><ul><li>将U-Net中的两个3*3的卷积替换成3*3，7*7卷积运算与5*5卷积运算并行合并，使用多分辨率思路替换传统卷积层。</li><li>使用Res Path替换传统U-Net中的简单的跳过连接。</li><li>在具有挑战性的训练集有着卓越的提高。</li></ul><h2 id="2、Architecture-1"><a href="#2、Architecture-1" class="headerlink" title="2、Architecture"></a><strong>2、Architecture</strong></h2><p><img src="https://i.loli.net/2021/07/05/iyK21bS4GRvjDEN.png" alt="MultiRes U-Net"></p><ul><li><strong>MultiBlock</strong><br>多分辨率分析来扩展U-Net的最简单方法是将3*3和7*7卷积运算与5*5卷积运算并行地合并，如图a所示。<br>论文中使用一系列更小，更轻便的3*3卷积块来分解更大，更苛刻的5*5和7*7卷积层，如图b所示。<br>2个3*3卷积块的输出有效地近似5*5卷积运算，3个3*3卷积块的输出有效地近似7*7卷积运算。<br>最终MultiResUnet使用了三个3*3的卷积替换了U-Net中的模块，并且引入了1*1卷积层，添加了剩余连接，使模型能够理解一些其他空间信息。<br><img src="https://i.loli.net/2021/07/05/WMOix2Y1JyXc3DA.png" alt="MultiRes U-Net中的模块详细结构"></li><li><strong>Res Path</strong><br>引入残差连接，不是简单地将特征图从编码器级连接到解码器级，而是先将它们<strong>穿过带有残差连接的卷积层链，然后再与解码器特征连接</strong>。<br><img src="https://i.loli.net/2021/07/05/BAfbVU9ECgsH8xv.png" alt="MultiRes U-Net中的Residual Path"></li></ul><h1 id="六、R2-U-Net"><a href="#六、R2-U-Net" class="headerlink" title="六、R2 U-Net"></a><strong>六、R2 U-Net</strong></h1><h2 id="1、Contribution-2"><a href="#1、Contribution-2" class="headerlink" title="1、Contribution"></a><strong>1、Contribution</strong></h2><ul><li>提出了两个新模型RU-Net和R2U-Net用于医学图像分割。</li><li>对医学成像的三种不同模式进行了实验，包括视网膜血管分割，皮肤癌分割和肺分割。</li><li>对基于模型的视网膜血管分割任务和基于端到端图像的皮肤病变和肺分割任务的模型，对提出的模型进行性能评估。</li><li>与最近提出的最新方法进行比较，该方法与具有相同数量网络参数的等效模型相比具有<strong>优越的性能</strong>。</li></ul><h2 id="2、Architecture-2"><a href="#2、Architecture-2" class="headerlink" title="2、Architecture"></a><strong>2、Architecture</strong></h2><p><img src="https://i.loli.net/2021/07/05/ZpW2dLlRz693COE.png" alt="(a)为普通的两个Conv模块，(b)为使用了Recurrent Conv的模块，(c)为使用了Residual Conv的模块，(d)是同时使用了Residual和Recurrent Conv的模块(即R2)"><br><img src="https://i.loli.net/2021/07/05/mg4VzWa1nGPO6JU.png" alt="Unfolded Recurrent Convolutional Units"></p><ul><li>一个Recurrent Block其实就是t个普通的Conv（Conv+BN+ReLU）的堆叠，只是每个堆叠层的输入有所变化，第一个Conv的输入为X，之后的Cconv的输入为前一个Conv的输出加上X，写成代码则为：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class Recurrent_block(nn.Module):<br>    def __init__(self, out_channels, t=2):<br>        super(Recurrent_block, self).__init__()<br>        self.t = t<br>        self.out_channels = out_channels<br>        self.conv = nn.Sequential(<br>            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),<br>    nn.BatchNorm2d(out_channels),<br>nn.ReLU(inplace=True)<br>        )<br>    def forward(self,x):<br>        for i in range(self.t):<br>            if i==0:<br>                x1 = self.conv(x)<br>            x1 = self.conv(x+x1)<br>        return x1<br></code></pre></td></tr></table></figure><img src="https://i.loli.net/2021/07/05/6S8tnJWjUEBP5KX.png" alt="Recurrent U-Net"></li><li>在U-Net的基础上添加了Recurrent Conv模块来改进得到的RU-Net（Recurrent U-Net），如上图。</li><li>在上图的基础上加入如ResU-Net的残差模块后得到R2U-Net（Recurrent Residual U-Net）。</li></ul><h1 id="七、Attention-U-Net"><a href="#七、Attention-U-Net" class="headerlink" title="七、Attention U-Net"></a><strong>七、Attention U-Net</strong></h1><p>Codes: <a href="https://github.com/SvyJ/svyj.github.io/blob/master/codes/Attention_UNet.py">https://github.com/SvyJ/svyj.github.io/blob/master/codes/Attention_UNet.py</a><br>Attention，即为注意力机制。在医疗图像中，就是把注意力集中到对特定任务有用的显著特征（比如说相关组织或者是器官），抑制输入图像中的不相关区域。在级联神经网络中，需要明确的外部组织&#x2F;器官定位模块，而使用Attention就不需要了。</p><h2 id="1、Contribution-3"><a href="#1、Contribution-3" class="headerlink" title="1、Contribution"></a><strong>1、Contribution</strong></h2><ul><li>进一步采用注意力方法，提出基于网格的门控（Grid-Based Gating），使注意系数（Attention Coefficients）更加特定于局部区域。 与基于全局特征向量的门控相比，这提高了性能。 此外，本方法可用于密集预测，因为不执行自适应池化。</li><li>提出了应用于医学成像任务的前馈CNN模型中的软注意（Soft-Attention）技术的第一个用例之一。 提出的注意力可以取代图像分类中使用的硬注意方法和图像分割框架中的外部器官定位模型。</li><li>提出了标准U-Net模型的扩展，以提高模型对前景（Foreground）像素的灵敏度，而无需复杂的启发式算法。 通过实验观察到U-Net的准确度改进在不同的图像数据集中是一致的。</li></ul><h2 id="2、Architecture-3"><a href="#2、Architecture-3" class="headerlink" title="2、Architecture"></a><strong>2、Architecture</strong></h2><p><img src="https://i.loli.net/2021/07/05/agW8D9eQtEMYOvm.png" alt="Attention Gate"></p><ul><li>Xl代表输入Feature Map, g代表Gate Signal，Wg:1*1*1，Wx:1*1*1，代表三维卷积核尺寸为1*1*1的卷积。</li><li>Attention Gate可以理解为：<strong>输入Feature Map和Gate Signal在Add之后依次经过1*1*1的卷积核压缩，ReLU，再压缩，Sigmoid增加非线性，得到与输入Feature Map相同大小的注意力系数，将注意力系数与输入Feature Map相乘得到输出</strong><br><img src="https://i.loli.net/2021/07/05/7krwMtndac5Ph9z.png" alt="Attention U-Net"></li><li>如上图为带有Attention Gate的U-Net模型，与标准U-Net相比，改进的地方在于Skip-Connection增加了Attention Gate，而<strong>Gating Signal是采用的较深层Feature Map</strong>。</li></ul><h1 id="八、U-Net-3"><a href="#八、U-Net-3" class="headerlink" title="八、U-Net 3+"></a><strong>八、U-Net 3+</strong></h1><h2 id="1、Contribution-4"><a href="#1、Contribution-4" class="headerlink" title="1、Contribution"></a><strong>1、Contribution</strong></h2><ul><li>设计了一种新的网络结构U-Net3+，通过引入全尺度的跳过连接，在全尺度特征映射中融合了低层细节和高层语义，充分利用了多尺度特征的同时具有更少的参数；</li><li>通过深度监督让网络从全尺度特征中学习分割表示，提出了更优的混合损失函数以增强器官的边界；</li><li>提出分类指导模块，通过与图像分类分支联合训练的方式，减少了网络在非器官图像的过度分割（Over-Segmentation）；</li><li>在肝脏和脾脏数据集上进行了广泛的实验，证明了U-Net 3+的有效性。</li></ul><h2 id="2、Architecture-4"><a href="#2、Architecture-4" class="headerlink" title="2、Architecture"></a><strong>2、Architecture</strong></h2><p><img src="https://i.loli.net/2021/07/05/ZCzPdn7qOWG539S.png" alt="U-Net(a)，U-Net++(b)，U-Net 3+(c)"></p><h2 id="3、Tricks-1"><a href="#3、Tricks-1" class="headerlink" title="3、Tricks"></a><strong>3、Tricks</strong></h2><p>UNET 3+论文笔记：<a href="https://zhuanlan.zhihu.com/p/134119120">https://zhuanlan.zhihu.com/p/134119120</a></p><ul><li>全尺度跳过连接</li><li>全尺度的深度监督<br>为了进一步<strong>优化网络对图像边界的分割</strong>，文章借鉴了图像质量评估中常用的多尺度SSIM（MS-SSIM）提出了MS-SSIM loss。本文最终采用了混合损失函数（Focal Loss，MS_SSIM_Loss和iou loss）来对各层进行监督。<br><img src="https://i.loli.net/2021/07/05/lT3dR2n9eNXYIwF.png" alt="MS SSIM Loss"></li><li>分类指导模块（CGM）</li></ul><h1 id="附：UNet-family论文及代码"><a href="#附：UNet-family论文及代码" class="headerlink" title="附：UNet-family论文及代码"></a><strong>附：UNet-family论文及代码</strong></h1><h2 id="2015"><a href="#2015" class="headerlink" title="2015"></a>2015</h2><ul><li>U-Net: Convolutional Networks for Biomedical Image Segmentation (MICCAI) [<a href="https://arxiv.org/pdf/1505.04597.pdf">paper</a>]  [<a href="https://github.com/ShawnBIT/UNet-family/blob/master/networks/UNet.py">my-pytorch</a>][<a href="https://github.com/zhixuhao/unet">keras</a>]</li></ul><h2 id="2016"><a href="#2016" class="headerlink" title="2016"></a>2016</h2><ul><li>V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation [<a href="http://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf">paper</a>] [<a href="https://github.com/faustomilletari/VNet">caffe</a>][<a href="https://github.com/mattmacy/vnet.pytorch">pytorch</a>]</li><li>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation [<a href="https://arxiv.org/pdf/1606.06650.pdf">paper</a>][<a href="https://github.com/wolny/pytorch-3dunet">pytorch</a>]</li></ul><h2 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h2><ul><li>H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes (IEEE Transactions on Medical Imaging)[<a href="https://arxiv.org/pdf/1709.07330.pdf">paper</a>][<a href="https://github.com/xmengli999/H-DenseUNet">keras</a>]</li><li>GP-Unet: Lesion Detection from Weak Labels with a 3D Regression Network (MICCAI) [<a href="https://arxiv.org/pdf/1705.07999.pdf">paper</a>]</li></ul><h2 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h2><ul><li>UNet++: A Nested U-Net Architecture for Medical Image Segmentation (MICCAI) [<a href="https://arxiv.org/pdf/1807.10165.pdf">paper</a>][<a href="https://github.com/ShawnBIT/UNet-family/blob/master/networks/UNet_Nested.py">my-pytorch</a>][<a href="https://github.com/MrGiovanni/UNetPlusPlus">keras</a>]</li><li>MDU-Net: Multi-scale Densely Connected U-Net for biomedical image segmentation [<a href="https://arxiv.org/pdf/1812.00352.pdf">paper</a>]</li><li>DUNet: A deformable network for retinal vessel segmentation [<a href="https://arxiv.org/pdf/1811.01206.pdf">paper</a>]</li><li>RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans [<a href="https://arxiv.org/pdf/1811.01328.pdf">paper</a>]</li><li>Dense Multi-path U-Net for Ischemic Stroke Lesion Segmentation in Multiple Image Modalities [<a href="https://arxiv.org/pdf/1810.07003.pdf">paper</a>]</li><li>Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment [<a href="https://arxiv.org/pdf/1812.01936.pdf">paper</a>]</li><li>Prostate Segmentation using 2D Bridged U-net [<a href="https://arxiv.org/pdf/1807.04459.pdf">paper</a>]</li><li>nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation [<a href="https://arxiv.org/pdf/1809.10486.pdf">paper</a>][<a href="https://github.com/MIC-DKFZ/nnUNet">pytorch</a>]</li><li>SUNet: a deep learning architecture for acute stroke lesion segmentation and<br>outcome prediction in multimodal MRI [<a href="https://arxiv.org/pdf/1810.13304.pdf">paper</a>]</li><li>IVD-Net: Intervertebral disc localization and segmentation in MRI with a multi-modal UNet [<a href="https://arxiv.org/pdf/1811.08305.pdf">paper</a>]</li><li>LADDERNET: Multi-Path Networks Based on U-Net for Medical Image Segmentation [<a href="https://arxiv.org/pdf/1810.07810.pdf">paper</a>][<a href="https://github.com/juntang-zhuang/LadderNet">pytorch</a>]</li><li>Glioma Segmentation with Cascaded Unet [<a href="https://arxiv.org/pdf/1810.04008.pdf">paper</a>]</li><li>Attention U-Net: Learning Where to Look for the Pancreas [<a href="https://arxiv.org/pdf/1804.03999.pdf">paper</a>]</li><li>Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation [<a href="https://arxiv.org/pdf/1802.06955.pdf">paper</a>]</li><li>Concurrent Spatial and Channel ‘Squeeze &amp; Excitation’ in Fully Convolutional Networks <a href="https://arxiv.org/pdf/1803.02579.pdf">[paper]</a></li><li>A Probabilistic U-Net for Segmentation of Ambiguous Images (NIPS) [<a href="https://arxiv.org/pdf/1806.05034.pdf">paper</a>] [<a href="https://github.com/SimonKohl/probabilistic_unet">tensorflow</a>]</li><li>AnatomyNet: Deep Learning for Fast and Fully Automated Whole-volume Segmentation of Head and Neck Anatomy [<a href="https://arxiv.org/pdf/1808.05238.pdf">paper</a>]</li><li>3D RoI-aware U-Net for Accurate and Efficient Colorectal Cancer Segmentation [<a href="https://arxiv.org/pdf/1806.10342.pdf">paper</a>][<a href="https://github.com/huangyjhust/3D-RU-Net">pytorch</a>]</li><li>Detection and Delineation of Acute Cerebral Infarct on DWI Using Weakly Supervised Machine Learning (Y-Net) (MICCAI) [<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00931-1.pdf">paper</a>](Page 82)</li><li>Fully Dense UNet for 2D Sparse Photoacoustic Tomography Artifact Removal [<a href="https://arxiv.org/pdf/1808.10848.pdf">paper</a>]</li></ul><h2 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h2><ul><li>MultiResUNet : Rethinking the U-Net Architecture for Multimodal Biomedical Image Segmentation [<a href="https://arxiv.org/pdf/1902.04049v1.pdf">paper</a>][<a href="https://github.com/nibtehaz/MultiResUNet">keras</a>]</li><li>U-NetPlus: A Modified Encoder-Decoder U-Net Architecture for Semantic and Instance Segmentation of Surgical Instrument [<a href="https://arxiv.org/pdf/1902.08994.pdf">paper</a>]</li><li>Probability Map Guided Bi-directional Recurrent UNet for Pancreas Segmentation [<a href="https://arxiv.org/pdf/1903.00923.pdf">paper</a>]</li><li>CE-Net: Context Encoder Network for 2D Medical Image Segmentation [<a href="https://arxiv.org/pdf/1903.02740.pdf">paper</a>][<a href="https://github.com/Guzaiwang/CE-Net">pytorch</a>]</li><li>Graph U-Net [<a href="https://openreview.net/pdf?id=HJePRoAct7">paper</a>]</li><li>A Novel Focal Tversky Loss Function with Improved Attention U-Net for Lesion Segmentation (ISBI) [<a href="https://arxiv.org/pdf/1810.07842.pdf">paper</a>]</li><li>ST-UNet: A Spatio-Temporal U-Network for Graph-structured Time Series Modeling [<a href="https://arxiv.org/pdf/1903.05631.pdf">paper</a>]</li><li>Connection Sensitive Attention U-NET for Accurate Retinal Vessel Segmentation [<a href="https://arxiv.org/pdf/1903.05558.pdf">paper</a>]</li><li>CIA-Net: Robust Nuclei Instance Segmentation with Contour-aware Information Aggregation [<a href="https://arxiv.org/pdf/1903.05358.pdf">paper</a>]</li><li>W-Net: Reinforced U-Net for Density Map Estimation [<a href="https://arxiv.org/pdf/1903.11249.pdf">paper</a>]</li><li>Automated Segmentation of Pulmonary Lobes using Coordination-guided Deep Neural Networks (ISBI oral) [<a href="https://arxiv.org/pdf/1904.09106.pdf">paper</a>]</li><li>U2-Net: A Bayesian U-Net Model with Epistemic Uncertainty Feedback for Photoreceptor Layer Segmentation in Pathological OCT Scans [<a href="https://arxiv.org/pdf/1901.07929.pdf">paper</a>]</li><li>ScleraSegNet: an Improved U-Net Model with Attention for Accurate Sclera Segmentation (ICB Honorable Mention Paper Award) [<a href="https://github.com/ShawnBIT/Paper-Reading/blob/master/ScleraSegNet.pdf">paper</a>]</li><li>AHCNet: An Application of Attention Mechanism and Hybrid Connection for Liver Tumor Segmentation in CT Volumes [<a href="https://github.com/ShawnBIT/Paper-Reading/blob/master/AHCNet.pdf">paper</a>]</li><li>A Hierarchical Probabilistic U-Net for Modeling Multi-Scale Ambiguities [<a href="https://arxiv.org/pdf/1905.13077.pdf">paper</a>]</li><li>Recurrent U-Net for Resource-Constrained Segmentation [<a href="https://arxiv.org/pdf/1906.04913.pdf">paper</a>]</li><li>MFP-Unet: A Novel Deep Learning Based Approach for Left Ventricle Segmentation in Echocardiography [<a href="https://arxiv.org/pdf/1906.10486.pdf">paper</a>]</li><li>A Partially Reversible U-Net for Memory-Efficient Volumetric Image Segmentation (MICCAI 2019) [<a href="https://arxiv.org/pdf/1906.06148.pdf">paper</a>][<a href="https://github.com/RobinBruegger/PartiallyReversibleUnet">pytorch</a>]</li><li>ResUNet-a: a deep learning framework for semantic segmentation of remotely sensed data [<a href="https://arxiv.org/pdf/1904.00592v2.pdf">paper</a>]</li><li>A multi-task U-net for segmentation with lazy labels [<a href="https://arxiv.org/pdf/1906.12177.pdf">paper</a>]</li><li>RAUNet: Residual Attention U-Net for Semantic Segmentation of Cataract Surgical Instruments [<a href="http://xxx.itp.ac.cn/pdf/1909.10360v1">paper</a>]</li><li>3D U2-Net: A 3D Universal U-Net for Multi-Domain Medical Image Segmentation (MICCAI 2019) [<a href="https://arxiv.org/pdf/1909.06012.pdf">paper</a>] [<a href="https://github.com/huangmozhilv/u2net_torch/">pytorch</a>]</li><li>SegNAS3D: Network Architecture Search with Derivative-Free Global Optimization for 3D Image Segmentation (MICCAI 2019) [<a href="https://arxiv.org/pdf/1909.05962.pdf">paper</a>]</li><li>3D Dilated Multi-Fiber Network for Real-time Brain Tumor Segmentation in MRI [<a href="https://arxiv.org/pdf/1904.03355.pdf">paper</a>][<a href="https://github.com/China-LiuXiaopeng/BraTS-DMFNet">pytorch</a>] (MICCAI 2019)</li><li>The Domain Shift Problem of Medical Image Segmentation and Vendor-Adaptation by Unet-GAN [<a href="https://arxiv.org/pdf/1910.13681.pdf">paper</a>]</li><li>Recurrent U-Net for Resource-Constrained Segmentation [<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Recurrent_U-Net_for_Resource-Constrained_Segmentation_ICCV_2019_paper.pdf">paper</a>] (ICCV 2019)</li><li>Siamese U-Net with Healthy Template for Accurate Segmentation of Intracranial Hemorrhage (MICCAI 2019)</li></ul><h2 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h2><ul><li>U^2-Net: Going Deeper with Nested U-Structure for Salient Object Detection (Pattern Recognition 2020) [<a href="https://arxiv.org/pdf/2005.09007v1.pdf">paper</a>][<a href="https://github.com/NathanUA/U-2-Net">pytorch</a>]</li><li>UNET 3+: A Full-Scale Connected UNet for Medical Image Segmentation (ICASSP 2020) [<a href="https://arxiv.org/pdf/2004.08790.pdf">paper</a>][<a href="https://github.com/ZJUGiveLab/UNet-Version">pytorch</a>]</li></ul>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 图像分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义分割综述</title>
      <link href="/2022/07/01/010-Semantic_Segementation/"/>
      <url>/2022/07/01/010-Semantic_Segementation/</url>
      
        <content type="html"><![CDATA[<h1 id="一、语义分割、实例分割和全景分割"><a href="#一、语义分割、实例分割和全景分割" class="headerlink" title="一、语义分割、实例分割和全景分割"></a><strong>一、语义分割、实例分割和全景分割</strong></h1><p><img src="https://i.loli.net/2021/07/05/z3mAyiTIeYhFDQO.png" alt="Segmentation"></p><h2 id="1、通俗理解"><a href="#1、通俗理解" class="headerlink" title="1、通俗理解"></a>1、通俗理解</h2><h3 id="（1）语义分割：分割出每个类别，即对图片的每个像素做分类"><a href="#（1）语义分割：分割出每个类别，即对图片的每个像素做分类" class="headerlink" title="（1）语义分割：分割出每个类别，即对图片的每个像素做分类"></a>（1）语义分割：分割出每个类别，即对图片的每个像素做分类</h3><p><img src="https://i.loli.net/2021/07/05/fHlDsbJ71j3whq5.png" alt="Semantic Segmentation"></p><h3 id="（2）实例分割：分割出每个实例（不含背景）"><a href="#（2）实例分割：分割出每个实例（不含背景）" class="headerlink" title="（2）实例分割：分割出每个实例（不含背景）"></a>（2）实例分割：分割出每个实例（不含背景）</h3><p><img src="https://i.loli.net/2021/07/05/xZ5RkFYwtPg96es.png" alt="Instance Segmentation"></p><h3 id="（3）全景分割：分割出每个实例（含背景）"><a href="#（3）全景分割：分割出每个实例（含背景）" class="headerlink" title="（3）全景分割：分割出每个实例（含背景）"></a>（3）全景分割：分割出每个实例（含背景）</h3><h1 id="二、语义分割的方法"><a href="#二、语义分割的方法" class="headerlink" title="二、语义分割的方法"></a><strong>二、语义分割的方法</strong></h1><h2 id="1、传统机器学习方法"><a href="#1、传统机器学习方法" class="headerlink" title="1、传统机器学习方法"></a>1、传统机器学习方法</h2><ul><li>传统方法Pipeline: 特征 + forst&#x2F;boost + CRF</li><li>劣势：单个学习分类器只针对单一的类别设计，导致分割类别多时有<strong>计算复杂度高</strong>和<strong>训练难度大</strong>的问题<br>（如像素级的决策树分类，参考TextonForest以及Random Forest based classifiers）</li></ul><h2 id="2、深度学习方法"><a href="#2、深度学习方法" class="headerlink" title="2、深度学习方法"></a>2、深度学习方法</h2><ul><li><strong>卷积神经网络</strong>：FCN、DeepLab-V1(2014), SegNet、UNet(2015), DeepLab-V2(2016)…<br>（一般都是在分类网络上进行精调，分类网络为了能获取更抽象的特征分层，采取了Conv+pool堆叠的方式，这导致了分辨率降低，丢失了很多信息，这对分割任务来说肯定是不好的，因为分割是对每一个像素进行分类，会造成定位精度不高。但同时更高层的特征对于分类又很重要。如何权衡这两者呢？）</li><li><strong>Encoder-Decoder方法</strong>：与经典的FCN中的Skip-Connection思想类似，Encoder为分类网络，用于提取特征，而Decoder则是将Encoder的先前丢失的空间信息逐渐恢复，Decoder的典型结构有U-Net&#x2F;Segnet&#x2F;RefineNet，该类方法虽然有一定的效果，能恢复部分信息，但毕竟信息已经丢失了，不可能完全恢复。</li><li><strong>Dialed FCN方法</strong>：Deeplab-V1提出的方法，将VGG的最后的两个Pool层步长置为1，这样网络的输出分辨率从1&#x2F;32变为1&#x2F;8。可以保留更多的细节信息，同时也丢掉了复杂的Decoder结构，但这种方法计算量大。</li><li>注：DeepLab-V3将Encoder-Decoder方法与Dialed FCN方法结合，达到了非常好的效果，同时计算量也非常巨大</li></ul><h1 id="三、语义分割的难点"><a href="#三、语义分割的难点" class="headerlink" title="三、语义分割的难点"></a><strong>三、语义分割的难点</strong></h1><ul><li>数据集问题：需要精确的像素级标注</li><li>计算资源问题：要求高精度 -&gt; 深层网络 -&gt; 分割预测每一个像素点 -&gt; 要求Feature Map有尽量高的分辨率 -&gt; 计算资源不足</li><li>精细分割：（1）<strong>大类别、小目标</strong>：分割精度高 （2）<strong>小类别、小目标</strong>：轮廓太小 -&gt; 分割精度低</li><li>上下文信息：忽略上下文信息会造成 一个类别目标分成多个类别part、不同类别目标分成相同类别<br>（什么是上下文信息？察觉并能应用能够影响场景和图像中的对象的一些或全部信息，通俗理解为<strong>像素以及周边像素的联系</strong>）</li></ul><h1 id="四、语义分割模型"><a href="#四、语义分割模型" class="headerlink" title="四、语义分割模型"></a><strong>四、语义分割模型</strong></h1><ul><li>一般的语义分割架构可以被认为是一个<strong>编码器-解码器</strong>网络。<br>编码器通常是一个预训练的分类网络，像VGG、ResNet，然后是一个解码器网络。<br>这些架构不同的地方主要在于解码器网络。解码器的任务是将编码器学习到的可判别特征（较低分辨率）从语义上投影到像素空间（较高分辨率），以获得密集分类。</li><li>不同于分类任务中网络的最终结果（对图像分类的概率）是唯一重要的事，语义分割不仅需要在像素级有判别能力，还需要有能将编码器在不同阶段学到的可判别特征投影到像素空间的机制。不同的架构采用不同的机制（跳跃连接、金字塔池化等）作为解码机制的一部分。</li></ul><h2 id="1、FCN"><a href="#1、FCN" class="headerlink" title="1、FCN"></a><strong>1、FCN</strong></h2><h3 id="（1）Architecture"><a href="#（1）Architecture" class="headerlink" title="（1）Architecture"></a><strong>（1）Architecture</strong></h3><p><img src="https://i.loli.net/2021/07/05/gDl2RIP3Xn9tJqA.png" alt="FCN"></p><h3 id="（2）Contribution"><a href="#（2）Contribution" class="headerlink" title="（2）Contribution"></a><strong>（2）Contribution</strong></h3><ul><li>为语义分割引入了<strong>端到端</strong>的全卷积网络，并流行开来</li><li>重新利用ImageNet的<strong>预训练网络</strong>用于语义分割</li><li>使用<strong>反卷积层</strong>代替线性插值法进行上采样</li><li>引入<strong>跳跃连接</strong>来改善上采样粗糙的像素定位</li></ul><h2 id="2、DeconvNet"><a href="#2、DeconvNet" class="headerlink" title="2、DeconvNet"></a><strong>2、DeconvNet</strong></h2><h3 id="（1）Architecture-1"><a href="#（1）Architecture-1" class="headerlink" title="（1）Architecture"></a><strong>（1）Architecture</strong></h3><p><img src="https://i.loli.net/2021/07/05/x78aYZkjCfKR2Qv.png" alt="DeConvNet"></p><h3 id="（2）Contribution-1"><a href="#（2）Contribution-1" class="headerlink" title="（2）Contribution"></a><strong>（2）Contribution</strong></h3><ul><li>UpPooling过程中与SegNet类似，但除了还原记录的Pooling Indices之外，<strong>其他位置均补0</strong></li></ul><h2 id="3、SegNet"><a href="#3、SegNet" class="headerlink" title="3、SegNet"></a><strong>3、SegNet</strong></h2><h3 id="（1）Architecture-2"><a href="#（1）Architecture-2" class="headerlink" title="（1）Architecture"></a><strong>（1）Architecture</strong></h3><p><img src="https://i.loli.net/2021/07/05/YGlNT3qsgjM5imp.png" alt="SegNet"></p><h3 id="（2）Contribution-2"><a href="#（2）Contribution-2" class="headerlink" title="（2）Contribution"></a><strong>（2）Contribution</strong></h3><ul><li>将<strong>池化层结果</strong>应用到译码过程</li><li>引入了更多的编码信息</li><li>使用的是<strong>Pooling Indices</strong>，而不是直接复制特征，只是将编码过程中Pool的位置记下来，在UpPooling是使用该信息进行Pooling</li></ul><h2 id="4、UNet"><a href="#4、UNet" class="headerlink" title="4、UNet"></a><strong>4、UNet</strong></h2><h3 id="（1）Architecture-3"><a href="#（1）Architecture-3" class="headerlink" title="（1）Architecture"></a><strong>（1）Architecture</strong></h3><p><img src="https://i.loli.net/2021/07/05/ahHDQsSkVb32qTn.png" alt="UNet"></p><h3 id="（2）Contribution-3"><a href="#（2）Contribution-3" class="headerlink" title="（2）Contribution"></a><strong>（2）Contribution</strong></h3><ul><li>更规整的网络结构</li><li>通过将编码器的每层结果拼接到译码器中得到更好的结果</li></ul><h2 id="5、PSPNet"><a href="#5、PSPNet" class="headerlink" title="5、PSPNet"></a><strong>5、PSPNet</strong></h2><h3 id="（1）Architecture-4"><a href="#（1）Architecture-4" class="headerlink" title="（1）Architecture"></a><strong>（1）Architecture</strong></h3><p><img src="https://i.loli.net/2021/07/05/RnkbZFpxia9o3Bl.png"><br><img src="https://i.loli.net/2021/07/05/8pde1PxSbHFikyn.png" alt="PSPNet"></p><h3 id="（2）Contribution-4"><a href="#（2）Contribution-4" class="headerlink" title="（2）Contribution"></a><strong>（2）Contribution</strong></h3><ul><li>提出了<strong>金字塔池化模块</strong>来聚合图片信息</li><li>使用附加的<strong>损失函数</strong></li></ul><h2 id="6、RefineNet"><a href="#6、RefineNet" class="headerlink" title="6、RefineNet"></a><strong>6、RefineNet</strong></h2><h3 id="（1）Architecture-5"><a href="#（1）Architecture-5" class="headerlink" title="（1）Architecture"></a><strong>（1）Architecture</strong></h3><p><img src="https://i.loli.net/2021/07/05/o5dVb6LP3syM9qO.png" alt="RefineNet"></p><h3 id="（2）Contribution-5"><a href="#（2）Contribution-5" class="headerlink" title="（2）Contribution"></a><strong>（2）Contribution</strong></h3><ul><li>精心设计的<strong>译码模块</strong></li><li>所有模块遵循<strong>残差连接</strong>设计</li></ul>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> 图像分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一些常用的网络结构中的Module和Block</title>
      <link href="/2022/07/01/009-DL_ModuleAndBlocks/"/>
      <url>/2022/07/01/009-DL_ModuleAndBlocks/</url>
      
        <content type="html"><![CDATA[<h1 id="Group-Convolution"><a href="#Group-Convolution" class="headerlink" title="Group Convolution"></a><strong>Group Convolution</strong></h1><ul><li>分组卷积（来自AlexNet）详解： <a href="https://www.jianshu.com/p/a936b7bc54e3">https://www.jianshu.com/p/a936b7bc54e3</a><br><img src="https://i.loli.net/2021/07/05/LOKtuhrVn1IzcMQ.png" alt="Group Convolution"></li></ul><h1 id="Inception-Module"><a href="#Inception-Module" class="headerlink" title="Inception Module"></a><strong>Inception Module</strong></h1><ul><li>多尺度特征提取再融合（来自Inception-V1）<br><img src="https://i.loli.net/2021/07/05/o1YXLwst4yxKDpN.png" alt="Inception Module Naive"><br><img src="https://i.loli.net/2021/07/05/dGZsqEUuWJRgFtx.png" alt="Inception Module"></li></ul><h1 id="Residual-Block"><a href="#Residual-Block" class="headerlink" title="Residual Block"></a><strong>Residual Block</strong></h1><ul><li>残差结构（来自ResNet，先降维再升维）<br><img src="https://i.loli.net/2021/07/05/4yzacFg9E5HL278.png" alt="Residual Block"></li></ul><h1 id="Element-wise-Addition"><a href="#Element-wise-Addition" class="headerlink" title="Element-wise Addition"></a><strong>Element-wise Addition</strong></h1><ul><li>更激进的密集连接机制（来自ResNet）<br><img src="https://i.loli.net/2021/07/05/syD4ke5ThOYurPt.png" alt="ResNet网络的短路连接机制（其中+代表的是元素级相加操作）"></li></ul><h1 id="BottleNeck"><a href="#BottleNeck" class="headerlink" title="BottleNeck"></a><strong>BottleNeck</strong></h1><ul><li>瓶颈结构（来自ResNet）<br><img src="https://i.loli.net/2021/07/05/Il3QpnzgZud2X9S.png" alt="BottleNeck"></li></ul><h1 id="Channel-wise-Concatenation"><a href="#Channel-wise-Concatenation" class="headerlink" title="Channel-wise Concatenation"></a><strong>Channel-wise Concatenation</strong></h1><ul><li>实现特征重用，提升效率（来自DenseNet）<br><img src="https://i.loli.net/2021/07/05/1oBtSAL32kVpi9d.png" alt="DenseNet网络的密集连接机制"></li></ul><h1 id="Inverted-Residual-Block"><a href="#Inverted-Residual-Block" class="headerlink" title="Inverted Residual Block"></a><strong>Inverted Residual Block</strong></h1><ul><li>倒残差结构（来自MobileNet-V2，先升维再降维）<table><thead><tr><th align="center">Input</th><th align="center">Operator</th><th align="center">Output</th></tr></thead><tbody><tr><td align="center">H×W×tK</td><td align="center">1×1 conv2d, ReLU6</td><td align="center">H×W×tK</td></tr><tr><td align="center">H×W×tK</td><td align="center">3×3 dwise s&#x3D;s, ReLU6</td><td align="center">H&#x2F;s×W&#x2F;s×tK</td></tr><tr><td align="center">H&#x2F;s×W&#x2F;s×tK</td><td align="center">linear 1×1 conv2d</td><td align="center">H&#x2F;s×W&#x2F;s×K’</td></tr></tbody></table></li></ul><p><img src="https://i.loli.net/2021/07/05/SnlzHsNB6Uu9eT2.png" alt="Inverted Residual Block"></p><h1 id="Squeeze-and-Excitation-Block"><a href="#Squeeze-and-Excitation-Block" class="headerlink" title="Squeeze-and-Excitation-Block"></a><strong>Squeeze-and-Excitation-Block</strong></h1><ul><li>压缩-扩张模块（来自SeNet）<br><img src="https://i.loli.net/2021/07/05/9NroyP4m5SECJba.png" alt="Squeeze-and-Excitation-Block"></li></ul><h1 id="未完…"><a href="#未完…" class="headerlink" title="未完…"></a><strong>未完…</strong></h1>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch中常用的Transforms方法</title>
      <link href="/2022/07/01/019-PytorchTransforms/"/>
      <url>/2022/07/01/019-PytorchTransforms/</url>
      
        <content type="html"><![CDATA[<h1 id="一、裁剪——Crop"><a href="#一、裁剪——Crop" class="headerlink" title="一、裁剪——Crop"></a><strong>一、裁剪——Crop</strong></h1><h2 id="1、随机裁剪：transforms-RandomCrop"><a href="#1、随机裁剪：transforms-RandomCrop" class="headerlink" title="1、随机裁剪：transforms.RandomCrop"></a>1、随机裁剪：transforms.RandomCrop</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=‘constant’)<br></code></pre></td></tr></table></figure><ul><li>功能：依据给定的size随机裁剪</li><li>参数：<br><strong>size</strong> - (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)<br><strong>padding</strong> - (sequence or int, optional)，此参数是设置填充多少个pixel。当为int时，图像上下左右均填充int个，例如padding&#x3D;4，则上下左右均填充4个pixel，若为3232，则会变成4040。当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。<br><strong>fill</strong> - (int or tuple) 填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。<br>padding_mode- 填充模式，这里提供了4种填充模式，1.constant，常量。2.edge 按照图片边缘的像素值来填充。3.reflect，暂不了解。 4. symmetric，暂不了解。</li></ul><h2 id="2、中心裁剪：transforms-CenterCrop"><a href="#2、中心裁剪：transforms-CenterCrop" class="headerlink" title="2、中心裁剪：transforms.CenterCrop"></a>2、中心裁剪：transforms.CenterCrop</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.CenterCrop(size)<br></code></pre></td></tr></table></figure><ul><li>功能：依据给定的size从中心裁剪</li><li>参数：<br><strong>size</strong> - (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)</li></ul><h2 id="3、随机长宽比裁剪-transforms-RandomResizedCrop"><a href="#3、随机长宽比裁剪-transforms-RandomResizedCrop" class="headerlink" title="3、随机长宽比裁剪 transforms.RandomResizedCrop"></a>3、随机长宽比裁剪 transforms.RandomResizedCrop</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)<br></code></pre></td></tr></table></figure><ul><li>功能：随机大小，随机长宽比裁剪原始图片，最后将图片resize到设定好的size</li><li>参数：<br><strong>size</strong> - 输出的分辨率<br><strong>scale</strong> - 随机crop的大小区间，如scale&#x3D;(0.08, 1.0)，表示随机crop出来的图片会在的0.08倍至1倍之间。<br><strong>ratio</strong> - 随机长宽比设置<br><strong>interpolation</strong> - 插值的方法，默认为双线性插值(PIL.Image.BILINEAR)</li></ul><h2 id="4、上下左右中心裁剪：transforms-FiveCrop"><a href="#4、上下左右中心裁剪：transforms-FiveCrop" class="headerlink" title="4、上下左右中心裁剪：transforms.FiveCrop"></a>4、上下左右中心裁剪：transforms.FiveCrop</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.FiveCrop(size)<br></code></pre></td></tr></table></figure><ul><li>功能：对图片进行上下左右以及中心裁剪，获得5张图片，返回一个4D-tensor</li><li>参数：<br><strong>size</strong> - (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)</li></ul><h2 id="5、上下左右中心裁剪后翻转-transforms-TenCrop"><a href="#5、上下左右中心裁剪后翻转-transforms-TenCrop" class="headerlink" title="5、上下左右中心裁剪后翻转: transforms.TenCrop"></a>5、上下左右中心裁剪后翻转: transforms.TenCrop</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.TenCrop(size, vertical_flip=False)<br></code></pre></td></tr></table></figure><ul><li>功能：对图片进行上下左右以及中心裁剪，然后全部翻转（水平或者垂直），获得10张图片，返回一个4D-tensor。</li><li>参数：<br><strong>size</strong> - (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)<br><strong>vertical_flip (bool)</strong> - 是否垂直翻转，默认为flase，即默认为水平翻转</li></ul><h1 id="二、翻转和旋转——Flip-and-Rotation"><a href="#二、翻转和旋转——Flip-and-Rotation" class="headerlink" title="二、翻转和旋转——Flip and Rotation"></a><strong>二、翻转和旋转——Flip and Rotation</strong></h1><h2 id="1、依概率p水平翻转transforms-RandomHorizontalFlip"><a href="#1、依概率p水平翻转transforms-RandomHorizontalFlip" class="headerlink" title="1、依概率p水平翻转transforms.RandomHorizontalFlip"></a>1、依概率p水平翻转transforms.RandomHorizontalFlip</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.RandomHorizontalFlip(p=0.5)<br></code></pre></td></tr></table></figure><ul><li>功能：依据概率p对PIL图片进行水平翻转</li><li>参数：<br><strong>p</strong> - 概率，默认值为0.5</li></ul><h2 id="2、依概率p垂直翻转transforms-RandomVerticalFlip"><a href="#2、依概率p垂直翻转transforms-RandomVerticalFlip" class="headerlink" title="2、依概率p垂直翻转transforms.RandomVerticalFlip"></a>2、依概率p垂直翻转transforms.RandomVerticalFlip</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.RandomVerticalFlip(p=0.5)<br></code></pre></td></tr></table></figure><ul><li>功能：依据概率p对PIL图片进行垂直翻转</li><li>参数：<br><strong>p</strong> - 概率，默认值为0.5</li></ul><h2 id="3、随机旋转：transforms-RandomRotation"><a href="#3、随机旋转：transforms-RandomRotation" class="headerlink" title="3、随机旋转：transforms.RandomRotation"></a>3、随机旋转：transforms.RandomRotation</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.RandomRotation(degrees, resample=False, expand=False, center=None)<br></code></pre></td></tr></table></figure><ul><li>功能：依degrees随机旋转一定角度</li><li>参数：<br><strong>degress</strong> - (sequence or float or int) ，若为单个数，如 30，则表示在（-30，+30）之间随机旋转<br>若为sequence，如(30，60)，则表示在30-60度之间随机旋转<br><strong>resample</strong> - 重采样方法选择，可选 PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC，默认为最近邻<br><strong>expand</strong> - 未了解<br><strong>center</strong> - 可选为中心旋转还是左上角旋转</li></ul><h1 id="三、图像变换"><a href="#三、图像变换" class="headerlink" title="三、图像变换"></a><strong>三、图像变换</strong></h1><h2 id="1、resize：transforms-Resize"><a href="#1、resize：transforms-Resize" class="headerlink" title="1、resize：transforms.Resize"></a>1、resize：transforms.Resize</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.Resize(size, interpolation=2)<br></code></pre></td></tr></table></figure><ul><li>功能：重置图像分辨率</li><li>参数：<br><strong>size</strong> - If size is an int, if height &gt; width, then image will be rescaled to (size * height &#x2F; width, size)，所以建议size设定为h*w<br><strong>interpolation</strong> - 插值方法选择，默认为PIL.Image.BILINEAR</li></ul><h2 id="2、标准化：transforms-Normalize"><a href="#2、标准化：transforms-Normalize" class="headerlink" title="2、标准化：transforms.Normalize"></a>2、标准化：transforms.Normalize</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.Normalize(mean, std)<br></code></pre></td></tr></table></figure><ul><li>功能：对数据按通道进行标准化，即先减均值，再除以标准差，注意是 chw</li></ul><h2 id="3、转为tensor：transforms-ToTensor"><a href="#3、转为tensor：transforms-ToTensor" class="headerlink" title="3、转为tensor：transforms.ToTensor"></a>3、转为tensor：transforms.ToTensor</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.ToTensor<br></code></pre></td></tr></table></figure><ul><li>功能：将PIL Image或者 ndarray 转换为tensor，并且归一化至[0-1]</li><li>注意事项：归一化至[0-1]是直接除以255，若自己的ndarray数据尺度有变化，则需要自行修改。</li></ul><h2 id="4、填充：transforms-Pad"><a href="#4、填充：transforms-Pad" class="headerlink" title="4、填充：transforms.Pad"></a>4、填充：transforms.Pad</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.Pad(padding, fill=0, padding_mode=‘constant’)<br></code></pre></td></tr></table></figure><ul><li>功能：对图像进行填充</li><li>参数：<br><strong>padding</strong> -(sequence or int, optional)，此参数是设置填充多少个pixel。<br>当为int时，图像上下左右均填充int个，例如padding&#x3D;4，则上下左右均填充4个pixel，若为3232，则会变成4040。<br>当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。<br><strong>fill</strong> - (int or tuple) 填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。<br><strong>padding_mode</strong> - 填充模式，这里提供了4种填充模式，1.constant，常量。2.edge 按照图片边缘的像素值来填充。</li></ul><h2 id="5、修改亮度、对比度和饱和度：transforms-ColorJitter"><a href="#5、修改亮度、对比度和饱和度：transforms-ColorJitter" class="headerlink" title="5、修改亮度、对比度和饱和度：transforms.ColorJitter"></a>5、修改亮度、对比度和饱和度：transforms.ColorJitter</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)<br></code></pre></td></tr></table></figure><ul><li>功能：修改修改亮度、对比度和饱和度</li></ul><h2 id="6、转灰度图：transforms-Grayscale"><a href="#6、转灰度图：transforms-Grayscale" class="headerlink" title="6、转灰度图：transforms.Grayscale"></a>6、转灰度图：transforms.Grayscale</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.Grayscale(num_output_channels=1)<br></code></pre></td></tr></table></figure><ul><li>功能：将图片转换为灰度图</li><li>参数：<br><strong>num_output_channels</strong> - (int) ，当为1时，正常的灰度图，当为3时， 3 channel with r &#x3D;&#x3D; g &#x3D;&#x3D; b</li></ul><h2 id="7、线性变换：transforms-LinearTransformation"><a href="#7、线性变换：transforms-LinearTransformation" class="headerlink" title="7、线性变换：transforms.LinearTransformation()"></a>7、线性变换：transforms.LinearTransformation()</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.LinearTransformation(transformation_matrix)<br></code></pre></td></tr></table></figure><ul><li>功能：对矩阵做线性变化，可用于白化处理！ whitening: zero-center the data, compute the data covariance matrix</li><li>参数：<br><strong>transformation_matrix (Tensor)</strong> – tensor [D x D], D &#x3D; C x H x W</li></ul><h2 id="8、仿射变换：transforms-RandomAffine"><a href="#8、仿射变换：transforms-RandomAffine" class="headerlink" title="8、仿射变换：transforms.RandomAffine"></a>8、仿射变换：transforms.RandomAffine</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)<br></code></pre></td></tr></table></figure><ul><li>功能：仿射变换</li></ul><h2 id="9、依概率p转为灰度图：transforms-RandomGrayscale"><a href="#9、依概率p转为灰度图：transforms-RandomGrayscale" class="headerlink" title="9、依概率p转为灰度图：transforms.RandomGrayscale"></a>9、依概率p转为灰度图：transforms.RandomGrayscale</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.RandomGrayscale(p=0.1)<br></code></pre></td></tr></table></figure><ul><li>功能：依概率p将图片转换为灰度图，若通道数为3，则3 channel with r &#x3D;&#x3D; g &#x3D;&#x3D; b</li></ul><h2 id="10、将数据转换为PILImage：transforms-ToPILImage"><a href="#10、将数据转换为PILImage：transforms-ToPILImage" class="headerlink" title="10、将数据转换为PILImage：transforms.ToPILImage"></a>10、将数据转换为PILImage：transforms.ToPILImage</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class torchvision.transforms.ToPILImage(mode=None)<br></code></pre></td></tr></table></figure><ul><li>功能：将tensor 或者 ndarray的数据转换为 PIL Image 类型数据</li><li>参数：<br><strong>mode</strong> - 为None时，为1通道， mode&#x3D;3通道默认转换为RGB，4通道默认转换为RGBA</li></ul><h2 id="11、transforms-Lambda"><a href="#11、transforms-Lambda" class="headerlink" title="11、transforms.Lambda"></a>11、transforms.Lambda</h2><p>Apply a user-defined lambda as a transform.<br>待补充。</p><h1 id="四、对transforms操作，使数据增强更灵活"><a href="#四、对transforms操作，使数据增强更灵活" class="headerlink" title="四、对transforms操作，使数据增强更灵活"></a><strong>四、对transforms操作，使数据增强更灵活</strong></h1><p>PyTorch不仅可设置对图片的操作，还可以对这些操作进行随机选择、组合</p><h2 id="1、transforms-RandomChoice-transforms"><a href="#1、transforms-RandomChoice-transforms" class="headerlink" title="1、transforms.RandomChoice(transforms)"></a>1、transforms.RandomChoice(transforms)</h2><ul><li>功能：从给定的一系列transforms中选一个进行操作，randomly picked from a list</li></ul><h2 id="2、transforms-RandomApply-transforms-p-x3D-0-5"><a href="#2、transforms-RandomApply-transforms-p-x3D-0-5" class="headerlink" title="2、transforms.RandomApply(transforms, p&#x3D;0.5)"></a>2、transforms.RandomApply(transforms, p&#x3D;0.5)</h2><ul><li>功能：给一个transform加上概率，以一定的概率执行该操作</li></ul><h2 id="3、transforms-RandomOrder"><a href="#3、transforms-RandomOrder" class="headerlink" title="3、transforms.RandomOrder"></a>3、transforms.RandomOrder</h2><ul><li>功能：将transforms中的操作顺序随机打乱</li></ul><p>转载自：<a href="https://blog.csdn.net/u011995719/article/details/85107009">https://blog.csdn.net/u011995719/article/details/85107009</a><br>参考：<br>《PyTorch 模型训练实用教程》，全文pdf请点击：<a href="https://github.com/tensor-yu/PyTorch_Tutorial">https://github.com/tensor-yu/PyTorch_Tutorial</a><br>《PyTorch中文文档》 <a href="https://pytorch-cn.readthedocs.io/zh/latest/">https://pytorch-cn.readthedocs.io/zh/latest/</a></p>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CV Papers</title>
      <link href="/2022/06/30/006-DL_Papers/"/>
      <url>/2022/06/30/006-DL_Papers/</url>
      
        <content type="html"><![CDATA[<h1 id="read-paper-list"><a href="#read-paper-list" class="headerlink" title="read-paper-list"></a>read-paper-list</h1><p>semantic segmentation&#x2F;object detection&#x2F;light-weight network&#x2F;instance segmentation</p><h1 id="Deep-base-network"><a href="#Deep-base-network" class="headerlink" title="Deep-base-network"></a><strong>Deep-base-network</strong></h1><ul><li>ImageNet Classification with Deep Convolutional Neural Networks(<strong>AlexNet</strong>)</li><li>Very Deep Convolutional Networks For Large-Scale Image Recognition(<strong>VGG</strong>)</li><li>Network In Network(<strong>NIN</strong>)</li><li>Going Deeper with Convolutions(<strong>GoogleNet</strong>)</li><li>Deep Residual Learning for Image Recognition(<strong>ResNet</strong>)</li><li>Densely Connected Convolutional Networks(<strong>DenseNet</strong>)</li><li>Squeeze-and-Excitation Networks(<strong>SENet</strong>)</li><li>Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks(<strong>GENet</strong>)</li><li>Non-local Neural Networks</li><li>Convolutional Neural Networks with layer reuse(<strong>LruNet</strong>)</li><li>GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond(<strong>GCNet</strong>)</li><li>Rethinking ImageNet Pre-training</li><li>Multi-Stage HRNet: Multiple Stage High-Resolution Network for Human Pose Estimation</li></ul><h1 id="light-weight-network"><a href="#light-weight-network" class="headerlink" title="light-weight network"></a><strong>light-weight network</strong></h1><ul><li>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5 MB model size(<strong>SqueezeNet</strong>)</li><li>Mobilenets: Efficient convolutional neural networks for mobile vision applications(<strong>Mobilenet V1</strong>)</li><li>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices(<strong>ShuffleNet V1</strong>)</li><li>Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation(<strong>Mobilenet V2</strong>)</li><li>SqueezeNext: Hardware-Aware Neural Network Design(<strong>SqueezeNext</strong>)</li><li>CondenseNet: An Efficient DenseNet using Learned Group Convolutions(<strong>CondenseNet</strong>)</li><li>Pelee: A Real-Time Object Detection System on Mobile Devices(<strong>PeleeNet</strong>)</li><li>ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design(<strong>ShuffleNet V2</strong>)</li><li>ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation(<strong>ESPNet</strong>)</li><li>ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions(<strong>ChannelNets</strong>)</li><li>ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network(<strong>ESPNetV2</strong>)</li><li>Interleaved Group Convolutions for Deep Neural Networks(<strong>IGCV1</strong>)</li><li>IGCV2: Interleaved Structured Sparse Convolutional Neural Networks(<strong>IGCV2</strong>)</li><li>IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks(<strong>IGCV3</strong>)</li><li>MnasNet: Platform-Aware Neural Architecture Search for Mobile(<strong>MnasNet</strong>)</li><li>FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search(<strong>FBNet</strong>)</li><li>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks(<strong>EfficientNet</strong>)</li><li>DiCENet: Dimension-wise Convolutions for Efficient Networks(<strong>DiCENet</strong>)</li><li>Hybrid Composition with IdleBlock: More Efficient Networks for Image Recognition</li><li>An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection</li></ul><h1 id="semantic-segmentation"><a href="#semantic-segmentation" class="headerlink" title="semantic segmentation"></a><strong>semantic segmentation</strong></h1><ul><li>Fully Convolutional Networks for Semantic Segmentation(<strong>FCN</strong>)</li><li>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation(<strong>SegNet</strong>)</li><li>U-Net: Convolutional Networks for Biomedical Image Segmentation(<strong>UNet</strong>)</li><li>Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs(<strong>Deeplab v1</strong>)</li><li>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs(<strong>Deeplab v2</strong>)</li><li>Understanding Convolution for Semantic Segmentation(<strong>DUC</strong>)</li><li>Pyramid Scene Parsing Network(<strong>PSPNet</strong>)</li><li>Large Kernel Matters – Improve Semantic Segmentation by Global Convolutional Network(<strong>GCN</strong>)</li><li>Rethinking Atrous Convolution for Semantic Image Segmentation(<strong>Deeplab v3</strong>)</li><li>DenseASPP for Semantic Segmentation in Street Scenes（<strong>DenseASPP</strong>）</li><li>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation(<strong>Deeplab v3plus</strong>）</li><li>Context Encoding for Semantic Segmentation(<strong>EncNet</strong>)</li><li>Learning a Discriminative Feature Network for Semantic Segmentation(<strong>DFN</strong>)</li><li>Smoothed Dilated Convolutions for Improved Dense Prediction(<strong>SDC</strong>)</li><li>Pyramid Attention Network for Semantic Segmentation(<strong>PAN</strong>)</li><li>Exploring Context with Deep Structured models for Semantic Segmentation(<strong>FeatMap-Net</strong>)</li><li>ExFuse: Enhancing Feature Fusion for Semantic Segmentation(<strong>ExFuse</strong>)</li><li>Dilated Residual Networks(<strong>DRN</strong>)</li><li>Dual Attention Network for Scene Segmentation(<strong>DANet</strong>)</li><li>OCNet:Object Context Network for Scene Parsing(<strong>OCNet</strong>)</li><li>RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation(<strong>RefineNet</strong>)</li><li>Dense Relation Network: Learning Consistent And Context-Aware Prepresentation For Semantic Image Segmentation(<strong>DRN</strong>)</li><li>CCNet: Criss-Cross Attention for Semantic Segmentation(<strong>CCNet</strong>)</li><li>Unified Perceptual Parsing for Scene Understanding(<strong>UPerNet</strong>)</li><li>Tree-structured Kronecker Convolutional Networks for Semantic Segmentation(<strong>TKNet</strong>)</li><li>NeuroIoU: Learning a Surrogate Loss for Semantic Segmentation(<strong>NeuroIoU</strong>)</li><li>Decoders Matter for Semantic Segmentation:Data-Dependent Decoding Enables Flexible Feature Aggregation</li><li>GFF: Gated Fully Fusion for Semantic Segmentation（<strong>GFF</strong>）</li><li>Learning Fully Dense Neural Networks for Image Semantic Segmentation（<strong>FDNet</strong>）</li><li>ZigZagNet: Fusing Top-Down and Bottom-Up Context for Object Segmentation(<strong>ZigZagNet</strong>)</li><li>Adaptive Pyramid Context Network for Semantic Segmentation(<strong>APCNet</strong>)</li><li>Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation</li><li>ACFNet: Attentional Class Feature Network for Semantic Segmentation(<strong>ACFNet</strong>)</li><li>Miss Detection vs. False Alarm: Adversarial Learning for Small Object Segmentation in Infrared Images</li><li>Dual Graph Convolutional Network for Semantic Segmentation</li><li>Global Aggregation then Local Distribution in Fully Convolutional Networks</li><li>Dynamic Multi-scale Filters for Semantic Segmentation</li><li>Unifying Training and Inference for Panoptic Segmentation</li><li>Semantic Flow for Fast and Accurate Scene Parsing</li><li>AlignSeg: Feature-Aligned Segmentation Networks</li><li>Cars Can’t Fly up in the Sky: Improving Urban-Scene Segmentation via Height-driven Attention Networks</li><li>Context Prior for Scene Segmentation</li></ul><h1 id="fast-x2F-real-time-segmentation"><a href="#fast-x2F-real-time-segmentation" class="headerlink" title="fast&#x2F;real-time segmentation"></a><strong>fast&#x2F;real-time segmentation</strong></h1><ul><li>ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation(<strong>ENet</strong>)</li><li>ICNet for Real-Time Semantic Segmentation(<strong>ICNet</strong>)</li><li>BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation(<strong>BiSeNet</strong>)</li><li>LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation(<strong>LinkNet</strong>)</li><li>Rtseg: Real-Time Semantic Segmentation Comparative Study</li><li>Shuffleseg: Real-Time Semantic Segmentation Network(<strong>Shuffleseg</strong>)</li><li>ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation(<strong>ESPNet</strong>)</li><li>Light-Weight RefineNet for Real-Time Semantic Segmentation(<strong>Light-Weight RefineNet</strong>)</li><li>LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation(<strong>LinkNet</strong>)</li><li>D-LinkNet: LinkNet with Pretrained Encoder and Dilated Convolution for High Resolution Satellite Imagery Road Extraction(<strong>D-LinkNet</strong>)</li><li>CGNet: A Light-weight Context Guided Network for Semantic Segmentation(<strong>CGNet</strong>)</li><li>Efficient ConvNet for Real-time Semantic Segmentation</li><li>A Comparative Study of Real-time Semantic Segmentation for Autonomous Driving</li><li>ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time(<strong>ContextNet</strong>)</li><li>ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network(<strong>ESPNetV2</strong>)</li><li>ShelfNet for Real-time semantic segmentation(<strong>ShelfNet</strong>)</li><li>ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation(<strong>ERFNet</strong>)</li><li>Concentrated-Comprehensive Convolutions for lightweight semantic segmentation(<strong>CCCNet</strong>)</li><li>DSNet for Real-Time Driving Scene Semantic Segmentation(<strong>DSNet</strong>)</li><li>Efficient Dense Modules of Asymmetric Convolution for Real-Time Semantic Segmentation(<strong>EDANet</strong>)</li><li>Fast-SCNN: Fast Semantic Segmentation Network(<strong>Fast-SCNN</strong>)</li><li>Guided Upsampling Network for Real-Time Semantic Segmentation(<strong>GUN</strong>)</li><li>In Defense of Pre-trained ImageNet Architecturesfor Real-time Semantic Segmentation of Road-driving Images(<strong>SwiftNetRN</strong>)</li><li>Residual Pyramid Learning for Single-Shot Semantic Segmentation(<strong>RPNet</strong>)</li><li>DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation(<strong>DFANet</strong>)</li><li>DSNet: An Efficient CNN for Road Scene Segmentation(<strong>DSNet</strong>) </li><li>Spatial Sampling Network for Fast Scene Understanding</li><li>RGPNET: A REAL-TIME GENERAL PURPOSE SEMANTIC SEGMENTATION</li><li>LiteSeg: A Novel Lightweight ConvNet for Semantic Segmentation</li><li>FASTERSEG: SEARCHING FOR FASTER REAL-TIME SEMANTIC SEGMENTATION</li><li>Partial Order Pruning: for Best Speed&#x2F;Accuracy Trade-off in Neural Architecture Search</li><li>Customizable Architecture Search for Semantic Segmentation</li><li>Semantic Flow for Fast and Accurate Scene Parsing</li><li>BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation</li><li>ASNet: Aggregated Scale Transformations for Real-Time Semantic Segmentation</li></ul><h1 id="Deep-object-detection"><a href="#Deep-object-detection" class="headerlink" title="Deep object detection"></a><strong>Deep object detection</strong></h1><ul><li>Rich feature hierarchies for accurate object detection and semantic segmentation(<strong>R-CNN</strong>)</li><li>SSD: Single Shot MultiBox Detector(<strong>SSD</strong>)</li><li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks(<strong>Faster R-CNN</strong>)</li><li>Feature Pyramid Networks for Object Detection(<strong>FPN</strong>) </li><li>Is Faster R-CNN Doing Well for Pedestrian Detection?(<strong>RPN_BF</strong>)</li><li>Training Region-based Object Detectors with Online Hard Example Mining(<strong>OHEM</strong>)</li><li>Receptive Field Block Net for Accurate and Fast Object Detection(<strong>RFBNet</strong>)</li><li>Focal Loss for Dense Object Detection(<strong>RetinaNet</strong>)</li><li>Single-Shot Refinement Neural Network for Object Detection(<strong>RefinDet</strong>)</li><li>PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection(<strong>PVANET</strong>)</li><li>Multi-label learning of part detectors for heavily occluded pedestrian detection(<strong>JL-TopS</strong>)</li><li>Graininess-aware Deep Feature Learning for Pedestrian Detection(<strong>GDFL</strong>)</li><li>M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network(<strong>M2Det</strong>) </li><li>CFENet: An Accurate and Efficient Single-Shot Object Detector for Autonomous Driving(<strong>CFENet</strong>) </li><li>ScratchDet: Training Single-Shot Object Detectors from Scratch(<strong>ScratchDet</strong>) </li><li>Pooling Pyramid Network for Object Detection（<strong>PPN</strong>）</li><li>ThunderNet: Towards Real-time Generic Object Detection(<strong>ThunderNet</strong>)</li><li>Light-Weight RetinaNet for Object Detection</li><li>CornerNet: Detecting Objects as Paired Keypoints(<strong>CornerNet</strong>)</li><li>Bottom-up Object Detection by Grouping Extreme and Center Points(<strong>ExtremeNet</strong>)</li><li>RepPoints: Point Set Representation for Object Detection(<strong>RepPoints</strong>)</li><li>FCOS: Fully Convolutional One-Stage Object Detection(<strong>FCOS</strong>)</li><li>Mask-Guided Attention Network for Occluded Pedestrian Detection</li><li>Learning Rich Features at High-Speed for Single-Shot Object Detection. </li><li>Dynamic Anchor Feature Selection for Single-Shot Object Detection.</li><li>Contextual Attention for Hand Detection in the Wild</li><li>Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</li><li>Multiple Anchor Learning for Visual Object Detection</li><li>NETNet: Neighbor Erasing and Transferring Network for Better Single Shot Object Detection</li><li>Is Sampling Heuristics Necessary in Training Deep Object Detectors?</li><li>Rethinking Classification and Localization for Object Detection</li><li>Multiple Anchor Learning for Visual Object Detection</li><li>Learning from Noisy Anchors for One-stage Object Detection</li><li>Learning a Unified Sample Weighting Network for Object Detection∗</li><li>D2Det: Towards High Quality Object Detection and Instance Segmentation</li><li>AugFPN: Improving Multi-scale Feature Learning for Object Detection</li><li>Scale-Equalizing Pyramid Convolution for Object Detection</li></ul><h1 id="Face-Detection"><a href="#Face-Detection" class="headerlink" title="Face Detection"></a><strong>Face Detection</strong></h1><ul><li>S3FD: Single Shot Scale-invariant Face Detector(<strong>SFD</strong>)</li><li>FaceBoxes: A CPU Real-time Face Detector with High Accuracy(<strong>FaceBoxes</strong>)</li><li>Detecting Face with Densely Connected Face Proposal Network(<strong>DCFPN</strong>)</li><li>SSH: Single Stage Headless Face Detector(<strong>SSH</strong>)</li><li>DSFD: Dual Shot Face Detector(<strong>DSFD</strong>)</li><li>Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks(<strong>MTCNN</strong>)</li><li>PyramidBox: A Context-assisted Single Shot Face Detector(<strong>PyramidBox</strong>)</li><li>SRN:Selective Refinement Network for High Performance Face Detection(<strong>SRN</strong>)</li><li>Single Shot Attention-Based Face Detector(<strong>AFN</strong>)</li><li>Improved Selective Refinement Network for Face Detection(<strong>ISRN</strong>)</li><li>PyramidBox++: High Performance Detector for Finding Tiny Face(<strong>PyramidBox++</strong>)</li><li>RetinaFace: Single-stage Dense Face Localisation in the Wild(<strong>RetinaFace</strong>)</li></ul><h1 id="Instance-segmentation"><a href="#Instance-segmentation" class="headerlink" title="Instance segmentation"></a><strong>Instance segmentation</strong></h1><ul><li>Fully Convolutional Instance-aware Semantic Segmentation(<strong>FCIS</strong>)</li><li>Instance-aware Semantic Segmentation via Multi-task Network Cascades(<strong>MNC</strong>)</li><li>Mask R-CNN</li><li>Mask Scoring R-CNN</li><li>Path Aggregation Network for Instance Segmentation(<strong>PANet</strong>)</li><li>RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free(<strong>RetinaMask</strong>)</li><li>YOLACT Real-time Instance Segmentation(<strong>YOLACT</strong>)</li><li>Parsing R-CNN for Instance-Level Human Analysis(<strong>Parsing R-CNN</strong>)</li><li>BlitzNet: A Real-Time Deep Network for Scene Understanding(<strong>BlitzNet</strong>)</li><li>Hybrid Task Cascade for Instance Segmentation(<strong>HTC</strong>)</li><li>Triply Supervised Decoder Networks for Joint Detection and Segmentation(<strong>TripleNet</strong>)</li><li>ZigZagNet: Fusing Top-Down and Bottom-Up Context for Object Segmentation(<strong>ZigZagNet</strong>)</li><li>Bounding Box Embedding for Single Shot Person Instance Segmentation</li><li>Shape-aware Feature Extraction for Instance Segmentation</li><li>Real-Time Panoptic Segmentation from Dense Detections</li><li>EmbedMask: Embedding Coupling for One-stage Instance Segmentation</li><li>PolyTransform: Deep Polygon Transformer for Instance Segmentation</li><li>SOLO: Segmenting Objects by Locations</li><li>RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation</li><li>SSAP: Single-Shot Instance Segmentation With Affinity Pyramid</li><li>YOLACT++：Better Real-time Instance Segmentation</li><li>SAIS: Single-stage Anchor-free Instance Segmentation</li><li>PolarMask: Single Shot Instance Segmentation with Polar Representation</li><li>BANet: Bidirectional Aggregation Network with Occlusion Handling for Panoptic Segmentation</li></ul><h1 id="Mutil-task-learning"><a href="#Mutil-task-learning" class="headerlink" title="Mutil-task learning"></a><strong>Mutil-task learning</strong></h1><ul><li>End-to-End Multi-Task Learning with Attention.</li><li>Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics.</li><li>BlitzNet: A Real-Time Deep Network for Scene Understanding.</li><li>Triply Supervised Decoder Networks for Joint Detection and Segmentation</li><li>Real-time Joint Object Detection and Semantic Segmentation Network for Automated Driving.</li><li>Driving Scene Perception Network: Real-time Joint Detection, Depth Estimation and Semantic Segmentation.</li><li>GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Network</li><li>MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving</li><li>MultiNet++: Multi-Stream Feature Aggregation and Geometric Loss Strategy for Multi-Task Learning</li><li>Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems</li><li>MTI-Net: Multi-Scale Task Interaction Networks for Multi-Task Learning</li><li>AP-MTL: Attention Pruned Multi-task Learning Model for Real-time Instrument Detection and Segmentation in Robot-assisted Surgery</li></ul><h1 id="non-deep-object-detection"><a href="#non-deep-object-detection" class="headerlink" title="non-deep object detection"></a><strong>non-deep object detection</strong></h1><ul><li>Robust Real-Time Face Detection(<strong>Haar+Adaboost</strong>)</li><li>Integral Channel Features(<strong>ICF</strong>)</li><li>The Fastest Pedestrian Detector in the West(<strong>FPDW</strong>)</li><li>Fast Feature Pyramids for Object Detection(<strong>ACF</strong>)</li><li>Local Decorrelation for Improved Pedestrian Detection(<strong>LDCF</strong>)</li><li>Convolutional Channel Features(<strong>CCF</strong>)</li><li>Informed Haar-like Features Improve Pedestrian Detection(<strong>InformedHaar</strong>)</li><li>Fast Pedestrian Detection for Mobile Devices(<strong>FastCF</strong>)</li><li>Pedestrian detection at 100 Frames Per Second(<strong>VeryFast</strong>)</li><li>To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection(<strong>ACF+&#x2F;LDCF+</strong>)</li><li>Filtered channel features for pedestrian detection(<strong>Checkerboard</strong>)</li><li>Pedestrian Detection Inspired by Appearance Constancy and Shape Symmetry(<strong>NNNF</strong>)</li><li>Aggregate Channel Features for Multi-view Face Detection(<strong>ACFFace</strong>)</li><li>Pedestrian Detection with Spatially Pooled Features and Structured Ensemble Learning(<strong>SpatialPooling+</strong>)</li><li>BAdaCost: Multi-class Boosting with Costs(<strong>BAdaCost</strong>)</li><li>Exploring Prior Knowledge for Pedestrian Detection(<strong>SCCPriors</strong>)</li><li>A Fast, Modular Scene Understanding System using Context-Aware Object Detection(<strong>SC-ACF</strong>）</li><li>Ten Years of Pedestrian Detection,What Have We Learned?(<strong>Katamari</strong>)</li><li>How Far are We from Solving Pedestrian Detection?</li><li>What Can Help Pedestrian Detection?</li><li>Taking a Deeper Look at Pedestrians</li><li>Semantic Channels for Fast Pedestrian Detection(<strong>MRFC+Semantic</strong>)</li><li>Fast Boosting based Detection using Scale Invariant Multimodal Multiresolution Filtered Features</li><li>Learning Multilayer Channel Features for Pedestrian Detection</li><li>Fast and Robust Object Detection Using Visual Subcategories</li><li>Learning to Detect Vehicles by Clustering Appearance Patterns(<strong>Subcat</strong>)</li><li>Looking at Pedestrians at Different Scales: A Multiresolution Approach and Evaluations(<strong>MR-ACF</strong>)</li><li>Multiresolution models for object detection</li><li>Face Detection without Bells and Whistles</li><li>Fast Detection of Multiple Objects in Traffic Scenes With a Common Detection Framework</li><li>An Exploration of Why and When Pedestrian Detection Fails </li><li>Discriminative Sub-categorization</li></ul><h1 id="Image-Stitching"><a href="#Image-Stitching" class="headerlink" title="Image Stitching"></a><strong>Image Stitching</strong></h1><ul><li>Automatic Panoramic Image Stitching Using Invariant Features(<strong>IJCV2007</strong>) </li><li>As-Projective-As-Possible Image Stitching with Moving DLT(<strong>APAP</strong>)</li><li>Shape-Preserving Half-Projective Warps for Image Stitching(<strong>SPHP</strong>)</li><li>Adaptive As-Natural-As-Possible Image Stitching(<strong>AANAP</strong>)</li><li>MAGSAC: marginalizing sample consensus</li><li>MAGSAC++, a fast, reliable and accurate robust estimator</li><li>An Evaluation of Feature Matchers for Fundamental Matrix Estimation</li><li>GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature correspondence</li><li>Vanishing Point Guided Natural Image Stitching</li><li>Warping Residual Based Image Stitching for Large Parallax</li></ul>]]></content>
      
      
      <categories>
          
          <category> 参考文献 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 网络模型 </tag>
            
            <tag> 参考文献 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/06/30/001-hello-world/"/>
      <url>/2022/06/30/001-hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Copyright-SvyJ"><a href="#Copyright-SvyJ" class="headerlink" title="Copyright@SvyJ"></a>Copyright@SvyJ</h2><h2 id="Quick-Start-First"><a href="#Quick-Start-First" class="headerlink" title="Quick Start First"></a>Quick Start First</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>遥感图像变化检测相关</title>
      <link href="/2021/12/23/020-RemoteSensingChangeDetection/"/>
      <url>/2021/12/23/020-RemoteSensingChangeDetection/</url>
      
        <content type="html"><![CDATA[<h2 id="Notes：源自于老板最近想接的横向项目，记录一下从调研到跑路的历程-D。（单纯做图像处理的一般都是Matlab代码，Python实现的代码可太少了-…）"><a href="#Notes：源自于老板最近想接的横向项目，记录一下从调研到跑路的历程-D。（单纯做图像处理的一般都是Matlab代码，Python实现的代码可太少了-…）" class="headerlink" title="Notes：源自于老板最近想接的横向项目，记录一下从调研到跑路的历程:D。（单纯做图像处理的一般都是Matlab代码，Python实现的代码可太少了:(…）"></a>Notes：源自于老板最近想接的横向项目，记录一下从调研到跑路的历程:D。<br>（单纯做图像处理的一般都是Matlab代码，Python实现的代码可太少了:(…）</h2><h1 id="一、遥感图像变化检测"><a href="#一、遥感图像变化检测" class="headerlink" title="一、遥感图像变化检测"></a><strong>一、遥感图像变化检测</strong></h1><h2 id="1、任务目标"><a href="#1、任务目标" class="headerlink" title="1、任务目标"></a>1、任务目标</h2><ul><li>利用多时相的遥感数据，采用多种图像处理和模式识别方法提取变化信息，并定量分析和确定地表变化的特征与过程。它涉及变化的类型、分布状况与变化量，即需要确定变化前、后的地面类型、界线、及变化趋势，进而分析这些动态变化的特点与原因。<br>(1）判断是否发生了变化；<br>(2）确定发生变化的区域；<br>(3）鉴别变化的性质；<br>(4）评估变化的时间和空间分布模式。<br><img src="https://i.loli.net/2021/07/05/HcgyZO8mVIuNqdR.png" alt="变化检测"><br>猜测前两点的变化检测就是“他们”要达到的基本目标了，前两点也是变化检测研究中需要首先解决的问题，狭义上的变化检测概念即是由这两方面构成。</li></ul><h2 id="2、影响因素（难点）"><a href="#2、影响因素（难点）" class="headerlink" title="2、影响因素（难点）"></a>2、影响因素（难点）</h2><ul><li>遥感系统因素的影响(时间、空间、光谱、辐射分辨率)</li><li>环境因素的影响(大气、土壤湿度状况、物候特征)</li><li>成像设备？（猜测本次任务存在这个问题）</li></ul><h1 id="二、数据集整理"><a href="#二、数据集整理" class="headerlink" title="二、数据集整理"></a><strong>二、数据集整理</strong></h1><ul><li>放链接：<a href="https://blog.csdn.net/weixin_41868601/article/details/113888782">数据集</a></li></ul><h1 id="三、已有方法"><a href="#三、已有方法" class="headerlink" title="三、已有方法"></a><strong>三、已有方法</strong></h1><h2 id="1、传统方法"><a href="#1、传统方法" class="headerlink" title="1、传统方法"></a>1、传统方法</h2><h3 id="（1）比较后分类变化检测算法"><a href="#（1）比较后分类变化检测算法" class="headerlink" title="（1）比较后分类变化检测算法"></a>（1）比较后分类变化检测算法</h3><ul><li>基于简单代数运算的变化检测方法<br>  归一化图像差值法<br>  <strong>（已实现）图像灰度差值法（图像的灰度信息反映了地物的波谱反射特性）</strong><br>  图像纹理特征差值法<br>  图像回归法<br>  图像植被指数差分法<br>  图像比值法<br>  变化向量分析法</li><li>基于图像变换的变化检测方法<br>  <strong>（已实现）主成分分析法PCA</strong><br>  独立成分分析法ICA<br>  正交变换<br>  缨帽变换（K-T变换）<br>  典型相关分析（CCA）<br>  <strong>（已实现）多元变化检测(multivariate alteration detection，MAD)</strong><br>  Gramm-Schmidt变换<br>  Chi-square变换<br>  穗帽变换(Tasseled Cap Transformation)<br>  HSI变换</li><li>基于图像空间特征的变化检测方法<br>  基于统计特征的变化检测算法<br>  基于纹理特征的变化检测算法<br>  基于空间结构特征的变化检测算法</li></ul><h3 id="（2）分类后比较变化检测算法"><a href="#（2）分类后比较变化检测算法" class="headerlink" title="（2）分类后比较变化检测算法"></a>（2）分类后比较变化检测算法</h3><h3 id="（3）基于对象变化检测算法"><a href="#（3）基于对象变化检测算法" class="headerlink" title="（3）基于对象变化检测算法"></a>（3）基于对象变化检测算法</h3><h3 id="（4）基于统计模型变化检测方法"><a href="#（4）基于统计模型变化检测方法" class="headerlink" title="（4）基于统计模型变化检测方法"></a>（4）基于统计模型变化检测方法</h3><h2 id="2、部分还没看的论文（传统方法）"><a href="#2、部分还没看的论文（传统方法）" class="headerlink" title="2、部分还没看的论文（传统方法）"></a>2、部分还没看的论文（传统方法）</h2><p>(1) 2019.Bobholamovic&#x2F;ChangeDetectionToolbox<br>(2) 2019.M J Canty. Image Analysis, Classification and Change Detection in Remote Sensing(Fourth Revised Edition)<br>(3) 2017.M J Canty. Change Detection with Google Earth Engine Imagery<br>(4) 2014.M J Canty. Image Analysis, Classification and Change Detection in Remote Sensing(Third Revised Edition)<br>(5) Zhu Zhe.Algorithm developed for Continuous Change Detection and Classification (CCDC) of land cover using all available Landsat data<br>(6) Implementation of “ 2009.Celik T. Unsupervised change detection in satellite images using principal component analysis and k-means clustering “.(Matlab, Python)<br>(7) 2007.Allan Aasbjerg Nielsen.IR-MAD(The Regularized Iteratively Reweighted Multivariate Alteration Detection)</p><h1 id="四、后处理"><a href="#四、后处理" class="headerlink" title="四、后处理"></a><strong>四、后处理</strong></h1><ul><li>经过图像处理会出现较多的小连通区域或孔洞，只能通过后处理来解决。</li></ul><h2 id="1、填充孔洞"><a href="#1、填充孔洞" class="headerlink" title="1、填充孔洞"></a>1、填充孔洞</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"># 膨胀再腐蚀回去<br>img = img.astype(&#x27;uint8&#x27;)<br>kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))<br>img = cv2.dilate(img, kernel)<br>img = cv2.erode(img, kernel)<br></code></pre></td></tr></table></figure><h2 id="2、小连通区域的去除"><a href="#2、小连通区域的去除" class="headerlink" title="2、小连通区域的去除"></a>2、小连通区域的去除</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"># 阈值法，连通区域面积小于设定阈值时填充为背景<br>contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)<br>for contour in contours:<br>    area = cv2.contourArea(contour)<br>    if area &lt; threshold:<br>        cv2.drawContours(img, [contour], -1, (0,0,0), thickness=-1)     # 填充为背景(0,0,0)<br>        continue<br></code></pre></td></tr></table></figure><ul><li>参考资料：<br>[1] 《遥感应用分析原理与方法》 赵英时 第二版<br>[2] <a href="https://gitee.com/jia_leilei001/awesome-remote-sensing-change-detection">https://gitee.com/jia_leilei001/awesome-remote-sensing-change-detection</a><br>[3] <a href="https://blog.csdn.net/qq_37554556/article/details/104134021">https://blog.csdn.net/qq_37554556/article/details/104134021</a><br>[4] <a href="https://blog.csdn.net/zhang22huan/article/details/8482490">https://blog.csdn.net/zhang22huan/article/details/8482490</a><br>[5] <a href="https://blog.csdn.net/weixin_39802680/article/details/106026342">https://blog.csdn.net/weixin_39802680/article/details/106026342</a><br>[6] <a href="https://blog.csdn.net/weixin_45145485/article/details/109005509">https://blog.csdn.net/weixin_45145485/article/details/109005509</a> (这里吐槽以下：这个方法出来的检测图全黑是什么鬼？)<br>[7] <a href="https://gitee.com/weijujie/changeDetection">https://gitee.com/weijujie/changeDetection</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
